#!/usr/bin/env python3
"""
ä¿®å¤åçš„checkpoint-4250æµ‹è¯•è„šæœ¬
ä½¿ç”¨æ­£ç¡®çš„Qwen2.5-VLæ ¼å¼å¤„ç†å›¾åƒtokenåŒ¹é…é—®é¢˜
"""
import os
import torch
from PIL import Image

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def load_checkpoint_model(checkpoint_path):
    """åŠ è½½checkpoint-4250æ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        print("âœ… æ•°å€¼å¢å¼ºæ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def correct_inference_test(model, processor, image_path, question):
    """ä½¿ç”¨æ­£ç¡®æ ¼å¼çš„æ¨ç†æµ‹è¯•"""
    try:
        print(f"ğŸ–¼ï¸ å¤„ç†å›¾åƒ: {image_path}")
        print(f"â“ é—®é¢˜: {question}")
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return ""
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        print("ğŸ”„ ç”Ÿæˆå›ç­”ä¸­...")
        
        # ä½¿ç”¨Qwen2.5-VLå®˜æ–¹æ¨èçš„æ ¼å¼
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": question}
                ]
            }
        ]
        
        # æ–¹æ³•1: å¦‚æœå¤„ç†å™¨æ”¯æŒapply_chat_template
        try:
            text = processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = processor.process_vision_info(messages)
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            print("âœ“ ä½¿ç”¨å®˜æ–¹æ ‡å‡†æ ¼å¼")
            
        except AttributeError:
            # æ–¹æ³•2: æ‰‹åŠ¨æ„å»ºæ­£ç¡®çš„æ ¼å¼
            print("âš ï¸ å¤„ç†å™¨æ— èŠå¤©æ¨¡æ¿ï¼Œä½¿ç”¨æ‰‹åŠ¨æ ¼å¼")
            
            # ä½¿ç”¨Qwen2.5-VLçš„æ ‡å‡†æ ¼å¼
            prompt = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>" + question + "<|im_end|>\n<|im_start|>assistant\n"
            
            inputs = processor(
                text=prompt,
                images=image,
                return_tensors="pt",
                padding=True
            )
            print("âœ“ ä½¿ç”¨æ‰‹åŠ¨æ ‡å‡†æ ¼å¼")
        
        except Exception as e:
            # æ–¹æ³•3: æœ€ç®€å•çš„æ ¼å¼
            print(f"âš ï¸ æ ‡å‡†æ ¼å¼å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•æœ€ç®€å•æ ¼å¼")
            
            # æœ€ç®€å•çš„å›¾åƒæ–‡æœ¬ç»„åˆ
            prompt = f"<image>\n{question}"
            
            inputs = processor(
                text=prompt,
                images=image,
                return_tensors="pt",
                padding=True
            )
            print("âœ“ ä½¿ç”¨æœ€ç®€å•æ ¼å¼")
        
        # ç§»åŠ¨åˆ°GPU
        inputs = inputs.to("cuda")
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        answer = output_text[0].strip() if output_text else ""
        print(f"ğŸ¤– æ¨¡å‹å›ç­”: {answer}")
        
        return answer
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return ""

def simple_text_only_test(model, processor, question):
    """çº¯æ–‡æœ¬æµ‹è¯•ï¼ˆä¸å«å›¾åƒï¼‰"""
    try:
        print(f"ğŸ’¬ çº¯æ–‡æœ¬é—®é¢˜: {question}")
        
        # æ„å»ºçº¯æ–‡æœ¬è¾“å…¥
        prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = processor.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True
        )
        inputs = inputs.to("cuda")
        
        print("ğŸ”„ ç”Ÿæˆå›ç­”ä¸­...")
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        answer = output_text[0].strip() if output_text else ""
        print(f"ğŸ¤– æ¨¡å‹å›ç­”: {answer}")
        
        return answer
        
    except Exception as e:
        print(f"âŒ çº¯æ–‡æœ¬æ¨ç†å¤±è´¥: {e}")
        return ""

def test_numeric_capabilities(model, processor):
    """æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ§® æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›")
    print("="*60)
    
    # æµ‹è¯•æ•°å€¼æ–‡æœ¬å¤„ç†
    test_cases = [
        "è¿™ä¸ªè½¨è¿¹åŒ…å«åæ ‡ <num><3.14> å’Œ <num><-2.5>",
        "ä½ç½®ä¿¡æ¯: (<num><+11.3>, <num><-4.0>)",
        "è½¨è¿¹ [PT, (<num><+3.41>, <num><-0.06>), (<num><+6.96>, <num><-0.20>)]"
    ]
    
    for i, test_text in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•æ•°å€¼æ–‡æœ¬ {i}:")
        print(f"åŸå§‹: {test_text}")
        
        try:
            # æµ‹è¯•æ•°å€¼tokenå¤„ç†
            result = processor._process_text_with_numeric_tokens(test_text)
            
            if isinstance(result, tuple):
                processed_text, numeric_values = result
                print(f"å¤„ç†å: {processed_text}")
                print(f"æ•°å€¼: {numeric_values}")
            elif isinstance(result, dict):
                print(f"å¤„ç†å: {result['text']}")
                print(f"æ•°å€¼: {result['numeric_values']}")
            else:
                print(f"ç»“æœ: {result}")
                
        except Exception as e:
            print(f"âŒ æ•°å€¼å¤„ç†å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¿®å¤åçš„checkpoint-4250æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    # é…ç½®è·¯å¾„
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    # 1. åŠ è½½æ¨¡å‹
    try:
        model, processor = load_checkpoint_model(checkpoint_path)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•: {e}")
        return
    
    # 2. æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›
    test_numeric_capabilities(model, processor)
    
    # 3. çº¯æ–‡æœ¬æµ‹è¯•ï¼ˆæ›´å®‰å…¨ï¼‰
    print("\n" + "="*60)
    print("ğŸ’¬ çº¯æ–‡æœ¬æ¨ç†æµ‹è¯•")
    print("="*60)
    
    text_questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        "ä»€ä¹ˆæ˜¯æ•°å€¼å¢å¼ºçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ",
        "è¯·ç”Ÿæˆä¸€ä¸ªç®€å•çš„è½¨è¿¹åºåˆ—ã€‚"
    ]
    
    for i, question in enumerate(text_questions, 1):
        print(f"\n--- çº¯æ–‡æœ¬æµ‹è¯• {i} ---")
        answer = simple_text_only_test(model, processor, question)
        if answer:
            print(f"âœ… çº¯æ–‡æœ¬æµ‹è¯• {i} æˆåŠŸ")
        else:
            print(f"âŒ çº¯æ–‡æœ¬æµ‹è¯• {i} å¤±è´¥")
    
    # 4. å°è¯•VQAæµ‹è¯•ï¼ˆä½¿ç”¨æ­£ç¡®æ ¼å¼ï¼‰
    print("\n" + "="*60)
    print("ğŸ–¼ï¸ VQAæ¨ç†æµ‹è¯•ï¼ˆä½¿ç”¨ä¿®å¤æ ¼å¼ï¼‰")
    print("="*60)
    
    # å‡†å¤‡æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "image": "/data1/wangzhiye/data2/nuscenes/samples/CAM_FRONT/n015-2018-07-11-11-54-16+0800__CAM_FRONT__1531281439762460.jpg",
            "question": "æè¿°è¿™å¼ å›¾ç‰‡ã€‚"
        },
        {
            "image": "/data1/wangzhiye/data2/nuscenes/samples/CAM_FRONT/n015-2018-07-11-11-54-16+0800__CAM_FRONT__1531281439762460.jpg", 
            "question": "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ"
        }
    ]
    
    # è¿è¡ŒVQAæµ‹è¯•
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- VQAæµ‹è¯• {i} ---")
        answer = correct_inference_test(
            model, 
            processor, 
            test_case["image"], 
            test_case["question"]
        )
        
        if answer:
            print(f"âœ… VQAæµ‹è¯• {i} æˆåŠŸ")
        else:
            print(f"âŒ VQAæµ‹è¯• {i} å¤±è´¥")
    
    print("\n" + "="*60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*60)

if __name__ == "__main__":
    print(">>> æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹ç»„ä»¶å·²æ³¨å†Œ")
    main()
