#!/usr/bin/env python3
"""
ç®€åŒ–çš„checkpoint-4250æµ‹è¯•è„šæœ¬
é™åˆ¶å›¾ç‰‡å¤„ç†ï¼Œä¸“æ³¨äºåŸºç¡€åŠŸèƒ½æµ‹è¯•
"""
import os
import torch
from PIL import Image

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def load_checkpoint_model(checkpoint_path):
    """åŠ è½½checkpoint-4250æ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        print("âœ… æ•°å€¼å¢å¼ºæ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def simple_text_inference(model, processor, question):
    """çº¯æ–‡æœ¬æ¨ç†æµ‹è¯•"""
    try:
        print(f"ğŸ’¬ é—®é¢˜: {question}")
        
        # æ„å»ºçº¯æ–‡æœ¬è¾“å…¥
        prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = processor.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True
        )
        inputs = inputs.to("cuda")
        
        print("ğŸ”„ ç”Ÿæˆå›ç­”ä¸­...")
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        answer = output_text[0].strip() if output_text else ""
        print(f"ğŸ¤– æ¨¡å‹å›ç­”: {answer}")
        
        return answer
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return ""

def test_single_image_vqa(model, processor, image_path, question):
    """æµ‹è¯•å•å¼ å›¾ç‰‡çš„VQAï¼ˆä½¿ç”¨æœ€å°åŒ–å¤„ç†ï¼‰"""
    try:
        print(f"ğŸ–¼ï¸ å¤„ç†å›¾åƒ: {os.path.basename(image_path)}")
        print(f"â“ é—®é¢˜: {question}")
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return ""
        
        # åŠ è½½å¹¶é™åˆ¶å›¾åƒå¤§å°
        image = Image.open(image_path).convert('RGB')
        # è°ƒæ•´å›¾åƒå¤§å°ä»¥å‡å°‘tokenæ•°é‡
        image = image.resize((224, 224))  # é™åˆ¶ä¸ºå°å°ºå¯¸
        
        print("ğŸ”„ ç”Ÿæˆå›ç­”ä¸­...")
        
        # ä½¿ç”¨æœ€ç®€å•çš„æ ¼å¼ï¼Œé™åˆ¶ä¸ºå•å¼ å›¾ç‰‡
        try:
            # å°è¯•ä½¿ç”¨<image>å ä½ç¬¦
            prompt = f"<image>\nç”¨æˆ·: {question}\nåŠ©æ‰‹:"
            
            inputs = processor(
                text=prompt,
                images=image,
                return_tensors="pt",
                padding=True
            )
            print("âœ“ ä½¿ç”¨ç®€å•å›¾åƒæ ¼å¼")
            
        except Exception as e:
            print(f"âš ï¸ å›¾åƒå¤„ç†å¤±è´¥: {e}")
            # å¦‚æœå›¾åƒå¤„ç†å¤±è´¥ï¼Œé€€å›åˆ°çº¯æ–‡æœ¬
            return simple_text_inference(model, processor, f"å‡è®¾çœ‹åˆ°ä¸€å¼ å›¾ç‰‡ï¼Œè¯·å›ç­”ï¼š{question}")
        
        # ç§»åŠ¨åˆ°GPU
        inputs = inputs.to("cuda")
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=128,  # å‡å°‘ç”Ÿæˆé•¿åº¦
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        answer = output_text[0].strip() if output_text else ""
        print(f"ğŸ¤– æ¨¡å‹å›ç­”: {answer}")
        
        return answer
        
    except Exception as e:
        print(f"âŒ VQAæ¨ç†å¤±è´¥: {e}")
        # å¦‚æœVQAå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å¼
        return simple_text_inference(model, processor, f"å‡è®¾çœ‹åˆ°ä¸€å¼ å›¾ç‰‡ï¼Œè¯·å›ç­”ï¼š{question}")

def test_numeric_capabilities(model, processor):
    """æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ§® æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›")
    print("="*60)
    
    # æµ‹è¯•æ•°å€¼æ–‡æœ¬å¤„ç†
    test_cases = [
        "è¿™ä¸ªè½¨è¿¹åŒ…å«åæ ‡ <num><3.14> å’Œ <num><-2.5>",
        "ä½ç½®ä¿¡æ¯: (<num><+11.3>, <num><-4.0>)",
        "è½¨è¿¹ [PT, (<num><+3.41>, <num><-0.06>), (<num><+6.96>, <num><-0.20>)]"
    ]
    
    for i, test_text in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•æ•°å€¼æ–‡æœ¬ {i}:")
        print(f"åŸå§‹: {test_text}")
        
        try:
            # æµ‹è¯•æ•°å€¼tokenå¤„ç†
            result = processor._process_text_with_numeric_tokens(test_text)
            
            if isinstance(result, tuple):
                processed_text, numeric_values = result
                print(f"å¤„ç†å: {processed_text}")
                print(f"æ•°å€¼: {numeric_values}")
            elif isinstance(result, dict):
                print(f"å¤„ç†å: {result['text']}")
                print(f"æ•°å€¼: {result['numeric_values']}")
            else:
                print(f"ç»“æœ: {result}")
                
        except Exception as e:
            print(f"âŒ æ•°å€¼å¤„ç†å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç®€åŒ–çš„checkpoint-4250æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    # é…ç½®è·¯å¾„
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    # 1. åŠ è½½æ¨¡å‹
    try:
        model, processor = load_checkpoint_model(checkpoint_path)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•: {e}")
        return
    
    # 2. æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›
    test_numeric_capabilities(model, processor)
    
    # 3. çº¯æ–‡æœ¬æµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ’¬ çº¯æ–‡æœ¬æ¨ç†æµ‹è¯•")
    print("="*60)
    
    text_questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        "è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å«æ•°å€¼çš„è½¨è¿¹åºåˆ—ï¼Œæ ¼å¼ä¸º [PT, (x1, y1), (x2, y2)]ã€‚",
        "è§£é‡Šä»€ä¹ˆæ˜¯è‡ªåŠ¨é©¾é©¶ä¸­çš„è½¨è¿¹è§„åˆ’ã€‚"
    ]
    
    for i, question in enumerate(text_questions, 1):
        print(f"\n--- çº¯æ–‡æœ¬æµ‹è¯• {i} ---")
        answer = simple_text_inference(model, processor, question)
        if answer:
            print(f"âœ… çº¯æ–‡æœ¬æµ‹è¯• {i} æˆåŠŸ")
        else:
            print(f"âŒ çº¯æ–‡æœ¬æµ‹è¯• {i} å¤±è´¥")
    
    # 4. ç®€åŒ–çš„VQAæµ‹è¯•ï¼ˆé™åˆ¶ä¸º1å¼ å›¾ç‰‡ï¼‰
    print("\n" + "="*60)
    print("ğŸ–¼ï¸ ç®€åŒ–VQAæµ‹è¯•ï¼ˆå•å¼ å›¾ç‰‡ï¼‰")
    print("="*60)
    
    # åªæµ‹è¯•ä¸€å¼ å›¾ç‰‡
    test_image = "/data1/wangzhiye/data2/nuscenes/samples/CAM_FRONT/n015-2018-07-11-11-54-16+0800__CAM_FRONT__1531281439762460.jpg"
    test_questions = [
        "è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ",
        "æè¿°ä¸€ä¸‹è·¯é¢æƒ…å†µã€‚"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n--- VQAæµ‹è¯• {i} ---")
        answer = test_single_image_vqa(model, processor, test_image, question)
        
        if answer:
            print(f"âœ… VQAæµ‹è¯• {i} æˆåŠŸ")
        else:
            print(f"âŒ VQAæµ‹è¯• {i} å¤±è´¥")
    
    print("\n" + "="*60)
    print("ğŸ‰ ç®€åŒ–æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“:")
    print("âœ… æ¨¡å‹åŠ è½½: æˆåŠŸ")
    print("âœ… æ•°å€¼å¢å¼ºåŠŸèƒ½: å·¥ä½œæ­£å¸¸")
    print("âœ… çº¯æ–‡æœ¬æ¨ç†: æˆåŠŸ")
    print("ğŸ”„ å›¾åƒ-æ–‡æœ¬æ¨ç†: å·²å°è¯•ï¼ˆå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ï¼‰")

if __name__ == "__main__":
    print(">>> æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹ç»„ä»¶å·²æ³¨å†Œ")
    main()
