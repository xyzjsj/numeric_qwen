#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨æ…¢é€Ÿå›¾åƒå¤„ç†å™¨æµ‹è¯•å›¾åƒæ¨ç†
"""

import torch
from PIL import Image
import numpy as np
import os

# æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹ç»„ä»¶
from numeric_qwen2_5_vl import (
    NumericQwen2_5_VLConfig,
    NumericQwen2_5_VLForConditionalGeneration, 
    NumericQwen2_5_VLProcessor
)

print("ğŸ¯ ä½¿ç”¨æ…¢é€Ÿå›¾åƒå¤„ç†å™¨æµ‹è¯•å›¾åƒæ¨ç†")
print("=" * 60)

def create_test_image():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ"""
    # åˆ›å»º 224x224 çš„ç®€å•å›¾åƒ
    image = Image.new('RGB', (224, 224), color='red')
    
    # åœ¨å›¾åƒä¸Šæ·»åŠ ä¸€äº›ç®€å•çš„å½¢çŠ¶
    from PIL import ImageDraw
    draw = ImageDraw.Draw(image)
    draw.rectangle([50, 50, 174, 174], fill='blue')
    draw.ellipse([75, 75, 149, 149], fill='yellow')
    
    return image

def test_image_inference_with_slow_processor():
    """ä½¿ç”¨æ…¢é€Ÿå¤„ç†å™¨æµ‹è¯•å›¾åƒæ¨ç†"""
    try:
        print("ğŸ”§ åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨...")
        
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # åŠ è½½å¤„ç†å™¨ï¼Œå¼ºåˆ¶ä½¿ç”¨æ…¢é€Ÿå›¾åƒå¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output",
            use_fast=False  # å…³é”®ï¼šä½¿ç”¨æ…¢é€Ÿå¤„ç†å™¨
        )
        
        print("âœ… æ¨¡å‹å’Œæ…¢é€Ÿå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        image = create_test_image()
        print("âœ… æµ‹è¯•å›¾åƒåˆ›å»ºæˆåŠŸ")
        
        # æ„å»ºå¯¹è¯
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image,
                    },
                    {"type": "text", "text": "æè¿°è¿™ä¸ªå›¾åƒä¸­ä½ çœ‹åˆ°çš„å†…å®¹ã€‚"}
                ],
            }
        ]
        
        # åº”ç”¨chat template
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        print("ğŸ“ Chat templateåº”ç”¨æˆåŠŸ")
        print("Generated text preview:", text[:200] + "..." if len(text) > 200 else text)
        
        # å¤„ç†è¾“å…¥
        print("ğŸ–¼ï¸ å¤„ç†å›¾åƒè¾“å…¥...")
        image_inputs = processor(
            text=[text],
            images=[image],
            padding=True,
            return_tensors="pt"
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        image_inputs = image_inputs.to(model.device)
        print("âœ… è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"è¾“å…¥å½¢çŠ¶: {image_inputs.input_ids.shape}")
        print(f"å›¾åƒç‰¹å¾å½¢çŠ¶: {image_inputs.pixel_values.shape}")
        
        # ç”Ÿæˆ
        print("ğŸš€ å¼€å§‹ç”Ÿæˆ...")
        with torch.no_grad():
            generated_ids = model.generate(
                **image_inputs,
                max_new_tokens=128,
                do_sample=True,
                temperature=0.7
            )
        
        # æå–æ–°ç”Ÿæˆçš„tokens
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(image_inputs.input_ids, generated_ids)
        ]
        
        # è§£ç è¾“å‡º
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        print("âœ… å›¾åƒæ¨ç†æˆåŠŸ!")
        print("ğŸ‰ ç”Ÿæˆç»“æœ:")
        print("-" * 40)
        print(output_text[0])
        print("-" * 40)
        
        return True
        
    except Exception as e:
        print(f"âŒ å›¾åƒæ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_text_inference():
    """æµ‹è¯•æ–‡æœ¬æ¨ç†ä»¥ç¡®ä¿æ¨¡å‹åŸºæœ¬åŠŸèƒ½æ­£å¸¸"""
    try:
        print("\nğŸ“ æµ‹è¯•æ–‡æœ¬æ¨ç†...")
        
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output",
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output"
        )
        
        # çº¯æ–‡æœ¬å¯¹è¯
        messages = [
            {"role": "user", "content": "è®¡ç®— <num>25</num> + <num>37</num> = ?"}
        ]
        
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        inputs = processor(text=[text], return_tensors="pt")
        inputs = inputs.to(model.device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=64,
                do_sample=True,
                temperature=0.7
            )
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        print("âœ… æ–‡æœ¬æ¨ç†æˆåŠŸ!")
        print("ğŸ‰ è®¡ç®—ç»“æœ:")
        print("-" * 40)
        print(output_text[0])
        print("-" * 40)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬æ¨ç†å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•...")
    
    # å…ˆæµ‹è¯•æ–‡æœ¬æ¨ç†
    text_success = test_text_inference()
    
    # å†æµ‹è¯•å›¾åƒæ¨ç†
    image_success = test_image_inference_with_slow_processor()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æ–‡æœ¬æ¨ç†: {'æˆåŠŸ' if text_success else 'å¤±è´¥'}")
    print(f"âœ… å›¾åƒæ¨ç†: {'æˆåŠŸ' if image_success else 'å¤±è´¥'}")
    
    if text_success and image_success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å¯ä»¥æ­£å¸¸å¤„ç†æ–‡æœ¬å’Œå›¾åƒã€‚")
    elif text_success:
        print("âš ï¸ æ–‡æœ¬æ¨ç†æ­£å¸¸ï¼Œå›¾åƒæ¨ç†éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    else:
        print("âŒ æ¨¡å‹å­˜åœ¨åŸºç¡€é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥é…ç½®ã€‚")
