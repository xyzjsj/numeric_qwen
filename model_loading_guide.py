#!/usr/bin/env python3
"""
checkpoint-4250æ¨¡å‹åŠ è½½æŒ‡å—
æä¾›å¤šç§åŠ è½½æ–¹å¼ä¾›é€‰æ‹©
"""
import os
import torch
from PIL import Image

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

def load_numeric_enhanced_model():
    """
    æ–¹æ³•1ï¼šåŠ è½½å®Œæ•´çš„æ•°å€¼å¢å¼ºæ¨¡å‹
    é€‚ç”¨äºï¼šæ–‡æœ¬æ¨ç†ã€æ•°å€¼å¢å¼ºåŠŸèƒ½æµ‹è¯•
    """
    print("ğŸ”§ æ–¹æ³•1ï¼šåŠ è½½æ•°å€¼å¢å¼ºæ¨¡å‹")
    
    # æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
    import sys
    sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')
    
    from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        # è®¾ç½®chat templateï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not hasattr(processor, 'chat_template') or not processor.chat_template:
            official_chat_template = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
            processor.chat_template = official_chat_template
        
        print("âœ… æ•°å€¼å¢å¼ºæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"ğŸ“‹ æ¨¡å‹è®¾å¤‡: {model.device}")
        print(f"ğŸ“‹ æ¨¡å‹ç²¾åº¦: {model.dtype}")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        raise

def test_text_inference(model, processor):
    """æµ‹è¯•æ–‡æœ¬æ¨ç†åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬æ¨ç†åŠŸèƒ½...")
    
    # æµ‹è¯•æ•°å€¼å¢å¼º
    test_prompt = "è®¡ç®—è¿™äº›æ•°å€¼çš„è½¨è¿¹ï¼š<num>3.14</num>å’Œ<num>-2.5</num>"
    
    try:
        # æ„å»ºè¾“å…¥
        prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{test_prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = processor(
            text=[prompt],
            return_tensors="pt"
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ“ è¾“å…¥: {test_prompt}")
        print(f"ğŸ¯ è¾“å‡º: {response}")
        print("âœ… æ–‡æœ¬æ¨ç†æµ‹è¯•æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬æ¨ç†æµ‹è¯•å¤±è´¥: {e}")

def load_for_evaluation():
    """
    æ–¹æ³•2ï¼šä¸ºè¯„ä¼°ä¸“é—¨åŠ è½½æ¨¡å‹
    é€‚ç”¨äºï¼šcosine similarityè¯„ä¼°ã€æ€§èƒ½æµ‹è¯•
    """
    print("\nğŸ”§ æ–¹æ³•2ï¼šä¸ºè¯„ä¼°åŠ è½½æ¨¡å‹")
    
    try:
        model, processor = load_numeric_enhanced_model()
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        # ç¦ç”¨dropoutç­‰è®­ç»ƒç›¸å…³åŠŸèƒ½
        for module in model.modules():
            if hasattr(module, 'training'):
                module.training = False
        
        print("âœ… è¯„ä¼°æ¨¡å¼è®¾ç½®å®Œæˆ")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°æ¨¡å¼è®¾ç½®å¤±è´¥: {e}")
        raise

def quick_load_example():
    """å¿«é€ŸåŠ è½½ç¤ºä¾‹"""
    print("\nâš¡ å¿«é€ŸåŠ è½½ç¤ºä¾‹:")
    
    # æœ€ç®€å•çš„åŠ è½½æ–¹å¼
    model, processor = load_numeric_enhanced_model()
    
    # å¿«é€Ÿæµ‹è¯•
    test_text_inference(model, processor)
    
    return model, processor

def main():
    """æ¼”ç¤ºä¸åŒçš„åŠ è½½æ–¹å¼"""
    print("ğŸš€ checkpoint-4250æ¨¡å‹åŠ è½½æŒ‡å—")
    print("=" * 60)
    
    # æ–¹æ³•1ï¼šå®Œæ•´åŠ è½½
    model, processor = load_numeric_enhanced_model()
    
    # æµ‹è¯•æ–‡æœ¬åŠŸèƒ½
    test_text_inference(model, processor)
    
    # æ–¹æ³•2ï¼šè¯„ä¼°æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
    eval_model, eval_processor = load_for_evaluation()
    
    print("\nğŸ“‹ åŠ è½½å®Œæˆï¼å¯ç”¨åŠŸèƒ½:")
    print("   âœ… æ–‡æœ¬æ¨ç†")
    print("   âœ… æ•°å€¼å¢å¼ºå¤„ç†")
    print("   âœ… å¯¹è¯æ ¼å¼ç”Ÿæˆ")
    print("   âš ï¸ å›¾åƒæ¨ç†ï¼ˆéœ€è¦forwardæ–¹æ³•ä¿®å¤ï¼‰")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("   - æ–‡æœ¬ä»»åŠ¡ï¼šç›´æ¥ä½¿ç”¨ï¼Œæ€§èƒ½ä¼˜ç§€")
    print("   - æ•°å€¼å¢å¼ºï¼šå®Œå…¨æ”¯æŒ<num>æ ‡ç­¾")
    print("   - è¯„ä¼°æµ‹è¯•ï¼šä½¿ç”¨cosine similarityç­‰æŒ‡æ ‡")
    print("   - å›¾åƒä»»åŠ¡ï¼šç­‰å¾…åç»­ä¿®å¤æˆ–ä½¿ç”¨æ–‡æœ¬å›é€€")

if __name__ == "__main__":
    main()
