#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬çš„å›¾åƒTokenåŒ¹é…æµ‹è¯•
ç»•è¿‡è‡ªå®šä¹‰æ¨¡å‹çš„æ•°å€¼å¢å¼ºåŠŸèƒ½ï¼Œç›´æ¥è¿›è¡Œæ¨ç†
"""
import os
import torch
from PIL import Image
import json

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def test_inference_mode():
    """åœ¨çº¯æ¨ç†æ¨¡å¼ä¸‹æµ‹è¯•å›¾åƒTokenåŒ¹é…"""
    print("ğŸš€ æµ‹è¯•çº¯æ¨ç†æ¨¡å¼...")
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        # è®¾ç½®chat template
        official_chat_template = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
        
        processor.chat_template = official_chat_template
        
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = Image.new('RGB', (224, 224), color='blue')
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": test_image},
                    {"type": "text", "text": "è¿™å¼ å›¾ç‰‡æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"}
                ]
            }
        ]
        
        # åº”ç”¨chat template
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        print(f"ğŸ“ Chat templateåº”ç”¨æˆåŠŸ")
        
        # å¤„ç†è¾“å…¥
        image_inputs = processor(
            text=[text],
            images=[test_image],
            return_tensors="pt"
        )
        
        print(f"ğŸ“¦ è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š input_ids shape: {image_inputs.input_ids.shape}")
        print(f"ğŸ–¼ï¸ pixel_values shape: {image_inputs.pixel_values.shape}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device_inputs = {}
        for k, v in image_inputs.items():
            if hasattr(v, 'to'):
                device_inputs[k] = v.to(model.device)
            else:
                device_inputs[k] = v
        
        # æ–¹æ³•1ï¼šç›´æ¥è°ƒç”¨çˆ¶ç±»çš„forwardæ–¹æ³•ï¼ˆç»•è¿‡æˆ‘ä»¬çš„è‡ªå®šä¹‰forwardï¼‰
        print("\nğŸ”„ æ–¹æ³•1: ç›´æ¥è°ƒç”¨çˆ¶ç±»forward...")
        try:
            with torch.no_grad():
                # ç›´æ¥è°ƒç”¨Qwen2VLForConditionalGenerationçš„forwardæ–¹æ³•
                from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import Qwen2VLForConditionalGeneration
                
                # ä¸´æ—¶å°†æ¨¡å‹è½¬æ¢ä¸ºçˆ¶ç±»å®ä¾‹æ¥è°ƒç”¨forward
                parent_outputs = Qwen2VLForConditionalGeneration.forward(
                    model, 
                    **device_inputs
                )
                
                print("âœ… çˆ¶ç±»forwardè°ƒç”¨æˆåŠŸï¼")
                print(f"ğŸ“Š è¾“å‡ºlogits shape: {parent_outputs.logits.shape}")
                
                # å°è¯•ç”Ÿæˆ
                parent_generate_outputs = Qwen2VLForConditionalGeneration.generate(
                    model,
                    **device_inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
                
                # è§£ç å›ç­”
                generated_ids = parent_generate_outputs[0][device_inputs['input_ids'].shape[1]:]
                response = processor.decode(generated_ids, skip_special_tokens=True)
                
                print(f"ğŸ¯ çˆ¶ç±»æ¨¡å‹å›ç­”: {response}")
                print("âœ… æ–¹æ³•1æˆåŠŸï¼")
                
        except Exception as e:
            print(f"âŒ æ–¹æ³•1å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # æ–¹æ³•2ï¼šä¿®æ”¹æˆ‘ä»¬æ¨¡å‹çš„forwardè¡Œä¸º
        print("\nğŸ”„ æ–¹æ³•2: ä¿®æ”¹è‡ªå®šä¹‰æ¨¡å‹forwardè¡Œä¸º...")
        try:
            # æ¸…ç†device_inputsï¼Œç§»é™¤å¯èƒ½çš„é¢å¤–å‚æ•°
            clean_device_inputs = {}
            valid_params = ['input_ids', 'attention_mask', 'pixel_values', 'pixel_values_videos', 
                           'image_grid_thw', 'video_grid_thw', 'position_ids']
            
            for k, v in device_inputs.items():
                if k in valid_params:
                    clean_device_inputs[k] = v
            
            print(f"ğŸ§¹ æ¸…ç†åçš„è¾“å…¥å‚æ•°: {list(clean_device_inputs.keys())}")
            
            # æš‚æ—¶ç¦ç”¨æ•°å€¼å¢å¼ºåŠŸèƒ½
            original_forward = model.forward
            
            def simple_forward(self, **kwargs):
                """ç®€åŒ–çš„forwardæ–¹æ³•ï¼Œç›´æ¥è°ƒç”¨çˆ¶ç±»"""
                # ç§»é™¤æˆ‘ä»¬æ·»åŠ çš„è‡ªå®šä¹‰å‚æ•°
                clean_kwargs = {k: v for k, v in kwargs.items() 
                              if k not in ['numeric_values', 'numeric_positions']}
                
                # ç›´æ¥è°ƒç”¨çˆ¶ç±»forward
                return super(NumericQwen2_5_VLForConditionalGeneration, self).forward(**clean_kwargs)
            
            # ä¸´æ—¶æ›¿æ¢forwardæ–¹æ³•
            import types
            model.forward = types.MethodType(simple_forward, model)
            
            with torch.no_grad():
                outputs = model.generate(
                    **clean_device_inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
            
            # è§£ç å›ç­”
            generated_ids = outputs[0][clean_device_inputs['input_ids'].shape[1]:]
            response = processor.decode(generated_ids, skip_special_tokens=True)
            
            print(f"ğŸ¯ ä¿®æ”¹åæ¨¡å‹å›ç­”: {response}")
            print("âœ… æ–¹æ³•2æˆåŠŸï¼")
            
            # æ¢å¤åŸå§‹forwardæ–¹æ³•
            model.forward = original_forward
            
        except Exception as e:
            print(f"âŒ æ–¹æ³•2å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # ç¡®ä¿æ¢å¤åŸå§‹forwardæ–¹æ³•
            model.forward = original_forward
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_inference_mode()
