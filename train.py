#!/usr/bin/env python3
"""
æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹è®­ç»ƒè„šæœ¬

åŸºäºåŸç”ŸQwen2.5-VLæ¶æ„çš„ç«¯åˆ°ç«¯è®­ç»ƒ
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import sys
import torch
import swanlab  # æ›¿æ¢wandbä¸ºswanlab
from transformers import Trainer, set_seed
from transformers.trainer_utils import get_last_checkpoint

# æ·»åŠ å½“å‰ç›®å½•åˆ°path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from numeric_qwen2_5_vl import (
    NumericQwen2_5_VLConfig,
    NumericQwen2_5_VLProcessor,
    NumericQwen2_5_VLForConditionalGeneration
)
from training_config import (
    NumericTrainingArguments,
    NumericDataset,
    NumericDataCollator,
    create_model_and_processor,
    get_training_config,
    create_deepspeed_config,
    init_swanlab,
    log_model_info_to_swanlab
)
from swanlab_trainer import (
    create_swanlab_trainer,
    log_sample_data_to_swanlab,
    log_training_progress_to_swanlab
)


class NumericTrainer(Trainer):
    """
    è‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œæ”¯æŒæ•°å€¼å¢å¼ºåŠŸèƒ½
    """
    
    def __init__(self, numeric_loss_weight: float = 1.0, **kwargs):
        super().__init__(**kwargs)
        self.numeric_loss_weight = numeric_loss_weight
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        è‡ªå®šä¹‰æŸå¤±è®¡ç®—
        """
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs, return_dict=True)
        
        # è·å–æŸå¤±
        if hasattr(outputs, 'loss') and outputs.loss is not None:
            loss = outputs.loss
        else:
            # å¦‚æœæ¨¡å‹æ²¡æœ‰è¿”å›lossï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è®¡ç®—
            # è¿™ç§æƒ…å†µä¸‹é€šå¸¸æ„å‘³ç€æ²¡æœ‰ä¼ å…¥labels
            loss = torch.tensor(0.0, device=next(model.parameters()).device, requires_grad=True)
        
        return (loss, outputs) if return_outputs else loss


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    """
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è·å–è®­ç»ƒé…ç½®
    training_args = get_training_config(
        output_dir="/data1/wangzhiye/1a1a11/original/output",
        model_path="Qwen/Qwen2.5-VL-3B-Instruct",
        data_path="/data1/wangzhiye/LLaMA-Factory/data/6vqa_data_extracted_converted_numeric.json",
        val_data_path=None,
        test_data_path=None,
        image_folder=None,
        enable_swanlab=True,
        swanlab_project="qsinghua",
        swanlab_experiment="numeric_qwen2_5_vl_vqa_v1",
        exp_name="numeric_qwen2_5_vl_vqa_v1"
    )
    
    print("è®­ç»ƒé…ç½®:")
    print(f"- è¾“å‡ºç›®å½•: {training_args.output_dir}")
    print(f"- æ¨¡å‹è·¯å¾„: {training_args.model_name_or_path}")
    print(f"- è®­ç»ƒæ•°æ®è·¯å¾„: {training_args.data_path}")
    print(f"- éªŒè¯æ•°æ®è·¯å¾„: {training_args.val_data_path}")
    print(f"- æµ‹è¯•æ•°æ®è·¯å¾„: {training_args.test_data_path}")
    print(f"- æ•°å€¼æŸå¤±æƒé‡: {training_args.numeric_loss_weight}")
    print(f"- SwanLabé¡¹ç›®: {training_args.swanlab_project}")
    print(f"- SwanLabå®éªŒ: {training_args.swanlab_experiment}")
    print("- è¯„ä¼°ç­–ç•¥: {training_args.eval_strategy}")
    if training_args.eval_strategy != "no":
        print(f"- è¯„ä¼°æ­¥æ•°: {training_args.eval_steps}")
    
    # åˆå§‹åŒ–SwanLab
    swanlab_run = None
    if training_args.enable_swanlab:
        print("\næ­£åœ¨åˆå§‹åŒ–SwanLab...")
        swanlab_run = init_swanlab(training_args)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(training_args.output_dir, exist_ok=True)
    
    # åˆ›å»ºDeepSpeedé…ç½®
    deepspeed_config = create_deepspeed_config(
        output_file=os.path.join(training_args.output_dir, "ds_config.json"),
        gradient_accumulation_steps=training_args.gradient_accumulation_steps
    )
    training_args.deepspeed = deepspeed_config
    
    # åˆ›å»ºæ¨¡å‹å’Œå¤„ç†å™¨
    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨...")
    model, processor = create_model_and_processor(
        model_path=training_args.model_name_or_path,
        numeric_config={
            'numeric_embedding_dim': 512,
            'numeric_token': '<num>',
            'numeric_loss_weight': training_args.numeric_loss_weight
        }
    )
    
    print(f"æ¨¡å‹é…ç½®: {model.config}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.num_parameters():,}")
    
    # è®°å½•æ¨¡å‹ä¿¡æ¯åˆ°SwanLab
    if swanlab_run is not None:
        log_model_info_to_swanlab(swanlab_run, model, processor)
    
    # åˆ›å»ºæ•°æ®é›†
    print("æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®é›†...")
    train_dataset = NumericDataset(
        data_path=training_args.data_path,
        processor=processor,
        image_folder=training_args.image_folder,
        max_length=8192  # å¢åŠ åˆ°8192é¿å…å›¾åƒtokenè¢«æˆªæ–­
    )
    print(f"åŠ è½½äº† {len(train_dataset)} æ¡è®­ç»ƒæ•°æ®")
    
    # åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆå¦‚æœæŒ‡å®šäº†ï¼‰
    eval_dataset = None
    if training_args.val_data_path:
        print("æ­£åœ¨åŠ è½½éªŒè¯æ•°æ®é›†...")
        eval_dataset = NumericDataset(
            data_path=training_args.val_data_path,
            processor=processor,
            image_folder=training_args.image_folder,
            max_length=8192
        )
        print(f"åŠ è½½äº† {len(eval_dataset)} æ¡éªŒè¯æ•°æ®")
    else:
        print("æœªæŒ‡å®šéªŒè¯æ•°æ®é›†")
    
    # è®°å½•æµ‹è¯•æ•°æ®é›†ä¿¡æ¯ï¼ˆä¸åŠ è½½ï¼Œåªè®°å½•è·¯å¾„ï¼‰
    if training_args.test_data_path:
        print(f"æµ‹è¯•æ•°æ®é›†è·¯å¾„: {training_args.test_data_path}")
    else:
        print("æœªæŒ‡å®šæµ‹è¯•æ•°æ®é›†")
    
    # åˆ›å»ºæ•°æ®æ•´ç†å™¨
    data_collator = NumericDataCollator(processor=processor)
    
    # è®°å½•æ•°æ®æ ·æœ¬åˆ°SwanLab
    if swanlab_run is not None:
        print("æ­£åœ¨è®°å½•æ•°æ®æ ·æœ¬åˆ°SwanLab...")
        log_sample_data_to_swanlab(swanlab_run, train_dataset, processor, num_samples=5)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹ç»­è®­
    last_checkpoint = None
    
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("æ­£åœ¨åˆ›å»ºSwanLabé›†æˆè®­ç»ƒå™¨...")
    trainer = create_swanlab_trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        swanlab_run=swanlab_run,
        processor=processor
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    try:
        # è®°å½•è®­ç»ƒå¼€å§‹ä¿¡æ¯åˆ°SwanLab
        if swanlab_run is not None:
            log_training_progress_to_swanlab(swanlab_run, {
                "training/dataset_size": len(train_dataset),
                "training/batch_size": training_args.per_device_train_batch_size,
                "training/gradient_accumulation_steps": training_args.gradient_accumulation_steps,
                "training/effective_batch_size": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,
                "training/total_epochs": training_args.num_train_epochs,
                "training/learning_rate": training_args.learning_rate,
                "training/vision_lr": training_args.vision_lr,
                "training/numeric_lr": training_args.numeric_lr,
            })
        
        if last_checkpoint is not None:
            trainer.train(resume_from_checkpoint=last_checkpoint)
        else:
            trainer.train()
        
        # è®­ç»ƒæˆåŠŸå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        
        # 1. ä¿å­˜æ¨¡å‹æœ¬èº«
        trainer.save_model()
        
        # 2. ä¿å­˜å¤„ç†å™¨ï¼ˆå®Œæ•´ä¿å­˜ï¼‰
        print("ä¿å­˜å¤„ç†å™¨...")
        processor.save_pretrained(training_args.output_dir)
        
        # 3. ä¿å­˜ç”Ÿæˆé…ç½®
        if hasattr(model, 'generation_config') and model.generation_config is not None:
            try:
                model.generation_config.save_pretrained(training_args.output_dir)
                print("âœ… generation_config ä¿å­˜æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ generation_config ä¿å­˜å¤±è´¥: {e}")
        
        # 4. åˆ›å»º/æ›´æ–° preprocessor_config.jsonï¼ˆç¡®ä¿å®Œæ•´ï¼‰
        import json
        preprocessor_config_path = os.path.join(training_args.output_dir, "preprocessor_config.json")
        preprocessor_config = {
            "processor_class": "NumericQwen2_5_VLProcessor",
            "auto_map": {
                "AutoProcessor": "numeric_qwen2_5_vl.NumericQwen2_5_VLProcessor"
            },
            "image_processor": {
                "do_convert_rgb": True,
                "do_normalize": True,
                "do_rescale": True,
                "do_resize": True,
                "image_mean": [0.48145466, 0.4578275, 0.40821073],
                "image_std": [0.26862954, 0.26130258, 0.27577711],
                "resample": 3,
                "size": {"shortest_edge": 336}
            },
            "image_processor_type": "Qwen2VLImageProcessor",  # æ·»åŠ è¿™ä¸ªå…³é”®å­—æ®µ
            "tokenizer": {
                "padding_side": "left",
                "truncation_side": "left",
                "model_max_length": 32768,
                "tokenizer_class": "Qwen2Tokenizer"
            },
            "num_token_id": 151665,
            "num_pad_token_id": 151666,
            "numeric_tokens": ["<num>", "<num_pad>"]
        }
        
        with open(preprocessor_config_path, 'w', encoding='utf-8') as f:
            json.dump(preprocessor_config, f, indent=2, ensure_ascii=False)
        print("âœ… preprocessor_config.json å·²æ›´æ–°")
        
        # 5. ä¿å­˜è®­ç»ƒçŠ¶æ€
        trainer.save_state()
        
        # 6. éªŒè¯ä¿å­˜ç»“æœ
        saved_files = os.listdir(training_args.output_dir)
        print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜çš„æ–‡ä»¶: {saved_files}")
        
        required_files = [
            "config.json",
            "added_tokens.json", 
            "special_tokens_map.json",
            "tokenizer_config.json",
            "preprocessor_config.json"
        ]
        
        missing_files = [f for f in required_files if f not in saved_files]
        if missing_files:
            print(f"âš ï¸ ç¼ºå¤±æ–‡ä»¶: {missing_files}")
        else:
            print("âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å·²ä¿å­˜")
        
        # è®°å½•è®­ç»ƒå®Œæˆä¿¡æ¯åˆ°SwanLab
        if swanlab_run is not None:
            log_training_progress_to_swanlab(swanlab_run, {
                "training/status": 2.0,  # 2.0è¡¨ç¤ºæˆåŠŸå®Œæˆ
                "training/model_saved": 1.0
            })
            print("âœ… è®­ç»ƒä¿¡æ¯å·²å®Œæ•´è®°å½•åˆ°SwanLab")
            
    except KeyboardInterrupt:
        print("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        # è®°å½•ä¸­æ–­ä¿¡æ¯åˆ°SwanLab
        if swanlab_run is not None:
            log_training_progress_to_swanlab(swanlab_run, {
                "training/status": 0.0,  # 0.0è¡¨ç¤ºä¸­æ–­
                "training/interrupted": 1.0
            })
        # å³ä½¿ä¸­æ–­ä¹Ÿå°è¯•ä¿å­˜å½“å‰çŠ¶æ€
        print("å°è¯•ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€...")
        try:
            trainer.save_state()
        except:
            print("ä¿å­˜è®­ç»ƒçŠ¶æ€å¤±è´¥")
            
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        # è®°å½•é”™è¯¯ä¿¡æ¯åˆ°SwanLab
        if swanlab_run is not None:
            log_training_progress_to_swanlab(swanlab_run, {
                "training/status": -1.0,  # -1.0è¡¨ç¤ºå¤±è´¥
                "training/error_occurred": 1.0
            })
        # å³ä½¿å‡ºé”™ä¹Ÿå°è¯•ä¿å­˜å½“å‰çŠ¶æ€
        print("å°è¯•ä¿å­˜å½“å‰è®­ç»ƒçŠ¶æ€...")
        try:
            trainer.save_state()
        except:
            print("ä¿å­˜è®­ç»ƒçŠ¶æ€å¤±è´¥")
        raise
    
    print(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {training_args.output_dir}")
    
    # è¾“å‡ºSwanLabé“¾æ¥
    if swanlab_run is not None:
        print(f"ğŸ”— æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹: https://swanlab.cn/{training_args.swanlab_project}")
        print(f"ğŸ“Š å®éªŒåç§°: {training_args.swanlab_experiment}")
        try:
            swanlab_run.finish()
        except:
            pass

if __name__ == "__main__":
    main()
