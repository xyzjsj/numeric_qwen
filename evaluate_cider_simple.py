#!/usr/bin/env python3
"""
CIDErè¯„ä¼°è„šæœ¬ - NumericQwen2.5-VLæ¨¡å‹
ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬æ ¼å¼
"""

import os
import sys
import json
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from typing import List, Dict
from transformers import AutoTokenizer
import re
from collections import Counter

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æœ¬åœ°æ¨¡å—
sys.path.append('/data1/wangzhiye/1a1a11/original')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹ç»„ä»¶ - ä½¿ç”¨è®­ç»ƒæ—¶çš„æ¨¡å—
from numeric_qwen2_5_vl import (
    NumericQwen2_5_VLConfig, 
    NumericQwen2_5_VLForConditionalGeneration, 
    NumericQwen2_5_VLProcessor
)


class CIDErScorer:
    """
    CIDEr (Consensus-based Image Description Evaluation) è¯„åˆ†å™¨
    """
    
    def __init__(self, n: int = 4, sigma: float = 6.0):
        """
        åˆå§‹åŒ–CIDErè¯„åˆ†å™¨
        
        Args:
            n: n-gramçš„æœ€å¤§é•¿åº¦
            sigma: é«˜æ–¯æƒé‡çš„æ ‡å‡†å·®
        """
        self.n = n
        self.sigma = sigma
        self.document_frequency = {}
        
    def compute_cider(self, candidate: str, refs: List[str]) -> float:
        """
        è®¡ç®—å•ä¸ªå€™é€‰ç­”æ¡ˆçš„CIDEråˆ†æ•°
        
        Args:
            candidate: å€™é€‰ç­”æ¡ˆ
            refs: å‚è€ƒç­”æ¡ˆåˆ—è¡¨
            
        Returns:
            CIDEråˆ†æ•°
        """
        score = 0.0
        
        # å¯¹æ¯ä¸ªn-gramçº§åˆ«è®¡ç®—åˆ†æ•°
        for n in range(1, self.n + 1):
            # è·å–å€™é€‰ç­”æ¡ˆçš„n-gram
            candidate_ngrams = self._get_ngrams(candidate, n)
            candidate_counts = Counter(candidate_ngrams)
            
            # è®¡ç®—æ‰€æœ‰å‚è€ƒç­”æ¡ˆçš„n-gram
            ref_ngrams_list = []
            for ref in refs:
                ref_ngrams = self._get_ngrams(ref, n)
                ref_ngrams_list.append(Counter(ref_ngrams))
            
            # è®¡ç®—TF-IDFç›¸ä¼¼åº¦
            similarity = self._compute_tfidf_similarity(
                candidate_counts, ref_ngrams_list, n
            )
            
            # ç´¯åŠ å„çº§n-gramçš„åˆ†æ•°
            score += similarity
        
        # è¿”å›å¹³å‡åˆ†æ•°
        return score / self.n
    
    def _compute_tfidf_similarity(self, candidate_counts: Counter, 
                                  ref_counts_list: List[Counter], n: int) -> float:
        """
        è®¡ç®—TF-IDFç›¸ä¼¼åº¦
        """
        # è®¡ç®—å€™é€‰ç­”æ¡ˆçš„TF-IDFå‘é‡
        candidate_tfidf = self._compute_tfidf_vector(candidate_counts, n)
        
        # è®¡ç®—æ¯ä¸ªå‚è€ƒç­”æ¡ˆçš„TF-IDFå‘é‡
        ref_tfidf_list = []
        for ref_counts in ref_counts_list:
            ref_tfidf = self._compute_tfidf_vector(ref_counts, n)
            ref_tfidf_list.append(ref_tfidf)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for ref_tfidf in ref_tfidf_list:
            sim = self._cosine_similarity(candidate_tfidf, ref_tfidf)
            similarities.append(sim)
        
        # è¿”å›ä¸æ‰€æœ‰å‚è€ƒç­”æ¡ˆçš„å¹³å‡ç›¸ä¼¼åº¦
        return np.mean(similarities) if similarities else 0.0
    
    def _compute_tfidf_vector(self, ngram_counts: Counter, n: int) -> Dict[str, float]:
        """
        è®¡ç®—n-gramçš„TF-IDFå‘é‡
        """
        tfidf = {}
        total_ngrams = sum(ngram_counts.values())
        
        if total_ngrams == 0:
            return tfidf
        
        for ngram, count in ngram_counts.items():
            # è®¡ç®—TF (è¯é¢‘)
            tf = count / total_ngrams
            
            # è®¡ç®—IDF (é€†æ–‡æ¡£é¢‘ç‡)
            df = self.document_frequency.get((n, ngram), 1)
            total_docs = self.document_frequency.get(f'total_docs_{n}', 1)
            idf = np.log(total_docs / df)
            
            # è®¡ç®—TF-IDF
            tfidf[ngram] = tf * idf
        
        return tfidf
    
    def _cosine_similarity(self, vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        """
        # è·å–å…±åŒçš„è¯æ±‡
        common_words = set(vec1.keys()) & set(vec2.keys())
        
        if not common_words:
            return 0.0
        
        # è®¡ç®—ç‚¹ç§¯
        dot_product = sum(vec1[word] * vec2[word] for word in common_words)
        
        # è®¡ç®—å‘é‡çš„æ¨¡é•¿
        norm1 = np.sqrt(sum(val**2 for val in vec1.values()))
        norm2 = np.sqrt(sum(val**2 for val in vec2.values()))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _get_ngrams(self, text: str, n: int) -> List[str]:
        """
        æå–æ–‡æœ¬ä¸­çš„n-gram
        """
        text = self._preprocess_text(text)
        words = text.split()
        
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i+n])
            ngrams.append(ngram)
        
        return ngrams
    
    def _preprocess_text(self, text: str) -> str:
        """
        æ–‡æœ¬é¢„å¤„ç†
        """
        # è½¬å°å†™
        text = text.lower()
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s]', '', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()


def load_model_from_checkpoint(checkpoint_path: str):
    """
    ä»checkpointåŠ è½½æ¨¡å‹
    """
    print(f"ğŸ”„ ä» {checkpoint_path} åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)
    
    # åŠ è½½å¤„ç†å™¨
    processor = NumericQwen2_5_VLProcessor.from_pretrained(checkpoint_path)
    
    # ç¡®ä¿å›¾åƒå¤„ç†å™¨ä½¿ç”¨æ­£ç¡®çš„max_pixelsè®¾ç½®
    if hasattr(processor, 'image_processor'):
        processor.image_processor.max_pixels = 12845056
        print(f"ğŸ”§ è®¾ç½®å›¾åƒå¤„ç†å™¨max_pixelsä¸º: {processor.image_processor.max_pixels}")
    
    # åŠ è½½æ¨¡å‹
    model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
        checkpoint_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, processor, tokenizer


def load_test_dataset(data_path: str):
    """
    åŠ è½½æµ‹è¯•æ•°æ®é›†
    """
    print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®é›†: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… åŠ è½½äº† {len(data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return data


def generate_predictions(model, processor, tokenizer, test_data: List[Dict], num_samples: int = 3):
    """
    ä¸ºæµ‹è¯•æ•°æ®ç”Ÿæˆé¢„æµ‹ç»“æœï¼Œä½¿ç”¨ç®€åŒ–çš„è¾“å…¥æ ¼å¼
    """
    print(f"ğŸ”® ä¸º {num_samples} ä¸ªæ ·æœ¬ç”Ÿæˆé¢„æµ‹...")
    
    predictions = []
    references = []
    
    image_base_path = "/data1/wangzhiye/LLaMA-Factory/data"
    
    for i, sample in enumerate(tqdm(test_data[:num_samples], desc="ç”Ÿæˆé¢„æµ‹")):
        try:
            # è·å–æ¶ˆæ¯åˆ—è¡¨
            messages = sample['messages']
            
            # åŠ è½½å›¾åƒ
            image_list = []
            image_paths = sample.get('images', [])
            
            print(f"ğŸ“ æ ·æœ¬ {i} æœ‰ {len(image_paths)} å¼ å›¾ç‰‡")
            
            if len(image_paths) > 4:
                print(f"ğŸ“ æ ·æœ¬ {i} æœ‰ {len(image_paths)} å¼ å›¾ç‰‡ï¼Œå·²é™åˆ¶ä¸º 4 å¼ ")
                image_paths = image_paths[:4]  # é™åˆ¶ä¸º4å¼ å›¾ç‰‡
            
            for img_path in image_paths:
                full_img_path = os.path.join(image_base_path, img_path)
                if os.path.exists(full_img_path):
                    image = Image.open(full_img_path).convert('RGB')
                    print(f"ğŸ” æ ·æœ¬ {i} å›¾åƒ {len(image_list)}: {image.size} (WÃ—H)")
                    image_list.append(image)
                else:
                    print(f"âš ï¸ å›¾ç‰‡æœªæ‰¾åˆ°: {full_img_path}")
            
            # è·å–ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            user_messages = [msg for msg in messages if msg.get('role') == 'user']
            if user_messages:
                original_text = user_messages[-1]['content']
                
                print(f"ğŸ” æ ·æœ¬ {i} åŸå§‹è¾“å…¥: {str(original_text)[:100]}...")
                print(f"ğŸ” æ ·æœ¬ {i} å›¾ç‰‡æ•°é‡: {len(image_list)}")
                
                # ä½¿ç”¨æ­£ç¡®çš„å›¾åƒæ ‡è®° <|image_pad|>
                if isinstance(original_text, str):
                    # ç§»é™¤æ‰€æœ‰ç°æœ‰çš„å›¾åƒæ ‡è®°
                    text_without_images = original_text.replace('<image>', '').strip()
                    needed_image_count = len(image_list)
                    
                    print(f"ğŸ” æ ·æœ¬ {i} éœ€è¦å›¾åƒæ ‡è®°æ•°é‡: {needed_image_count}")
                    
                    # ä½¿ç”¨æ­£ç¡®çš„å›¾åƒæ ‡è®° <|image_pad|>
                    image_tokens = '<|image_pad|>' * needed_image_count
                    text_prompt = image_tokens + text_without_images
                    
                    print(f"ğŸ” æ ·æœ¬ {i} æœ€ç»ˆæ–‡æœ¬: {text_prompt[:200]}...")
                else:
                    # å¦‚æœä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢
                    text_content = str(original_text)
                    image_tokens = '<|image_pad|>' * len(image_list)
                    text_prompt = image_tokens + text_content
                    
            else:
                # æ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯çš„æƒ…å†µ
                text_prompt = '<|image_pad|>' * len(image_list) + "è¯·æè¿°è¿™äº›å›¾ç‰‡ã€‚"
                
            print(f"ğŸ’« æ ·æœ¬ {i} è°ƒç”¨å¤„ç†å™¨...")
            
            # ä½¿ç”¨å¤„ç†å™¨å¤„ç†è¾“å…¥ï¼Œæ˜ç¡®è®¾ç½®max_pixels
            inputs = processor(
                text=text_prompt,
                images=image_list if image_list else None,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_pixels=12845056  # æ˜ç¡®è®¾ç½®æœ€å¤§åƒç´ æ•°
            )
            
            print(f"ğŸ” æ ·æœ¬ {i} å¤„ç†å™¨å¤„ç†åçš„input_ids shape: {inputs['input_ids'].shape}")
            if 'pixel_values' in inputs:
                print(f"ğŸ” æ ·æœ¬ {i} pixel_values shape: {inputs['pixel_values'].shape}")
            
            # æ£€æŸ¥tokenized textä¸­çš„image tokenæ•°é‡
            input_ids = inputs['input_ids'][0].cpu().numpy()
            image_token_id = 151655  # <|image_pad|> çš„token id
            image_token_count = np.sum(input_ids == image_token_id)
            print(f"ğŸ” æ ·æœ¬ {i} tokenizedä¸­çš„å›¾åƒtokenæ•°é‡: {image_token_count}")
            
            # å¦‚æœå›¾åƒtokenæ•°é‡ä¸º0ï¼Œå°è¯•æ‰‹åŠ¨æ·»åŠ 
            if image_token_count == 0 and len(image_list) > 0:
                print(f"âš ï¸ æ ·æœ¬ {i} æ²¡æœ‰æ‰¾åˆ°å›¾åƒtokenï¼Œè·³è¿‡...")
                predictions.append("")
                references.append([""])
                continue
            
            # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
            device = next(model.parameters()).device
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            print(f"ğŸ’« æ ·æœ¬ {i} å¼€å§‹ç”Ÿæˆ...")
            
            # æ¸…ç†GPUå†…å­˜å’Œæ¨¡å‹ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ç”Ÿæˆé¢„æµ‹
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=False  # ç¦ç”¨ç¼“å­˜é¿å…å¹²æ‰°
                )
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            prediction = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            print(f"âœ… æ ·æœ¬ {i} é¢„æµ‹: {prediction[:100]}...")
            
            # è·å–å‚è€ƒç­”æ¡ˆ
            assistant_messages = [msg for msg in messages if msg.get('role') == 'assistant']
            reference = assistant_messages[0]['content'] if assistant_messages else ""
            
            predictions.append(prediction)
            references.append([reference])  # CIDEréœ€è¦å‚è€ƒç­”æ¡ˆåˆ—è¡¨æ ¼å¼
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
            import traceback
            print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            predictions.append("")
            references.append([""])
    
    return predictions, references


def generate_predictions_separately(model, processor, tokenizer, test_data: List[Dict], num_samples: int = 10):
    """
    ä¸ºæ¯ä¸ªæ ·æœ¬å•ç‹¬ç”Ÿæˆé¢„æµ‹ï¼Œç¡®ä¿å®Œå…¨çš„çŠ¶æ€éš”ç¦»
    """
    print(f"ğŸ”® ä¸º {num_samples} ä¸ªæ ·æœ¬å•ç‹¬ç”Ÿæˆé¢„æµ‹...")
    
    predictions = []
    references = []
    
    image_base_path = "/data1/wangzhiye/LLaMA-Factory/data"
    
    for i in range(min(num_samples, len(test_data))):
        print(f"\n=== å¤„ç†æ ·æœ¬ {i}/{num_samples} ===")
        
        try:
            sample = test_data[i]
            
            # å¼ºåˆ¶æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹çŠ¶æ€ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            model.eval()
            
            # è·å–æ¶ˆæ¯åˆ—è¡¨
            messages = sample['messages']
            
            # åŠ è½½å›¾åƒ
            image_list = []
            image_paths = sample.get('images', [])
            
            print(f"ğŸ“ æ ·æœ¬ {i} æœ‰ {len(image_paths)} å¼ å›¾ç‰‡")
            
            if len(image_paths) > 4:
                print(f"ğŸ“ æ ·æœ¬ {i} æœ‰ {len(image_paths)} å¼ å›¾ç‰‡ï¼Œå·²é™åˆ¶ä¸º 4 å¼ ")
                image_paths = image_paths[:4]  # é™åˆ¶ä¸º4å¼ å›¾ç‰‡
            
            for img_path in image_paths:
                full_path = os.path.join(image_base_path, img_path)
                if os.path.exists(full_path):
                    try:
                        image = Image.open(full_path).convert('RGB')
                        image_list.append(image)
                        print(f"ğŸ“· åŠ è½½å›¾ç‰‡: {img_path}, å°ºå¯¸: {image.size}")
                    except Exception as e:
                        print(f"âš ï¸ å›¾ç‰‡ {img_path} åŠ è½½å¤±è´¥: {e}")
                else:
                    print(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
            
            if not image_list:
                print(f"âš ï¸ æ ·æœ¬ {i} æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡")
                predictions.append("")
                references.append([""])
                continue
            
            # æ„å»ºæ–‡æœ¬è¾“å…¥ - ä½¿ç”¨æ ‡å‡†çš„å›¾åƒtokenå¤„ç†
            # æ³¨æ„ï¼šæˆ‘ä»¬çš„æ¨¡å‹åŒæ—¶æ”¯æŒå›¾åƒå’Œæ•°å€¼ï¼Œè¿™é‡Œå¤„ç†å›¾åƒè¾“å…¥
            
            # è·å–ç”¨æˆ·æ¶ˆæ¯
            user_message = ""
            reference = ""
            for msg in messages:
                if msg['role'] == 'user':
                    user_message = msg['content']
                elif msg['role'] == 'assistant':
                    reference = msg['content']
            
            # æ„å»ºè¾“å…¥æ–‡æœ¬ - ä½¿ç”¨Qwen2.5-VLçš„æ ‡å‡†æ ¼å¼
            # ä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ å›¾åƒtokenï¼Œè®©processorè‡ªåŠ¨å¤„ç†
            input_text = f"User: {user_message}\nAssistant:"
            
            print(f"ğŸ“ è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(input_text)}")
            print(f"ğŸ–¼ï¸ å›¾ç‰‡tokenæ•°: {len(image_list)}")
            print(f"ğŸ“ å®é™…å¤„ç†çš„å›¾ç‰‡æ•°é‡: {len(image_list)}")
            
            # ä½¿ç”¨processorå¤„ç†è¾“å…¥ - ç¡®ä¿å®Œå…¨é‡æ–°å¤„ç†
            with torch.no_grad():
                inputs = processor(
                    text=input_text,
                    images=image_list,
                    return_tensors="pt",
                    max_pixels=12845056
                )
                
                # ç§»åŠ¨åˆ°GPU
                inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
                
                print(f"ğŸ” è¾“å…¥å½¢çŠ¶æ£€æŸ¥:")
                for key, value in inputs.items():
                    if hasattr(value, 'shape'):
                        print(f"  {key}: {value.shape}")
                
                # æ£€æŸ¥image tokens
                input_ids = inputs['input_ids']
                # ä½¿ç”¨æ­£ç¡®çš„å›¾åƒtoken ID
                image_token_id = 151655  # <|image_pad|> token (è¿™æ˜¯æ­£ç¡®çš„å›¾åƒtoken)
                image_token_count = (input_ids == image_token_id).sum().item()
                print(f"ğŸ” è¾“å…¥ä¸­çš„å›¾åƒtokenæ•°é‡: {image_token_count}")
                
                # åŒæ—¶æ£€æŸ¥æ˜¯å¦æœ‰æ•°å€¼token (å¯èƒ½å’Œå›¾åƒtokenæ··ç”¨)
                num_token_id = 151665  # <num> token  
                num_pad_token_id = 151666  # <num_pad> token
                num_token_count = (input_ids == num_token_id).sum().item()
                num_pad_token_count = (input_ids == num_pad_token_id).sum().item()
                print(f"ğŸ” è¾“å…¥ä¸­çš„æ•°å€¼tokenæ•°é‡: <num>={num_token_count}, <num_pad>={num_pad_token_count}")
                
                # ç”Ÿæˆå“åº”
                print("ğŸš€ å¼€å§‹ç”Ÿæˆ...")
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=False,
                    temperature=0.0,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=False  # ç¦ç”¨ç¼“å­˜ä»¥é¿å…çŠ¶æ€æ±¡æŸ“
                )
                
                # è§£ç æ–°ç”Ÿæˆçš„tokens
                new_tokens = outputs[0][len(inputs['input_ids'][0]):]
                prediction = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()
                
                print(f"ğŸ¯ ç”Ÿæˆçš„é¢„æµ‹: {prediction[:100]}...")
                print(f"ğŸ¯ å‚è€ƒç­”æ¡ˆ: {reference[:100]}...")
            
            predictions.append(prediction)
            references.append([reference])  # CIDEréœ€è¦å‚è€ƒç­”æ¡ˆåˆ—è¡¨æ ¼å¼
            print(f"âœ… æ ·æœ¬ {i} å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
            import traceback
            print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            predictions.append("")
            references.append([""])
    
    print(f"\nâœ… æ€»å…±å¤„ç†äº† {len(predictions)} ä¸ªæ ·æœ¬")
    return predictions, references


def evaluate_with_cider(predictions: List[str], references: List[List[str]]) -> Dict:
    """
    ä½¿ç”¨CIDErè¯„ä¼°é¢„æµ‹ç»“æœ
    """
    print(f"ğŸ“Š è®¡ç®—CIDEråˆ†æ•°...")
    print(f"ğŸ” è®¡ç®— {len(predictions)} ä¸ªæ ·æœ¬çš„CIDEråˆ†æ•°...")
    
    # åˆå§‹åŒ–CIDErè¯„åˆ†å™¨
    scorer = CIDErScorer()
    
    # é¢„å¤„ç†å‚è€ƒç­”æ¡ˆä»¥è®¡ç®—æ–‡æ¡£é¢‘ç‡
    print("ğŸ”„ é¢„å¤„ç†å‚è€ƒç­”æ¡ˆ...")
    all_ngrams = {}
    
    for ref_list in tqdm(references, desc="å¤„ç†å‚è€ƒç­”æ¡ˆ"):
        for ref in ref_list:
            for n in range(1, scorer.n + 1):
                ngrams = scorer._get_ngrams(ref, n)
                for ngram in ngrams:
                    key = (n, ngram)
                    if key not in all_ngrams:
                        all_ngrams[key] = 0
                    all_ngrams[key] += 1
    
    # è®¾ç½®æ–‡æ¡£é¢‘ç‡
    for n in range(1, scorer.n + 1):
        scorer.document_frequency[f'total_docs_{n}'] = len(references)
    
    for key, freq in all_ngrams.items():
        scorer.document_frequency[key] = freq
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_ngrams)} ä¸ªå”¯ä¸€n-gram")
    
    # è®¡ç®—æ¯ä¸ªé¢„æµ‹çš„CIDEråˆ†æ•°
    scores = []
    for pred, ref_list in tqdm(zip(predictions, references), desc="è®¡ç®—CIDEr", total=len(predictions)):
        score = scorer.compute_cider(pred, ref_list)
        scores.append(score)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    min_score = np.min(scores)
    max_score = np.max(scores)
    
    results = {
        'cider_score': mean_score,
        'std': std_score,
        'min_score': min_score,
        'max_score': max_score,
        'scores': scores,
        'predictions': predictions,
        'references': references
    }
    
    return results


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸš€ CIDErè¯„ä¼°å™¨ - NumericQwen2.5-VL")
    print("=" * 60)
    
    # é…ç½®è·¯å¾„
    checkpoint_path = "/data1/wangzhiye/1a1a11/original/output/checkpoint-200"
    test_data_path = "/data1/wangzhiye/LLaMA-Factory/data/1bddx_test_converted1.json"
    
    # åŠ è½½æ¨¡å‹
    model, processor, tokenizer = load_model_from_checkpoint(checkpoint_path)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_dataset(test_data_path)
    
    # ç”Ÿæˆé¢„æµ‹ - å®Œæ•´æµ‹è¯•ä½†å•ç‹¬å¤„ç†æ¯ä¸ªæ ·æœ¬
    predictions, references = generate_predictions_separately(
        model, processor, tokenizer, test_data, num_samples=10  # æµ‹è¯•æ›´å¤šæ ·æœ¬
    )
    
    # è¯„ä¼°
    results = evaluate_with_cider(predictions, references)
    
    # æ‰“å°ç»“æœ
    print("=" * 60)
    print("ğŸ¯ è¯„ä¼°ç»“æœ:")
    print(f"   æ ·æœ¬æ•°é‡: {len(predictions)}")
    print(f"   CIDEråˆ†æ•°: {results['cider_score']:.4f}")
    print(f"   åˆ†æ•°èŒƒå›´: [{results['min_score']:.4f}, {results['max_score']:.4f}]")
    print(f"   åˆ†æ•°æ ‡å‡†å·®: {results['std']:.4f}")
    
    # ä¿å­˜ç»“æœ
    with open('cider_evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: cider_evaluation_results.json")
    
    # æ˜¾ç¤ºç¤ºä¾‹é¢„æµ‹ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç¤ºä¾‹é¢„æµ‹ç»“æœ:")
    
    for i, (pred, ref, score) in enumerate(zip(predictions[:3], references[:3], results['scores'][:3])):
        print(f"\næ ·æœ¬ {i+1}:")
        print(f"é¢„æµ‹: {pred}")
        print(f"å‚è€ƒ: {ref}")
        print(f"CIDEr: {score:.4f}")


if __name__ == "__main__":
    main()
