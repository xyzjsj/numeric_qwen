import os
import torch
from transformers import AutoTokenizer, AutoProcessor
from model.modeling_numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration
from training_config import get_training_config

def load_model_from_checkpoint(checkpoint_path, device="cuda"):
    """
    ä»æŒ‡å®šçš„checkpointè·¯å¾„åŠ è½½æ¨¡å‹
    
    Args:
        checkpoint_path: checkpointç›®å½•è·¯å¾„ï¼Œå¦‚ "/data1/wangzhiye/1a1a11/original/output/checkpoint-150"
        device: è®¾å¤‡ç±»å‹ï¼Œé»˜è®¤ä¸º "cuda"
    
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        processor: å¤„ç†å™¨
        tokenizer: åˆ†è¯å™¨
    """
    
    print(f"ğŸ”„ æ­£åœ¨ä» {checkpoint_path} åŠ è½½æ¨¡å‹...")
    
    # æ£€æŸ¥checkpointè·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpointè·¯å¾„ä¸å­˜åœ¨: {checkpoint_path}")
    
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = ["config.json", "model.safetensors", "trainer_state.json"]
    for file in required_files:
        file_path = os.path.join(checkpoint_path, file)
        if not os.path.exists(file_path):
            print(f"âš ï¸  è­¦å‘Š: {file} ä¸å­˜åœ¨äº {checkpoint_path}")
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“ åŠ è½½æ¨¡å‹é…ç½®...")
        
        # 2. åŠ è½½åˆ†è¯å™¨å’Œå¤„ç†å™¨ (ä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„)
        base_model_path = "Qwen/Qwen2.5-VL-3B-Instruct"
        print(f"ğŸ”¤ åŠ è½½åˆ†è¯å™¨ä»: {base_model_path}")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        
        print(f"ğŸ–¼ï¸  åŠ è½½å¤„ç†å™¨ä»: {base_model_path}")
        processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)
        
        # 3. åŠ è½½æ¨¡å‹ (ä»checkpoint)
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹ä»: {checkpoint_path}")
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # 4. ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if device != "auto":
            model = model.to(device)
        
        print(f"âœ… æ¨¡å‹æˆåŠŸåŠ è½½åˆ°è®¾å¤‡: {model.device}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
        
        # 5. åŠ è½½è®­ç»ƒçŠ¶æ€ä¿¡æ¯ (å¯é€‰)
        trainer_state_path = os.path.join(checkpoint_path, "trainer_state.json")
        if os.path.exists(trainer_state_path):
            import json
            with open(trainer_state_path, 'r') as f:
                trainer_state = json.load(f)
            print(f"ğŸ“ˆ è®­ç»ƒçŠ¶æ€ - å…¨å±€æ­¥æ•°: {trainer_state.get('global_step', 'Unknown')}")
            print(f"ğŸ“ˆ è®­ç»ƒçŠ¶æ€ - è½®æ¬¡: {trainer_state.get('epoch', 'Unknown')}")
        
        return model, processor, tokenizer
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
        raise e

def continue_training_from_checkpoint(checkpoint_path):
    """
    ä»checkpointç»§ç»­è®­ç»ƒ
    
    Args:
        checkpoint_path: checkpointç›®å½•è·¯å¾„
    """
    
    print(f"ğŸš€ å‡†å¤‡ä» {checkpoint_path} ç»§ç»­è®­ç»ƒ...")
    
    # è·å–è®­ç»ƒé…ç½®
    training_args = get_training_config(
        output_dir="/data1/wangzhiye/1a1a11/original/output",
        data_path="/data1/wangzhiye/LLaMA-Factory/data/3bddx_train_converted1_train.json",
        val_data_path="/data1/wangzhiye/LLaMA-Factory/data/3bddx_train_converted1_val.json",
        test_data_path="/data1/wangzhiye/LLaMA-Factory/data/1bddx_test_converted.json",
        image_folder="/data1/wangzhiye/LLaMA-Factory/data"
    )
    
    # åŠ è½½æ¨¡å‹
    model, processor, tokenizer = load_model_from_checkpoint(checkpoint_path)
    
    # è¿™é‡Œå¯ä»¥ç»§ç»­è®¾ç½®è®­ç»ƒå™¨å’Œç»§ç»­è®­ç»ƒ
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
    
    return model, processor, tokenizer, training_args

def test_model_inference(checkpoint_path, test_text="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"):
    """
    æµ‹è¯•ä»checkpointåŠ è½½çš„æ¨¡å‹æ¨ç†èƒ½åŠ›
    
    Args:
        checkpoint_path: checkpointè·¯å¾„
        test_text: æµ‹è¯•æ–‡æœ¬
    """
    
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†èƒ½åŠ›...")
    
    # åŠ è½½æ¨¡å‹
    model, processor, tokenizer = load_model_from_checkpoint(checkpoint_path)
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    try:
        # å‡†å¤‡è¾“å…¥
        inputs = tokenizer(test_text, return_tensors="pt").to(model.device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ“ è¾“å…¥: {test_text}")
        print(f"ğŸ“¤ è¾“å‡º: {response}")
        
        return response
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        return None

if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    
    # æ–¹æ¡ˆ1: ç›´æ¥åŠ è½½æ¨¡å‹
    checkpoint_path = "/data1/wangzhiye/1a1a11/original/output/checkpoint-150"
    
    # æ£€æŸ¥å®é™…å­˜åœ¨çš„checkpoint
    output_dir = "/data1/wangzhiye/1a1a11/original/output"
    available_checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
    print(f"ğŸ“‚ å¯ç”¨çš„checkpoints: {available_checkpoints}")
    
    if available_checkpoints:
        # ä½¿ç”¨æœ€æ–°çš„checkpoint
        latest_checkpoint = max(available_checkpoints, key=lambda x: int(x.split("-")[1]))
        checkpoint_path = os.path.join(output_dir, latest_checkpoint)
        print(f"ğŸ¯ ä½¿ç”¨æœ€æ–°çš„checkpoint: {checkpoint_path}")
        
        try:
            # åŠ è½½æ¨¡å‹
            model, processor, tokenizer = load_model_from_checkpoint(checkpoint_path)
            
            # æµ‹è¯•æ¨ç†
            test_model_inference(checkpoint_path, "å›¾ç‰‡ä¸­æœ‰å¤šå°‘ä¸ªæ•°å­—?")
            
            # å¦‚æœè¦ç»§ç»­è®­ç»ƒï¼Œå–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œ
            # continue_training_from_checkpoint(checkpoint_path)
            
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„checkpoint")