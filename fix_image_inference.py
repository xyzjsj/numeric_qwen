#!/usr/bin/env python3
"""
ä¸“é—¨ä¿®å¤å›¾åƒæ¨ç†çš„æ–¹æ¡ˆ
åŸºäºå‰é¢çš„æ‰€æœ‰æµ‹è¯•ç»“æœï¼Œåˆ›å»ºèƒ½æ­£å¸¸å¤„ç†å›¾åƒçš„ç‰ˆæœ¬
"""
import os
import torch
from PIL import Image
import json

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

class InferenceOnlyQwen2_5_VL(NumericQwen2_5_VLForConditionalGeneration):
    """
    æ¨ç†ä¸“ç”¨ç‰ˆæœ¬çš„æ¨¡å‹
    ç»•è¿‡æ•°å€¼å¢å¼ºçš„forwardï¼Œç›´æ¥è°ƒç”¨çˆ¶ç±»æ–¹æ³•
    """
    
    def forward(self, **kwargs):
        """
        æ¨ç†ä¸“ç”¨çš„forwardæ–¹æ³•
        ç›´æ¥è°ƒç”¨Qwen2VLForConditionalGenerationçš„forward
        """
        # ç§»é™¤æˆ‘ä»¬è‡ªå®šä¹‰çš„å‚æ•°
        clean_kwargs = {k: v for k, v in kwargs.items() 
                      if k not in ['numeric_values', 'numeric_positions']}
        
        # ç›´æ¥è°ƒç”¨çˆ¶ç±»çš„çˆ¶ç±»ï¼ˆå³åŸå§‹Qwen2VLForConditionalGenerationï¼‰çš„forward
        from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLForConditionalGeneration
        return Qwen2VLForConditionalGeneration.forward(self, **clean_kwargs)

def load_inference_model():
    """åŠ è½½æ¨ç†ä¸“ç”¨æ¨¡å‹"""
    print("ğŸ”§ åŠ è½½æ¨ç†ä¸“ç”¨æ¨¡å‹...")
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    # å…ˆç”¨æ™®é€šæ–¹å¼åŠ è½½
    base_model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
        checkpoint_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # è½¬æ¢ä¸ºæ¨ç†ä¸“ç”¨æ¨¡å‹
    inference_model = InferenceOnlyQwen2_5_VL.from_pretrained(
        checkpoint_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½å¤„ç†å™¨
    processor = NumericQwen2_5_VLProcessor.from_pretrained(
        checkpoint_path,
        trust_remote_code=True
    )
    
    # è®¾ç½®chat template
    official_chat_template = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
    
    processor.chat_template = official_chat_template
    
    print("âœ… æ¨ç†ä¸“ç”¨æ¨¡å‹åŠ è½½æˆåŠŸ")
    return inference_model, processor

def test_image_inference_fixed(model, processor):
    """æµ‹è¯•ä¿®å¤åçš„å›¾åƒæ¨ç†"""
    print("\nğŸ–¼ï¸ æµ‹è¯•ä¿®å¤åçš„å›¾åƒæ¨ç†...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = Image.new('RGB', (224, 224), color='red')
    
    # åœ¨å›¾åƒä¸Šæ·»åŠ ä¸€äº›ç‰¹å¾
    import numpy as np
    img_array = np.array(test_image)
    
    # æ·»åŠ è“è‰²å¯¹è§’çº¿
    for i in range(min(224, 224)):
        img_array[i, i] = [0, 0, 255]
    
    # æ·»åŠ ç»¿è‰²æ¡çº¹
    for i in range(0, 224, 30):
        img_array[:, i:i+5] = [0, 255, 0]
    
    test_image = Image.fromarray(img_array)
    
    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": test_image},
                    {"type": "text", "text": "è¿™å¼ å›¾ç‰‡æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿå›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆå›¾æ¡ˆï¼Ÿ"}
                ]
            }
        ]
        
        # åº”ç”¨chat template
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        print(f"ğŸ“ Chat templateåº”ç”¨æˆåŠŸ")
        
        # å¤„ç†è¾“å…¥
        image_inputs = processor(
            text=[text],
            images=[test_image],
            return_tensors="pt"
        )
        
        print(f"ğŸ“¦ è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š input_ids shape: {image_inputs.input_ids.shape}")
        print(f"ğŸ–¼ï¸ pixel_values shape: {image_inputs.pixel_values.shape}")
        
        # æ£€æŸ¥vision tokens
        decoded_text = processor.tokenizer.decode(image_inputs.input_ids[0], skip_special_tokens=False)
        image_pad_count = decoded_text.count('<|image_pad|>')
        print(f"ğŸ”¢ å›¾åƒpad tokenæ•°é‡: {image_pad_count}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device_inputs = {}
        for k, v in image_inputs.items():
            if hasattr(v, 'to'):
                device_inputs[k] = v.to(model.device)
            else:
                device_inputs[k] = v
        
        # ç”Ÿæˆå›ç­”
        print("ğŸš€ å¼€å§‹å›¾åƒæ¨ç†...")
        with torch.no_grad():
            outputs = model.generate(
                **device_inputs,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_p=0.8,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ¯ å›¾åƒæ¨ç†ç»“æœ: {response}")
        print("âœ… å›¾åƒæ¨ç†æˆåŠŸï¼")
        
        return response
        
    except Exception as e:
        print(f"âŒ å›¾åƒæ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def alternative_approach():
    """å¤‡é€‰æ–¹æ¡ˆï¼šæ‰‹åŠ¨æ„å»ºprompt"""
    print("\nğŸ”„ å¤‡é€‰æ–¹æ¡ˆï¼šæ‰‹åŠ¨æ„å»ºå›¾åƒprompt...")
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    try:
        # ä½¿ç”¨åŸå§‹æ¨¡å‹ä½†æ‰‹åŠ¨å¤„ç†
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        # åˆ›å»ºç®€å•å›¾åƒ
        test_image = Image.new('RGB', (224, 224), color='blue')
        
        # æ‰‹åŠ¨æ„å»ºpromptï¼ˆä¸ä½¿ç”¨chat templateï¼‰
        manual_prompt = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>è¿™å¼ å›¾ç‰‡æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ<|im_end|>\n<|im_start|>assistant\n"
        
        # æ‰‹åŠ¨å¤„ç†è¾“å…¥
        inputs = processor(
            text=[manual_prompt],
            images=[test_image],
            return_tensors="pt"
        )
        
        print(f"ğŸ“¦ æ‰‹åŠ¨è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š input_ids shape: {inputs.input_ids.shape}")
        
        # ä¸´æ—¶ä¿®æ”¹æ¨¡å‹çš„forwardæ–¹æ³•
        original_forward = model.forward
        
        def bypass_forward(self, **kwargs):
            """ç»•è¿‡æ•°å€¼å¢å¼ºçš„forward"""
            clean_kwargs = {k: v for k, v in kwargs.items() 
                          if k not in ['numeric_values', 'numeric_positions']}
            # ç›´æ¥è°ƒç”¨çˆ¶ç±»forward
            return super(NumericQwen2_5_VLForConditionalGeneration, self).forward(**clean_kwargs)
        
        import types
        model.forward = types.MethodType(bypass_forward, model)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶ç”Ÿæˆ
        device_inputs = {}
        for k, v in inputs.items():
            if hasattr(v, 'to'):
                device_inputs[k] = v.to(model.device)
            else:
                device_inputs[k] = v
        
        with torch.no_grad():
            outputs = model.generate(
                **device_inputs,
                max_new_tokens=100,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # æ¢å¤åŸå§‹forward
        model.forward = original_forward
        
        # è§£ç ç»“æœ
        generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ¯ å¤‡é€‰æ–¹æ¡ˆç»“æœ: {response}")
        print("âœ… å¤‡é€‰æ–¹æ¡ˆæˆåŠŸï¼")
        
        return response
        
    except Exception as e:
        print(f"âŒ å¤‡é€‰æ–¹æ¡ˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ ä¸“é—¨ä¿®å¤å›¾åƒæ¨ç†é—®é¢˜")
    print("=" * 60)
    
    # æ–¹æ¡ˆ1ï¼šæ¨ç†ä¸“ç”¨æ¨¡å‹
    try:
        inference_model, processor = load_inference_model()
        result1 = test_image_inference_fixed(inference_model, processor)
        
        if result1:
            print("\nğŸ‰ æ–¹æ¡ˆ1æˆåŠŸï¼šæ¨ç†ä¸“ç”¨æ¨¡å‹å·¥ä½œæ­£å¸¸ï¼")
        else:
            print("\nâš ï¸ æ–¹æ¡ˆ1å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ...")
            
    except Exception as e:
        print(f"\nâŒ æ–¹æ¡ˆ1åŠ è½½å¤±è´¥: {e}")
        print("ğŸ”„ å°è¯•å¤‡é€‰æ–¹æ¡ˆ...")
    
    # æ–¹æ¡ˆ2ï¼šå¤‡é€‰æ–¹æ¡ˆ
    result2 = alternative_approach()
    
    if result2:
        print("\nğŸ‰ æ–¹æ¡ˆ2æˆåŠŸï¼šå¤‡é€‰æ–¹æ¡ˆå·¥ä½œæ­£å¸¸ï¼")
    else:
        print("\nâŒ ä¸¤ä¸ªæ–¹æ¡ˆéƒ½å¤±è´¥äº†")
    
    print("\nğŸ“‹ æ€»ç»“:")
    if result1 or result2:
        print("âœ… å›¾åƒæ¨ç†é—®é¢˜å·²è§£å†³ï¼")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šåˆ›å»ºç»•è¿‡æ•°å€¼å¢å¼ºforwardçš„æ¨ç†æ¨¡å‹")
    else:
        print("âŒ å›¾åƒæ¨ç†é—®é¢˜ä»éœ€è¿›ä¸€æ­¥è°ƒè¯•")
        print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥æ¨¡å‹æƒé‡å’Œforwardæ–¹æ³•å®ç°")

if __name__ == "__main__":
    main()
