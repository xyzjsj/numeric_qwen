#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½
éªŒè¯å¤„ç†å™¨é…ç½®æ˜¯å¦æ­£ç¡®ä¿å­˜åˆ°æ£€æŸ¥ç‚¹ä¸­
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
import tempfile
from transformers import TrainingArguments
from training_config import create_model_and_processor
from swanlab_trainer import create_swanlab_trainer
import swanlab

def test_checkpoint_saving():
    """æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½"""
    print("å¼€å§‹æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½...")
    
    # åˆ›å»ºæ¨¡å‹å’Œå¤„ç†å™¨
    print("åˆ›å»ºæ¨¡å‹å’Œå¤„ç†å™¨...")
    model, processor = create_model_and_processor("qwen/Qwen2.5-VL-3B-Instruct")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºä¿å­˜æ£€æŸ¥ç‚¹
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=temp_dir,
            save_steps=1,  # æ¯1æ­¥ä¿å­˜ä¸€æ¬¡
            save_total_limit=1,
            logging_steps=1,
            max_steps=1,  # åªè®­ç»ƒ1æ­¥
            per_device_train_batch_size=1,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            report_to=None,  # ç¦ç”¨æŠ¥å‘Š
        )
        
        # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†
        class DummyDataset:
            def __len__(self):
                return 1
            
            def __getitem__(self, idx):
                return {
                    'input_ids': torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
                    'attention_mask': torch.tensor([1, 1, 1, 1, 1], dtype=torch.long),
                    'labels': torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
                    'numeric_values': torch.tensor([1.0], dtype=torch.float32),
                    'numeric_masks': torch.tensor([1], dtype=torch.bool),
                }
        
        train_dataset = DummyDataset()
        
        # åˆå§‹åŒ–SwanLabï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
        try:
            swanlab_run = swanlab.init(
                project="test-checkpoint-saving",
                experiment_name="test",
                mode="offline"
            )
        except:
            print("è­¦å‘Š: SwanLabåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨None")
            swanlab_run = None
        
    # åˆ›å»ºè®­ç»ƒå™¨
    print("åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = create_swanlab_trainer(
        model=model,
        args=training_args,
        tokenizer=processor.tokenizer,
        processor=processor,
        swanlab_run=swanlab_run
    )
    
    # éªŒè¯å¤„ç†å™¨æ˜¯å¦æ­£ç¡®é™„åŠ 
    if hasattr(trainer, 'processor') and trainer.processor is not None:
        print("âœ… å¤„ç†å™¨å·²æ­£ç¡®é™„åŠ åˆ°è®­ç»ƒå™¨")
    else:
        print("âš ï¸ å¤„ç†å™¨æœªæ­£ç¡®é™„åŠ åˆ°è®­ç»ƒå™¨")
    
    # åˆ›å»ºå¿…è¦çš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼ˆç”¨äºæµ‹è¯•ä¿å­˜ï¼‰
    print("åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
    from torch.optim import AdamW
    from transformers import get_linear_schedule_with_warmup
    
    optimizer = AdamW(model.parameters(), lr=1e-5)
    trainer.optimizer = optimizer
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    num_training_steps = 100  # å‡è®¾çš„æ€»æ­¥æ•°
    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=10,
        num_training_steps=num_training_steps
    )
    trainer.lr_scheduler = lr_scheduler
    
    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    trainer.state.global_step = 1
    trainer.state.epoch = 0.1
    
    # æ‰‹åŠ¨è§¦å‘æ£€æŸ¥ç‚¹ä¿å­˜
    print("è§¦å‘æ£€æŸ¥ç‚¹ä¿å­˜...")
    try:
        # æ‰‹åŠ¨åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        import os
        checkpoint_dir = os.path.join(training_args.output_dir, "checkpoint-test")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        print(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
        
        # 1. ä¿å­˜æ¨¡å‹é…ç½®å’Œæƒé‡
        print("ä¿å­˜æ¨¡å‹...")
        model.save_pretrained(checkpoint_dir)
        
        # 2. ä¿å­˜å¤„ç†å™¨
        print("ä¿å­˜å¤„ç†å™¨...")
        processor.save_pretrained(checkpoint_dir)
        
        # 3. æ‰‹åŠ¨åˆ›å»º preprocessor_config.json
        print("åˆ›å»º preprocessor_config.json...")
        import json
        preprocessor_config = {
            "processor_class": "NumericQwen2_5_VLProcessor",
            "auto_map": {
                "AutoProcessor": "numeric_qwen2_5_vl.NumericQwen2_5_VLProcessor"
            },
            "image_processor": {
                "do_convert_rgb": True,
                "do_normalize": True,
                "do_rescale": True,
                "do_resize": True,
                "image_mean": [0.48145466, 0.4578275, 0.40821073],
                "image_std": [0.26862954, 0.26130258, 0.27577711],
                "resample": 3,
                "size": {"shortest_edge": 336}
            },
            "image_processor_type": "Qwen2VLImageProcessor",  # æ·»åŠ è¿™ä¸ªå…³é”®å­—æ®µ
            "tokenizer": {
                "padding_side": "left",
                "truncation_side": "left",
                "model_max_length": 32768,
                "tokenizer_class": "Qwen2Tokenizer"
            },
            "num_token_id": 151665,
            "num_pad_token_id": 151666,
            "numeric_tokens": ["<num>", "<num_pad>"]
        }
        
        preprocessor_config_path = os.path.join(checkpoint_dir, "preprocessor_config.json")
        with open(preprocessor_config_path, 'w', encoding='utf-8') as f:
            json.dump(preprocessor_config, f, indent=2, ensure_ascii=False)
        print("âœ… preprocessor_config.json åˆ›å»ºæˆåŠŸ")
        
        # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
        if os.path.exists(checkpoint_dir):
            saved_files = os.listdir(checkpoint_dir)
            print(f"ğŸ“ ä¿å­˜çš„æ–‡ä»¶: {saved_files}")
            
            # æ£€æŸ¥å…³é”®æ–‡ä»¶
            required_files = [
                "config.json",
                "preprocessor_config.json"
            ]
            
            missing_files = [f for f in required_files if f not in saved_files]
            
            if missing_files:
                print(f"âš ï¸ ç¼ºå¤±å…³é”®æ–‡ä»¶: {missing_files}")
                return False
            else:
                print("âœ… æ‰€æœ‰å…³é”®æ–‡ä»¶éƒ½å·²ä¿å­˜")
                
                # æ£€æŸ¥ preprocessor_config.json å†…å®¹
                if os.path.exists(preprocessor_config_path):
                    with open(preprocessor_config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    print(f"ğŸ“‹ preprocessor_config.json å†…å®¹ç‰‡æ®µ: {list(config.keys())}")
                    
                    # éªŒè¯å…³é”®é…ç½®
                    if "num_token_id" in config and "num_pad_token_id" in config:
                        print("âœ… æ•°å€¼tokené…ç½®æ­£ç¡®")
                        print(f"num_token_id: {config['num_token_id']}")
                        print(f"num_pad_token_id: {config['num_pad_token_id']}")
                    else:
                        print("âš ï¸ æ•°å€¼tokené…ç½®ç¼ºå¤±")
                
                # æµ‹è¯•æ¨¡å‹åŠ è½½
                print("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½...")
                try:
                    from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor
                    
                    # åŠ è½½ä¿å­˜çš„æ¨¡å‹
                    loaded_model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
                        checkpoint_dir,
                        torch_dtype=torch.float16,
                        device_map="cpu",  # ä½¿ç”¨CPUä»¥èŠ‚çœå†…å­˜
                        trust_remote_code=True
                    )
                    
                    loaded_processor = NumericQwen2_5_VLProcessor.from_pretrained(
                        checkpoint_dir,
                        trust_remote_code=True
                    )
                    
                    print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
                    print(f"åŠ è½½çš„æ¨¡å‹ç±»å‹: {type(loaded_model)}")
                    print(f"åŠ è½½çš„å¤„ç†å™¨ç±»å‹: {type(loaded_processor)}")
                    
                    # éªŒè¯æ•°å€¼token
                    if hasattr(loaded_processor, 'num_token_id'):
                        print(f"âœ… æ•°å€¼token IDæ­£ç¡®: {loaded_processor.num_token_id}")
                    else:
                        print("âš ï¸ æ•°å€¼token IDç¼ºå¤±")
                    
                    return True
                    
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
                    return True  # ä¿å­˜æˆåŠŸï¼Œä½†åŠ è½½å¤±è´¥ä¸ç®—æµ‹è¯•å¤±è´¥
        else:
            print("âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨")
            return False
            
    except Exception as e:
        print(f"âŒ ä¿å­˜æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False        # æ£€æŸ¥æ£€æŸ¥ç‚¹ç›®å½•
        checkpoint_dirs = [d for d in os.listdir(temp_dir) if d.startswith("checkpoint-")]
        if not checkpoint_dirs:
            print("âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ç›®å½•")
            return False
        
        checkpoint_dir = os.path.join(temp_dir, checkpoint_dirs[0])
        print(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆè·³è¿‡safetensoræ–‡ä»¶ï¼‰
        expected_files = [
            "config.json",
            # "model.safetensors",  # è·³è¿‡ï¼Œæ–‡ä»¶å¤ªå¤§
            "preprocessor_config.json",  # å¤„ç†å™¨é…ç½®
            "special_tokens_map.json",   # ç‰¹æ®Šä»¤ç‰Œæ˜ å°„
            "tokenizer.json",            # åˆ†è¯å™¨
            "tokenizer_config.json",     # åˆ†è¯å™¨é…ç½®
        ]
        
        print("æ£€æŸ¥æ–‡ä»¶:")
        all_files_exist = True
        for file_name in expected_files:
            file_path = os.path.join(checkpoint_dir, file_name)
            if os.path.exists(file_path):
                print(f"  âœ… {file_name}")
            else:
                print(f"  âŒ {file_name} (ç¼ºå¤±)")
                all_files_exist = False
        
        # åˆ—å‡ºå®é™…å­˜åœ¨çš„æ–‡ä»¶
        actual_files = os.listdir(checkpoint_dir)
        print(f"\nå®é™…æ–‡ä»¶åˆ—è¡¨: {actual_files}")
        
        # æ£€æŸ¥å¤„ç†å™¨é…ç½®å†…å®¹
        preprocessor_config_path = os.path.join(checkpoint_dir, "preprocessor_config.json")
        if os.path.exists(preprocessor_config_path):
            import json
            with open(preprocessor_config_path, 'r') as f:
                config = json.load(f)
            print(f"âœ… å¤„ç†å™¨é…ç½®å·²ä¿å­˜ï¼ŒåŒ…å« {len(config)} ä¸ªé…ç½®é¡¹")
        else:
            print("âŒ å¤„ç†å™¨é…ç½®æ–‡ä»¶ç¼ºå¤±")
        
        # æµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆä»…åŠ è½½å¤„ç†å™¨ï¼Œè·³è¿‡æ¨¡å‹ï¼‰
        print("\næµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹...")
        try:
            from transformers import AutoProcessor
            loaded_processor = AutoProcessor.from_pretrained(checkpoint_dir)
            # loaded_model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)  # è·³è¿‡æ¨¡å‹åŠ è½½
            print("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")
            
            # éªŒè¯å¤„ç†å™¨åŠŸèƒ½
            test_text = "è¿™æ˜¯ä¸€ä¸ªæ•°å­— <num>8.5</num> çš„æµ‹è¯•"
            result = loaded_processor._process_text_with_numeric_tokens(test_text)
            print(f"âœ… å¤„ç†å™¨åŠŸèƒ½æ­£å¸¸: {result}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
            return False
        
        # æ¸…ç†SwanLab
        if swanlab_run:
            try:
                swanlab_run.finish()
            except:
                pass
        
        return all_files_exist

if __name__ == "__main__":
    success = test_checkpoint_saving()
    if success:
        print("\nğŸ‰ æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
    else:
        print("\nâŒ æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½æµ‹è¯•å¤±è´¥!")
