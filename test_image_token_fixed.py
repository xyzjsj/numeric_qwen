#!/usr/bin/env python3
"""
åŸºäºGitCodeåšå®¢æ–‡ç« è§£å†³æ–¹æ¡ˆçš„å›¾åƒTokenåŒ¹é…ä¿®å¤æµ‹è¯•
å‚è€ƒ: https://blog.gitcode.com/d41d68b8e2ccdd03c0c59a4ca19a517b.html
"""
import os
import torch
from PIL import Image
import numpy as np

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def load_checkpoint_model(checkpoint_path):
    """åŠ è½½checkpoint-4250æ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        print("âœ… æ•°å€¼å¢å¼ºæ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def create_optimal_image(target_resolution=(224, 224)):
    """
    åˆ›å»ºä¼˜åŒ–å°ºå¯¸çš„æµ‹è¯•å›¾åƒ
    æ ¹æ®GitCodeæ–‡ç« å»ºè®®ï¼Œé™åˆ¶å›¾åƒå°ºå¯¸ä»¥é¿å…Tokenæ•°é‡è¿‡å¤š
    """
    print(f"ğŸ¨ åˆ›å»ºä¼˜åŒ–åˆ†è¾¨ç‡å›¾åƒ: {target_resolution}")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•å›¾åƒ - å°å°ºå¯¸é¿å…Tokenè¿‡å¤š
    image = Image.new('RGB', target_resolution, color='lightblue')
    
    # æ·»åŠ ä¸€äº›ç®€å•çš„è§†è§‰å…ƒç´ 
    import numpy as np
    img_array = np.array(image)
    
    # æ·»åŠ ä¸€äº›é¢œè‰²æ¡çº¹
    height, width = img_array.shape[:2]
    for i in range(0, height, 20):
        img_array[i:i+10, :] = [255, 200, 100]  # æ©™è‰²æ¡çº¹
    
    for j in range(0, width, 30):
        img_array[:, j:j+15] = [100, 255, 100]  # ç»¿è‰²æ¡çº¹
    
    return Image.fromarray(img_array)

def calculate_image_tokens(image_size, patch_size=28):
    """
    è®¡ç®—å›¾åƒTokenæ•°é‡
    æ ¹æ®GitCodeæ–‡ç« : Tokenæ•°é‡ = max_pixels/(28*28)
    """
    width, height = image_size
    total_pixels = width * height
    tokens = total_pixels // (patch_size * patch_size)
    
    print(f"ğŸ“Š å›¾åƒå°ºå¯¸: {width}x{height}")
    print(f"ğŸ“Š æ€»åƒç´ æ•°: {total_pixels}")
    print(f"ğŸ“Š é¢„è®¡Tokenæ•°é‡: {tokens}")
    
    return tokens

def test_image_inference_with_token_control(model, processor, max_prompt_length=2048):
    """
    å¸¦Tokenæ§åˆ¶çš„å›¾åƒæ¨ç†æµ‹è¯•
    æ ¹æ®GitCodeæ–‡ç« å»ºè®®è°ƒæ•´å‚æ•°é¿å…Tokenä¸åŒ¹é…
    """
    print("\nğŸ”§ å¼€å§‹å›¾åƒTokenæ§åˆ¶æµ‹è¯•...")
    
    # æ–¹æ¡ˆ1: ä½¿ç”¨å°å°ºå¯¸å›¾åƒ
    print("\nğŸ“‹ æ–¹æ¡ˆ1: å°å°ºå¯¸å›¾åƒæµ‹è¯•")
    small_image = create_optimal_image((224, 224))
    small_tokens = calculate_image_tokens(small_image.size)
    
    if small_tokens < max_prompt_length // 4:  # ä¿ç•™3/4ç©ºé—´ç»™æ–‡æœ¬
        print("âœ… å°å›¾åƒTokenæ•°é‡åˆç†ï¼Œè¿›è¡Œæ¨ç†æµ‹è¯•...")
        test_vqa_inference(model, processor, small_image, "è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆé¢œè‰²?")
    else:
        print("âš ï¸ å°å›¾åƒTokenæ•°é‡ä»ç„¶è¿‡å¤š")
    
    # æ–¹æ¡ˆ2: æ›´å°å°ºå¯¸å›¾åƒ
    print("\nğŸ“‹ æ–¹æ¡ˆ2: è¶…å°å°ºå¯¸å›¾åƒæµ‹è¯•")
    tiny_image = create_optimal_image((112, 112))
    tiny_tokens = calculate_image_tokens(tiny_image.size)
    
    if tiny_tokens < max_prompt_length // 8:  # ä¿ç•™7/8ç©ºé—´ç»™æ–‡æœ¬
        print("âœ… è¶…å°å›¾åƒTokenæ•°é‡åˆç†ï¼Œè¿›è¡Œæ¨ç†æµ‹è¯•...")
        test_vqa_inference(model, processor, tiny_image, "æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å›¾æ¡ˆã€‚")
    else:
        print("âš ï¸ è¶…å°å›¾åƒTokenæ•°é‡ä»ç„¶è¿‡å¤š")

def test_vqa_inference(model, processor, image, question):
    """æ‰§è¡Œè§†è§‰é—®ç­”æ¨ç†"""
    try:
        print(f"ğŸ¤” é—®é¢˜: {question}")
        
        # ä½¿ç”¨æœ€ç®€å•æœ‰æ•ˆçš„æ ¼å¼ - åŸºäºcheck_tokens.pyçš„å‘ç°
        prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|image|>{question}<|im_end|>\n<|im_start|>assistant\n"
        
        print(f"ğŸ“ æ„å»ºpromptæˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(prompt)}")
        
        # å¤„ç†è¾“å…¥ - é™åˆ¶å‚æ•°ä»¥é¿å…Tokenè¿‡å¤š
        image_inputs = processor(
            text=[prompt], 
            images=[image], 
            return_tensors="pt",
            max_length=1024,  # é™åˆ¶æœ€å¤§é•¿åº¦
            truncation=True   # å…è®¸æˆªæ–­
        )
        
        print("ğŸ“¦ è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š è¾“å…¥å½¢çŠ¶: {image_inputs.input_ids.shape}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒToken
        if hasattr(image_inputs, 'pixel_values') and image_inputs.pixel_values is not None:
            print(f"ğŸ–¼ï¸ å›¾åƒç‰¹å¾å½¢çŠ¶: {image_inputs.pixel_values.shape}")
        
        # è§£ç å¹¶æ£€æŸ¥Token
        decoded_text = processor.tokenizer.decode(image_inputs.input_ids[0], skip_special_tokens=False)
        image_token_count = decoded_text.count('<|image_pad|>')
        print(f"ğŸ”¢ å›¾åƒTokenæ•°é‡: {image_token_count}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡ - å®‰å…¨å¤„ç†æ‰€æœ‰ç±»å‹çš„æ•°æ®
        device_inputs = {}
        for k, v in image_inputs.items():
            if hasattr(v, 'to'):  # æ£€æŸ¥æ˜¯å¦æœ‰toæ–¹æ³•
                device_inputs[k] = v.to(model.device)
            else:
                device_inputs[k] = v  # ä¿æŒåŸæ ·
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = model.generate(
                **device_inputs,
                max_new_tokens=100,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ¯ å›ç­”: {response}")
        return response
        
    except Exception as e:
        print(f"âŒ VQAæ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_processor_configuration(processor):
    """æµ‹è¯•å¤„ç†å™¨é…ç½®å‚æ•°"""
    print("\nğŸ” æ£€æŸ¥å¤„ç†å™¨é…ç½®...")
    
    # æ£€æŸ¥å›¾åƒå¤„ç†å™¨é…ç½®
    if hasattr(processor, 'image_processor'):
        img_proc = processor.image_processor
        print("ğŸ–¼ï¸ å›¾åƒå¤„ç†å™¨é…ç½®:")
        
        # æ£€æŸ¥å…³é”®å‚æ•°
        if hasattr(img_proc, 'size'):
            print(f"   - é»˜è®¤å°ºå¯¸: {img_proc.size}")
        if hasattr(img_proc, 'max_pixels'):
            print(f"   - æœ€å¤§åƒç´ : {img_proc.max_pixels}")
        if hasattr(img_proc, 'min_pixels'):
            print(f"   - æœ€å°åƒç´ : {img_proc.min_pixels}")
    
    # æ£€æŸ¥åˆ†è¯å™¨é…ç½®
    if hasattr(processor, 'tokenizer'):
        tokenizer = processor.tokenizer
        print("ğŸ“ åˆ†è¯å™¨é…ç½®:")
        
        if hasattr(tokenizer, 'model_max_length'):
            print(f"   - æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")
        if hasattr(tokenizer, 'pad_token_id'):
            print(f"   - å¡«å……Token ID: {tokenizer.pad_token_id}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å›¾åƒTokenåŒ¹é…ä¿®å¤æµ‹è¯•")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„ - ä½¿ç”¨outputç›®å½•ä¸­çš„checkpoint-4250
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    try:
        # åŠ è½½æ¨¡å‹
        model, processor = load_checkpoint_model(checkpoint_path)
        
        # æ£€æŸ¥å¤„ç†å™¨é…ç½®
        test_processor_configuration(processor)
        
        # è¿›è¡ŒTokenæ§åˆ¶çš„å›¾åƒæ¨ç†æµ‹è¯•
        test_image_inference_with_token_control(model, processor)
        
        print("\nâœ… å›¾åƒTokenåŒ¹é…æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
