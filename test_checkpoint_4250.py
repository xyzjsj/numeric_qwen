#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„checkpoint-4250åŠ è½½åŠŸèƒ½ - åŒ…å«å›¾ç‰‡æµ‹è¯•
"""
import os

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
from PIL import Image
import json
from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def test_checkpoint_4250():
    """æµ‹è¯•checkpoint-4250åŠ è½½"""
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    print(f"ğŸ”„ æµ‹è¯•æ£€æŸ¥ç‚¹åŠ è½½: {checkpoint_path}")
    
    # 1. æ£€æŸ¥æ£€æŸ¥ç‚¹ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨: {checkpoint_path}")
        return False
    
    # 2. æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        "config.json",
        "preprocessor_config.json",
        "added_tokens.json",
        "special_tokens_map.json",
        "tokenizer_config.json"
    ]
    
    print(f"\nğŸ“ æ£€æŸ¥ç‚¹å†…å®¹:")
    files = os.listdir(checkpoint_path)
    print(f"æ–‡ä»¶åˆ—è¡¨: {sorted(files)}")
    
    missing_files = [f for f in required_files if f not in files]
    if missing_files:
        print(f"âš ï¸ ç¼ºå¤±æ–‡ä»¶: {missing_files}")
    else:
        print("âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å­˜åœ¨")
    
    # 3. æ£€æŸ¥ preprocessor_config.json å†…å®¹
    preprocessor_config_path = os.path.join(checkpoint_path, "preprocessor_config.json")
    if os.path.exists(preprocessor_config_path):
        import json
        with open(preprocessor_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"\nğŸ“‹ preprocessor_config.json å…³é”®å­—æ®µ:")
        print(f"- processor_class: {config.get('processor_class')}")
        print(f"- image_processor_type: {config.get('image_processor_type')}")
        print(f"- num_token_id: {config.get('num_token_id')}")
        print(f"- num_pad_token_id: {config.get('num_pad_token_id')}")
        print(f"- numeric_tokens: {config.get('numeric_tokens')}")
    else:
        print(f"âŒ preprocessor_config.json ä¸å­˜åœ¨")
        return False
    
    # 4. å°è¯•åŠ è½½æ¨¡å‹
    try:
        print(f"\nğŸ”„ åŠ è½½æ¨¡å‹...")
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.float16,  # ä½¿ç”¨float16ï¼ŒGPUä¸Šæ”¯æŒæ›´å¥½
            device_map="cuda",  # æ˜ç¡®ä½¿ç”¨GPU
            trust_remote_code=True
        )
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {type(model)}")
        
        # æ£€æŸ¥æ¨¡å‹çš„æ•°å€¼å¢å¼ºç»„ä»¶
        if hasattr(model, 'numeric_embedding'):
            print(f"âœ… æ•°å€¼åµŒå…¥å±‚å­˜åœ¨: {model.numeric_embedding}")
        else:
            print(f"âš ï¸ æ•°å€¼åµŒå…¥å±‚ç¼ºå¤±")
            
        if hasattr(model, 'regression_head'):
            print(f"âœ… å›å½’å¤´å­˜åœ¨: {model.regression_head}")
        else:
            print(f"âš ï¸ å›å½’å¤´ç¼ºå¤±")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 5. å°è¯•åŠ è½½å¤„ç†å™¨
    try:
        print(f"\nğŸ”„ åŠ è½½å¤„ç†å™¨...")
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        print(f"âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ: {type(processor)}")
        
        # æ£€æŸ¥æ•°å€¼token
        if hasattr(processor, 'num_token_id'):
            print(f"âœ… æ•°å€¼token ID: {processor.num_token_id}")
        else:
            print(f"âš ï¸ æ•°å€¼token IDç¼ºå¤±")
            
        if hasattr(processor, 'num_pad_token_id'):
            print(f"âœ… æ•°å€¼å¡«å……token ID: {processor.num_pad_token_id}")
        else:
            print(f"âš ï¸ æ•°å€¼å¡«å……token IDç¼ºå¤±")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
        return False
    
    # 6. æµ‹è¯•ç®€å•çš„æ–‡æœ¬å¤„ç†
    try:
        print(f"\nğŸ§ª æµ‹è¯•æ•°å€¼æ–‡æœ¬å¤„ç†...")
        test_cases = [
            "è¿™æ˜¯ä¸€ä¸ªåŒ…å«æ•°å­— <num><3.14> çš„æµ‹è¯•æ–‡æœ¬ã€‚",
            "ä½ç½®ä¿¡æ¯: (<num><+11.3>, <num><-4.0>)",
            "è½¨è¿¹ [PT, (<num><+3.41>, <num><-0.06>), (<num><+6.96>, <num><-0.20>)]"
        ]
        
        for i, test_text in enumerate(test_cases, 1):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i}:")
            print(f"åŸå§‹æ–‡æœ¬: {test_text}")
            
            # å¤„ç†æ–‡æœ¬
            result = processor._process_text_with_numeric_tokens(test_text)
            
            # æ£€æŸ¥è¿”å›å€¼ç±»å‹å¹¶æ­£ç¡®è§£æ
            if isinstance(result, tuple):
                processed_text, numeric_values = result
                print(f"å¤„ç†åæ–‡æœ¬: {processed_text}")
                print(f"æ•°å€¼åˆ—è¡¨: {numeric_values}")
                text_to_encode = processed_text
            elif isinstance(result, dict):
                processed_text = result['text']
                numeric_values = result['numeric_values']
                print(f"å¤„ç†åæ–‡æœ¬: {processed_text}")
                print(f"æ•°å€¼åˆ—è¡¨: {numeric_values}")
                text_to_encode = processed_text
            else:
                print(f"âš ï¸ æœªçŸ¥çš„è¿”å›ç±»å‹: {type(result)}")
                text_to_encode = test_text
            
            # ç¼–ç æ–‡æœ¬
            inputs = processor.tokenizer(
                text_to_encode,
                return_tensors="pt",
                padding=True,
                truncation=True
            )
            
            print(f"è¾“å…¥shape: {inputs['input_ids'].shape}")
        
        print(f"âœ… æ–‡æœ¬å¤„ç†æµ‹è¯•å…¨éƒ¨æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

    # 7. æµ‹è¯•å›¾ç‰‡å¤„ç†åŠŸèƒ½
    try:
        print(f"\nğŸ–¼ï¸ æµ‹è¯•å›¾ç‰‡å¤„ç†åŠŸèƒ½...")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾ç‰‡
        def create_test_image():
            """åˆ›å»ºæµ‹è¯•å›¾ç‰‡"""
            image = Image.new('RGB', (224, 224), color='white')
            from PIL import ImageDraw
            draw = ImageDraw.Draw(image)
            
            # ç”»ä¸€ä¸ªè“è‰²çŸ©å½¢
            draw.rectangle([50, 50, 150, 100], fill='blue')
            # ç”»ä¸€ä¸ªçº¢è‰²åœ†åœˆ
            draw.ellipse([100, 120, 180, 200], fill='red')
            # æ·»åŠ ä¸€äº›æ–‡å­—
            try:
                from PIL import ImageFont
                font = ImageFont.load_default()
                draw.text((60, 60), "Test", fill='white', font=font)
            except:
                draw.text((60, 60), "Test", fill='white')
            
            return image
        
        test_image = create_test_image()
        print(f"âœ… æµ‹è¯•å›¾ç‰‡åˆ›å»ºæˆåŠŸ: {test_image.size}")
        
        # æ‰‹åŠ¨æ„å»ºåŒ…å«å›¾ç‰‡çš„è¾“å…¥ï¼ˆè·³è¿‡chat templateï¼‰
        print(f"ğŸ”§ æµ‹è¯•å›¾ç‰‡å¤„ç†...")
        
        # æ„å»ºç®€å•çš„å›¾ç‰‡æè¿°ä»»åŠ¡çš„è¾“å…¥
        prompt_text = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>æè¿°è¿™ä¸ªå›¾ç‰‡ä¸­çš„å†…å®¹ã€‚<|im_end|>\n<|im_start|>assistant\n"
        
        print(f"ä½¿ç”¨æç¤ºæ–‡æœ¬: {prompt_text[:100]}...")
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[prompt_text],
            images=[test_image],
            padding=True,
            return_tensors="pt"
        )
        
        print(f"âœ… å›¾ç‰‡è¾“å…¥å¤„ç†æˆåŠŸ!")
        print(f"è¾“å…¥ID shape: {inputs.input_ids.shape}")
        print(f"å›¾ç‰‡ç‰¹å¾ shape: {inputs.pixel_values.shape}")
        
        # è®¡ç®—å›¾ç‰‡tokenæ•°é‡ï¼ˆ224x224å›¾ç‰‡åº”è¯¥äº§ç”Ÿ64ä¸ªtokensï¼‰
        expected_tokens = (224 * 224) // (28 * 28)  # æ¯ä¸ª28x28çš„patchä¸€ä¸ªtoken
        print(f"é¢„æœŸå›¾ç‰‡tokens: {expected_tokens}")
        print(f"å®é™…å›¾ç‰‡ç‰¹å¾æ•°é‡: {inputs.pixel_values.shape}")
        
        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        if hasattr(model, 'device'):
            inputs = inputs.to(model.device)
        
        # ç¡®ä¿è¾“å…¥ç²¾åº¦ä¸æ¨¡å‹ä¸€è‡´
        if 'pixel_values' in inputs:
            inputs.pixel_values = inputs.pixel_values.to(model.dtype)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­ï¼ˆä¸ç”Ÿæˆï¼Œåªæ£€æŸ¥æ˜¯å¦èƒ½æ­£å¸¸å¤„ç†ï¼‰
        print(f"ğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            try:
                # åªåšforwardï¼Œä¸ç”Ÿæˆ
                outputs = model(**inputs, labels=inputs.input_ids)
                print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ! Loss: {outputs.loss}")
            except Exception as forward_error:
                print(f"âš ï¸ å‰å‘ä¼ æ’­é‡åˆ°é—®é¢˜: {forward_error}")
                # å°è¯•ä¸ä½¿ç”¨labels
                try:
                    outputs = model(**inputs)
                    print(f"âœ… æ— labelså‰å‘ä¼ æ’­æˆåŠŸ!")
                except Exception as e2:
                    print(f"âŒ å‰å‘ä¼ æ’­å½»åº•å¤±è´¥: {e2}")
                    raise e2
        
        # å¦‚æœå‰å‘ä¼ æ’­æˆåŠŸï¼Œå°è¯•ç”Ÿæˆ
        print(f"ğŸš€ å°è¯•å›¾ç‰‡ç”Ÿæˆ...")
        with torch.no_grad():
            try:
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=32,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
                
                # è§£ç ç”Ÿæˆçš„å†…å®¹
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                output_text = processor.batch_decode(
                    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                )
                
                print(f"ğŸ‰ å›¾ç‰‡ç”ŸæˆæˆåŠŸ!")
                print(f"=" * 50)
                print(f"ç”Ÿæˆç»“æœ: {output_text[0]}")
                print(f"=" * 50)
                
            except Exception as gen_error:
                print(f"âš ï¸ ç”Ÿæˆå¤±è´¥ï¼Œä½†å‰å‘ä¼ æ’­æˆåŠŸ: {gen_error}")
                print(f"âœ… æ¨¡å‹å¯ä»¥å¤„ç†å›¾ç‰‡è¾“å…¥ï¼Œç”ŸæˆåŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        
        print(f"âœ… å›¾ç‰‡å¤„ç†æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ å›¾ç‰‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print(f"âš ï¸ å›¾ç‰‡åŠŸèƒ½æœ‰é—®é¢˜ï¼Œä½†æ–‡æœ¬åŠŸèƒ½æ­£å¸¸")
        # ä¸è¿”å›Falseï¼Œå› ä¸ºæ–‡æœ¬åŠŸèƒ½æ˜¯æ­£å¸¸çš„
    
    print(f"\nğŸ‰ checkpoint-4250 åŠ è½½æµ‹è¯•å®Œå…¨æˆåŠŸï¼")
    print(f"âœ… æ¨¡å‹å¯ä»¥æ­£å¸¸ç”¨äºæ¨ç†æˆ–ç»§ç»­è®­ç»ƒ")
    return True

if __name__ == "__main__":
    print(">>> æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹ç»„ä»¶å·²æ³¨å†Œ")
    
    success = test_checkpoint_4250()
    
    if success:
        print("\nğŸŠ æ­å–œï¼checkpoint-4250 å®Œå…¨å¯ç”¨ï¼")
    else:
        print("\nğŸ’¥ checkpoint-4250 å­˜åœ¨é—®é¢˜ï¼")
