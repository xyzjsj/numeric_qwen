#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„checkpoint-4250åŠ è½½åŠŸèƒ½
"""
import os
import torch
from PIL import Image

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def load_checkpoint_model(checkpoint_path):
    """åŠ è½½checkpoint-4250æ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        print("âœ… æ•°å€¼å¢å¼ºæ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def simple_inference_test(model, processor, image_path, question):
    """ç®€å•çš„æ¨ç†æµ‹è¯•"""
    try:
        print(f"ğŸ–¼ï¸ å¤„ç†å›¾åƒ: {image_path}")
        print(f"â“ é—®é¢˜: {question}")
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return ""
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        print("ğŸ”„ ç”Ÿæˆå›ç­”ä¸­...")
        
        # å°è¯•ä¸åŒçš„è¾“å…¥æ ¼å¼
        success = False
        
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨æ¶ˆæ¯æ ¼å¼
        try:
            messages = [
                {
                    "role": "user", 
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": question}
                    ]
                }
            ]
            
            text = processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = processor.process_vision_info(messages)
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            print("âœ“ ä½¿ç”¨æ ‡å‡†æ¶ˆæ¯æ ¼å¼")
            success = True
            
        except Exception as e1:
            print(f"âš ï¸ æ ‡å‡†æ ¼å¼å¤±è´¥: {e1}")
            
            # æ–¹æ³•2: ä½¿ç”¨ç®€å•æ ¼å¼
            try:
                text_prompt = f"<image>\n{question}"
                inputs = processor(
                    text=text_prompt,
                    images=image,
                    return_tensors="pt",
                    padding=True
                )
                print("âœ“ ä½¿ç”¨ç®€å•æ ¼å¼")
                success = True
                
            except Exception as e2:
                print(f"âš ï¸ ç®€å•æ ¼å¼å¤±è´¥: {e2}")
                
                # æ–¹æ³•3: æœ€åŸºç¡€æ ¼å¼
                try:
                    inputs = processor(
                        text=question,
                        images=image,
                        return_tensors="pt"
                    )
                    print("âœ“ ä½¿ç”¨åŸºç¡€æ ¼å¼")
                    success = True
                    
                except Exception as e3:
                    print(f"âŒ æ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥: {e3}")
                    return ""
        
        if not success:
            return ""
        
        # ç§»åŠ¨åˆ°GPU
        inputs = inputs.to("cuda")
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        answer = output_text[0].strip() if output_text else ""
        print(f"ğŸ¤– æ¨¡å‹å›ç­”: {answer}")
        
        return answer
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return ""

def test_numeric_capabilities(model, processor):
    """æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ§® æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›")
    print("="*60)
    
    # æµ‹è¯•æ•°å€¼æ–‡æœ¬å¤„ç†
    test_cases = [
        "è¿™ä¸ªè½¨è¿¹åŒ…å«åæ ‡ <num><3.14> å’Œ <num><-2.5>",
        "ä½ç½®ä¿¡æ¯: (<num><+11.3>, <num><-4.0>)",
        "è½¨è¿¹ [PT, (<num><+3.41>, <num><-0.06>), (<num><+6.96>, <num><-0.20>)]"
    ]
    
    for i, test_text in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•æ•°å€¼æ–‡æœ¬ {i}:")
        print(f"åŸå§‹: {test_text}")
        
        try:
            # æµ‹è¯•æ•°å€¼tokenå¤„ç†
            result = processor._process_text_with_numeric_tokens(test_text)
            
            if isinstance(result, tuple):
                processed_text, numeric_values = result
                print(f"å¤„ç†å: {processed_text}")
                print(f"æ•°å€¼: {numeric_values}")
            elif isinstance(result, dict):
                print(f"å¤„ç†å: {result['text']}")
                print(f"æ•°å€¼: {result['numeric_values']}")
            else:
                print(f"ç»“æœ: {result}")
                
        except Exception as e:
            print(f"âŒ æ•°å€¼å¤„ç†å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç®€å•æµ‹è¯•checkpoint-4250æ¨¡å‹")
    print("="*60)
    
    # é…ç½®è·¯å¾„
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    # 1. åŠ è½½æ¨¡å‹
    try:
        model, processor = load_checkpoint_model(checkpoint_path)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•: {e}")
        return
    
    # 2. æµ‹è¯•æ•°å€¼å¢å¼ºèƒ½åŠ›
    test_numeric_capabilities(model, processor)
    
    # 3. ç®€å•çš„VQAæµ‹è¯•
    print("\n" + "="*60)
    print("ğŸ–¼ï¸ ç®€å•VQAæ¨ç†æµ‹è¯•")
    print("="*60)
    
    # å‡†å¤‡ä¸€äº›ç®€å•çš„æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "image": "/data1/wangzhiye/data2/nuscenes/samples/CAM_FRONT/n015-2018-07-11-11-54-16+0800__CAM_FRONT__1531281439762460.jpg",
            "question": "æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚"
        },
        {
            "image": "/data1/wangzhiye/data2/nuscenes/samples/CAM_FRONT/n015-2018-07-11-11-54-16+0800__CAM_FRONT__1531281439762460.jpg", 
            "question": "è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆäº¤é€šå…ƒç´ ï¼Ÿ"
        },
        {
            "image": "/data1/wangzhiye/data2/nuscenes/samples/CAM_FRONT/n015-2018-07-11-11-54-16+0800__CAM_FRONT__1531281439762460.jpg",
            "question": "å¦‚æœè¦è§„åˆ’ä¸€æ¡è½¨è¿¹ï¼Œä½ ä¼šæ€ä¹ˆå»ºè®®ï¼Ÿ"
        }
    ]
    
    # è¿è¡Œæµ‹è¯•
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i} ---")
        answer = simple_inference_test(
            model, 
            processor, 
            test_case["image"], 
            test_case["question"]
        )
        
        if answer:
            print(f"âœ… æµ‹è¯•ç”¨ä¾‹ {i} æˆåŠŸ")
        else:
            print(f"âŒ æµ‹è¯•ç”¨ä¾‹ {i} å¤±è´¥")
    
    print("\n" + "="*60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*60)

if __name__ == "__main__":
    print(">>> æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹ç»„ä»¶å·²æ³¨å†Œ")
    main()
