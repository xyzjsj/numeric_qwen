#!/usr/bin/env python3
"""
æµ‹è¯•åŸç”ŸQwen2.5-VLæ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸å¤„ç†å›¾åƒToken
ç”¨äºè¯Šæ–­é—®é¢˜æ˜¯å¦åœ¨æˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å‹ä¸­
"""
import os
import torch
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import json

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

def test_native_qwen2_5_vl():
    """æµ‹è¯•åŸç”ŸQwen2.5-VLæ¨¡å‹"""
    print("ğŸ” æµ‹è¯•åŸç”ŸQwen2.5-VLæ¨¡å‹...")
    
    # ä½¿ç”¨æˆ‘ä»¬checkpointçš„é…ç½®ï¼Œä½†åŠ è½½åŸç”Ÿæ¨¡å‹ç±»
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    try:
        # ä½¿ç”¨åŸç”Ÿæ¨¡å‹ç±»åŠ è½½æˆ‘ä»¬çš„checkpoint
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="flash_attention_2" if torch.cuda.is_available() else "eager"
        )
        
        processor = AutoProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        # æ‰‹åŠ¨è®¾ç½®chat template
        official_chat_template = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
        
        processor.chat_template = official_chat_template
        
        print("âœ… åŸç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = Image.new('RGB', (224, 224), color='red')
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": test_image},
                    {"type": "text", "text": "è¿™å¼ å›¾ç‰‡æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"}
                ]
            }
        ]
        
        # åº”ç”¨chat template
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        print(f"ğŸ“ Chat templateåº”ç”¨æˆåŠŸï¼Œé•¿åº¦: {len(text)}")
        
        # å¤„ç†è¾“å…¥
        image_inputs = processor(
            text=[text],
            images=[test_image],
            return_tensors="pt",
            padding=True
        )
        
        print(f"ğŸ“¦ è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š input_ids shape: {image_inputs.input_ids.shape}")
        print(f"ğŸ–¼ï¸ pixel_values shape: {image_inputs.pixel_values.shape}")
        
        # æ£€æŸ¥tokenæ•°é‡
        decoded_text = processor.tokenizer.decode(image_inputs.input_ids[0], skip_special_tokens=False)
        image_pad_count = decoded_text.count('<|image_pad|>')
        print(f"ğŸ”¢ å›¾åƒpad tokenæ•°é‡: {image_pad_count}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device_inputs = {}
        for k, v in image_inputs.items():
            if hasattr(v, 'to'):
                device_inputs[k] = v.to(model.device)
            else:
                device_inputs[k] = v
        
        # æµ‹è¯•ç”Ÿæˆ
        print("ğŸš€ å¼€å§‹åŸç”Ÿæ¨¡å‹ç”Ÿæˆ...")
        with torch.no_grad():
            outputs = model.generate(
                **device_inputs,
                max_new_tokens=50,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ¯ åŸç”Ÿæ¨¡å‹å›ç­”: {response}")
        print("âœ… åŸç”Ÿæ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸç”Ÿæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_with_custom_model():
    """å¯¹æ¯”æˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å‹"""
    print("\nğŸ”„ åŠ è½½è‡ªå®šä¹‰æ•°å€¼å¢å¼ºæ¨¡å‹è¿›è¡Œå¯¹æ¯”...")
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    # æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
    import sys
    sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')
    
    try:
        from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor
        
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        print("âœ… è‡ªå®šä¹‰æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {type(model)}")
        print(f"ğŸ“‹ å¤„ç†å™¨ç±»å‹: {type(processor)}")
        
        # æ£€æŸ¥forwardæ–¹æ³•çš„ç­¾å
        import inspect
        forward_sig = inspect.signature(model.forward)
        print(f"ğŸ“‹ Forwardæ–¹æ³•å‚æ•°: {list(forward_sig.parameters.keys())}")
        
    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ è¯Šæ–­å›¾åƒTokenåŒ¹é…é—®é¢˜")
    print("=" * 60)
    
    # æµ‹è¯•åŸç”Ÿæ¨¡å‹
    native_success = test_native_qwen2_5_vl()
    
    # å¯¹æ¯”è‡ªå®šä¹‰æ¨¡å‹
    compare_with_custom_model()
    
    if native_success:
        print("\nâœ… è¯Šæ–­ç»“è®º: åŸç”Ÿæ¨¡å‹å·¥ä½œæ­£å¸¸ï¼Œé—®é¢˜å¯èƒ½åœ¨è‡ªå®šä¹‰æ¨¡å‹å®ç°ä¸­")
    else:
        print("\nâŒ è¯Šæ–­ç»“è®º: åŸç”Ÿæ¨¡å‹ä¹Ÿæœ‰é—®é¢˜ï¼Œå¯èƒ½æ˜¯é…ç½®æˆ–ç¯å¢ƒé—®é¢˜")

if __name__ == "__main__":
    main()
