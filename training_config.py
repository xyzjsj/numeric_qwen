#!/usr/bin/env python3
"""
æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹çš„è®­ç»ƒé…ç½®å’Œæ•°æ®å¤„ç†

åŸºäºåŸç”ŸQwen2.5-VLæ¶æ„çš„è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒ
"""

import os
import json
import torch
from typing import Dict, List, Optional, Union, Any
from dataclasses import dataclass, field
from transformers import TrainingArguments, AutoTokenizer
from torch.utils.data import Dataset
import copy
from PIL import Image
import swanlab

# å¯¼å…¥æˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å‹
from numeric_qwen2_5_vl import (
    NumericQwen2_5_VLConfig,
    NumericQwen2_5_VLProcessor,
    NumericQwen2_5_VLForConditionalGeneration
)


@dataclass
class NumericTrainingArguments(TrainingArguments):
    """
    æ•°å€¼å¢å¼ºè®­ç»ƒå‚æ•°
    """
    # æ•°å€¼ç›¸å…³å‚æ•°
    numeric_loss_weight: float = field(default=1.0, metadata={"help": "æ•°å€¼æŸå¤±æƒé‡"})
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    model_name_or_path: Optional[str] = field(default=None)
    
    # æ•°æ®ç›¸å…³å‚æ•°
    data_path: Optional[str] = field(default=None, metadata={"help": "è®­ç»ƒæ•°æ®è·¯å¾„"})
    val_data_path: Optional[str] = field(default=None, metadata={"help": "éªŒè¯æ•°æ®è·¯å¾„"})
    test_data_path: Optional[str] = field(default=None, metadata={"help": "æµ‹è¯•æ•°æ®è·¯å¾„"})
    image_folder: Optional[str] = field(default=None, metadata={"help": "å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„"})
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    vision_lr: Optional[float] = field(default=2e-6, metadata={"help": "è§†è§‰ç¼–ç å™¨å­¦ä¹ ç‡"})
    numeric_lr: Optional[float] = field(default=1e-4, metadata={"help": "æ•°å€¼å±‚å­¦ä¹ ç‡"})
    
    # SwanLabå¯è§†åŒ–å‚æ•°
    swanlab_project: Optional[str] = field(default="qsinghua", metadata={"help":  "SwanLabé¡¹ç›®åç§°"})
    swanlab_experiment: Optional[str] = field(default=None, metadata={"help":  "SwanLabå®éªŒåç§°"})
    enable_swanlab: bool = field(default=True, metadata={"help":  "æ˜¯å¦å¯ç”¨SwanLabå¯è§†åŒ–"})


class NumericDataset(Dataset):
    """
    æ•°å€¼å¢å¼ºçš„æ•°æ®é›†ç±»
    æ”¯æŒå¤šæ¨¡æ€æ•°æ®å’Œæ•°å€¼æ ‡æ³¨
    """
    
    def __init__(
        self,
        data_path: str,
        processor: NumericQwen2_5_VLProcessor,
        image_folder: Optional[str] = None,
        max_length: int = 8192
    ):
        self.processor = processor
        self.image_folder = image_folder
        self.max_length = max_length
        
        # åŠ è½½æ•°æ®
        if data_path.endswith('.json'):
            with open(data_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
        elif data_path.endswith('.jsonl'):
            self.data = []
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    self.data.append(json.loads(line.strip()))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_path}")
            
        print(f"åŠ è½½äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        return self._process_item(item)
    
    def _process_item(self, item: Dict) -> Dict:
        """
        å¤„ç†å•ä¸ªæ•°æ®é¡¹ - ä½¿ç”¨Qwen2.5-VLçš„æ ‡å‡†å¯¹è¯æ¨¡æ¿
        """
        # æ–°æ ¼å¼ï¼š{"images": [...], "messages": [...]}
        if 'messages' in item:
            messages = item.get('messages', [])
            images = item.get('images', [])
        else:
            # å…¼å®¹åŸæœ‰æ ¼å¼ï¼š{"conversations": [...], "image": "..."}
            conversations = item.get('conversations', [])
            images = [item.get('image')] if item.get('image') else []
            
            # è½¬æ¢ä¸ºæ ‡å‡†messagesæ ¼å¼
            messages = []
            for turn in conversations:
                role = turn.get('from', '')
                content = turn.get('value', '')
                
                if role == 'human':
                    messages.append({"role": "user", "content": content})
                elif role == 'gpt':
                    messages.append({"role": "assistant", "content": content})
        
        # å¤„ç†å›¾åƒ
        image_list = []
        if images:
            # é™åˆ¶å›¾åƒæ•°é‡ï¼Œé¿å…è¿‡å¤šå›¾åƒå¯¼è‡´å†…å­˜å’Œå¤„ç†é—®é¢˜
            max_images = 6  # æœ€å¤šä½¿ç”¨6å¼ å›¾åƒ
            for i, img_path in enumerate(images[:max_images]):
                if not img_path:
                    continue
                    
                # å¦‚æœæœ‰image_folderï¼Œåˆ™æ‹¼æ¥è·¯å¾„
                if self.image_folder and not os.path.isabs(img_path):
                    img_path = os.path.join(self.image_folder, img_path)
                
                try:
                    if os.path.exists(img_path):
                        image = Image.open(img_path).convert('RGB')
                        image_list.append(image)
                    else:
                        print(f"è­¦å‘Š: å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•åŠ è½½å›¾åƒ {img_path}: {e}")
        
        # ä½¿ç”¨Qwen2.5-VLæ ‡å‡†çš„å¤„ç†æ–¹å¼
        try:
            if image_list:
                # è½¬æ¢messagesæ ¼å¼ï¼Œç¡®ä¿åŒ…å«å›¾åƒä¿¡æ¯
                formatted_messages = []
                for msg in messages:
                    role = msg.get('role', '')
                    content = msg.get('content', '')
                    
                    if role == 'user' and '<image>' in content:
                        # å¦‚æœç”¨æˆ·æ¶ˆæ¯åŒ…å«<image>ï¼Œè½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
                        content_parts = []
                        # ä¸ºæ¯ä¸ªå›¾åƒæ·»åŠ å›¾åƒéƒ¨åˆ†
                        for i, _ in enumerate(image_list):
                            content_parts.append({"type": "image"})
                        
                        # æ·»åŠ æ–‡æœ¬éƒ¨åˆ†ï¼ˆç§»é™¤<image>æ ‡è®°ï¼‰
                        text_content = content.replace('<image>', '').strip()
                        if text_content:
                            content_parts.append({"type": "text", "text": text_content})
                        
                        formatted_messages.append({
                            "role": role,
                            "content": content_parts
                        })
                    else:
                        formatted_messages.append({
                            "role": role,
                            "content": content
                        })
                
                # ä½¿ç”¨tokenizerçš„apply_chat_template
                text = self.processor.tokenizer.apply_chat_template(
                    formatted_messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
                
                # ç„¶åä½¿ç”¨processorå¤„ç†
                processed = self.processor(
                    text=[text],
                    images=image_list,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.max_length
                )
            else:
                # å¯¹äºçº¯æ–‡æœ¬ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†
                text = self.processor.tokenizer.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=False
                )
                
                # å¤„ç†çº¯æ–‡æœ¬
                processed = self.processor(
                    text=[text],
                    images=None,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=self.max_length
                )
            
            # å°†tensorè½¬ä¸ºåˆé€‚çš„æ ¼å¼
            result = {}
            for key, value in processed.items():
                if isinstance(value, torch.Tensor):
                    # ç§»é™¤batchç»´åº¦
                    result[key] = value.squeeze(0)
                    
                    # ç‰¹æ®Šå¤„ç†image_grid_thw
                    if key == 'image_grid_thw':
                        if value.dim() == 0:
                            del result[key]
                        elif value.dim() == 1 and value.numel() == 0:
                            del result[key]
                        elif value.dim() == 1 and value.numel() == 3:
                            result[key] = value.unsqueeze(0)
                else:
                    result[key] = value
            
            # åˆ›å»ºlabels (ç”¨äºè¯­è¨€å»ºæ¨¡)
            if 'input_ids' in result:
                result['labels'] = result['input_ids'].clone()
                # è·å–å¤„ç†å™¨ç”Ÿæˆçš„æ•°å€¼æ ‡æ³¨å¹¶æ‰å¹³åŒ–
                numeric_values = result.get('numeric_values', [])
                if isinstance(numeric_values, list) and len(numeric_values) == 1:
                    numeric_values = numeric_values[0]
                numeric_positions = result.get('numeric_positions', [])
                if isinstance(numeric_positions, list) and len(numeric_positions) == 1:
                    numeric_positions = numeric_positions[0]
                # æŸ¥æ‰¾ <num_pad> çš„ token id
                num_pad_token_id = self.processor.tokenizer.convert_tokens_to_ids('<num_pad>')
                if num_pad_token_id is not None:
                    input_ids_list = result['input_ids'].tolist()
                    positions = [i for i, token_id in enumerate(input_ids_list) if token_id == num_pad_token_id]
                    # å¯¹é½æ•°å€¼ä¸ä½ç½®æ•°é‡
                    if numeric_values and positions:
                        min_len = min(len(numeric_values), len(positions))
                        numeric_values = numeric_values[:min_len]
                        numeric_positions = positions[:min_len]
                    else:
                        numeric_values = []
                        numeric_positions = []
                else:
                    numeric_values = []
                    numeric_positions = []
                result['numeric_values'] = numeric_values
                result['numeric_positions'] = numeric_positions

            return result
            
        except Exception as e:
            print(f"å¤„ç†æ•°æ®é¡¹æ—¶å‡ºé”™: {e}")
            # è¿”å›ä¸€ä¸ªæœ€å°çš„æœ‰æ•ˆæ ·æœ¬
            return {
                'input_ids': torch.tensor([self.processor.tokenizer.eos_token_id]),
                'labels': torch.tensor([self.processor.tokenizer.eos_token_id]),
                'attention_mask': torch.tensor([1])
            }


class NumericDataCollator:
    """
    æ•°å€¼å¢å¼ºçš„æ•°æ®æ•´ç†å™¨
    """
    
    def __init__(self, processor: NumericQwen2_5_VLProcessor, padding_side: str = "right"):
        self.processor = processor
        self.tokenizer = processor.tokenizer
        self.padding_side = padding_side
    
    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:
        """
        æ•´ç†æ‰¹æ¬¡æ•°æ®
        """
        batch = {}
        
        # æ”¶é›†æ‰€æœ‰key
        all_keys = set()
        for feature in features:
            all_keys.update(feature.keys())
        
        # å¤„ç†æ¯ä¸ªkey
        for key in all_keys:
            values = [feature.get(key) for feature in features if key in feature]
            
            if not values:
                continue
                
            if key in ['input_ids', 'labels', 'attention_mask']:
                # æ–‡æœ¬ç›¸å…³çš„tensoréœ€è¦padding
                batch[key] = self._pad_sequence(values, key)
            elif key.startswith('pixel_values'):
                # å›¾åƒæ•°æ®
                if all(v is not None for v in values):
                    batch[key] = torch.stack(values)
            elif key == 'image_grid_thw':
                # å›¾åƒç½‘æ ¼ç»´åº¦ä¿¡æ¯ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                if all(v is not None for v in values):
                    valid_values = []
                    for v in values:
                        if isinstance(v, torch.Tensor):
                            if v.dim() == 1:
                                v = v.unsqueeze(0)
                            elif v.dim() == 0:
                                continue
                            valid_values.append(v)
                    
                    if valid_values:
                        batch[key] = torch.cat(valid_values, dim=0)
                    else:
                        batch[key] = torch.empty(0, 3, dtype=torch.long)
            elif key in ['numeric_values', 'numeric_positions']:
                # æ•°å€¼ç›¸å…³çš„åˆ—è¡¨æ•°æ®
                batch[key] = values
            elif isinstance(values[0], torch.Tensor):
                try:
                    batch[key] = torch.stack(values)
                except:
                    batch[key] = values
            else:
                batch[key] = values
        
        return batch
    
    def _pad_sequence(self, sequences: List[torch.Tensor], key: str) -> torch.Tensor:
        """
        å¯¹åºåˆ—è¿›è¡Œpadding
        """
        if not sequences:
            return torch.tensor([])
        
        max_len = max(len(seq) for seq in sequences)
        
        if key == 'labels':
            pad_value = -100
        elif key == 'attention_mask':
            pad_value = 0
        else:  # input_ids
            pad_value = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id
        
        padded_sequences = []
        for seq in sequences:
            seq_len = len(seq)
            if seq_len < max_len:
                pad_len = max_len - seq_len
                if self.padding_side == "right":
                    padded = torch.cat([seq, torch.full((pad_len,), pad_value, dtype=seq.dtype)])
                else:
                    padded = torch.cat([torch.full((pad_len,), pad_value, dtype=seq.dtype), seq])
            else:
                padded = seq
            padded_sequences.append(padded)
        
        return torch.stack(padded_sequences)


def init_swanlab(config: NumericTrainingArguments) -> Optional[object]:
    """
    åˆå§‹åŒ–SwanLabå®éªŒè·Ÿè¸ª
    
    Args:
        config: è®­ç»ƒé…ç½®
        
    Returns:
        SwanLab runå¯¹è±¡æˆ–None
    """
    if not config.enable_swanlab:
        print("SwanLabå¯è§†åŒ–å·²ç¦ç”¨")
        return None
    
    try:
        experiment_name = config.swanlab_experiment
        if experiment_name is None:
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"numeric_qwen2_5_vl_{timestamp}"
        
        run = swanlab.init(
            project=config.swanlab_project,
            experiment_name=experiment_name,
            config={
                "model_name": config.model_name_or_path,
                "architecture": "Numeric-Qwen2.5-VL",
                "num_train_epochs": config.num_train_epochs,
                "per_device_train_batch_size": config.per_device_train_batch_size,
                "gradient_accumulation_steps": config.gradient_accumulation_steps,
                "learning_rate": config.learning_rate,
                "vision_lr": config.vision_lr,
                "numeric_lr": config.numeric_lr,
                "weight_decay": config.weight_decay,
                "warmup_ratio": config.warmup_ratio,
                "lr_scheduler_type": config.lr_scheduler_type,
                "numeric_loss_weight": config.numeric_loss_weight,
                "bf16": config.bf16,
                "tf32": config.tf32,
                "gradient_checkpointing": config.gradient_checkpointing,
                "data_path": config.data_path,
                "image_folder": config.image_folder,
                "output_dir": config.output_dir,
                "save_strategy": config.save_strategy,
                "save_steps": config.save_steps,
                "logging_steps": config.logging_steps,
            },
            description=f"æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹è®­ç»ƒå®éªŒ - {experiment_name}"
        )
        
        print(f"âœ… SwanLabå®éªŒå·²åˆå§‹åŒ–:")
        print(f"   é¡¹ç›®: {config.swanlab_project}")
        print(f"   å®éªŒ: {experiment_name}")
        print(f"   æŸ¥çœ‹é“¾æ¥: https://swanlab.cn/{config.swanlab_project}")
        
        return run
        
    except Exception as e:
        print(f"âš ï¸  SwanLabåˆå§‹åŒ–å¤±è´¥: {e}")
        print("   è®­ç»ƒå°†ç»§ç»­è¿›è¡Œï¼Œä½†ä¸ä¼šè®°å½•åˆ°SwanLab")
        return None

def log_to_swanlab(run: object, metrics: Dict[str, Any], step: Optional[int] = None):
    """
    è®°å½•æŒ‡æ ‡åˆ°SwanLab
    
    Args:
        run: SwanLab runå¯¹è±¡
        metrics: è¦è®°å½•çš„æŒ‡æ ‡å­—å…¸
        step: è®­ç»ƒæ­¥æ•°
    """
    if run is None:
        return
    
    try:
        if step is not None:
            run.log(metrics, step=step)
        else:
            run.log(metrics)
    except Exception as e:
        print(f"âš ï¸  SwanLabè®°å½•å¤±è´¥: {e}")


def log_model_info_to_swanlab(run: object, model: object, processor: object):
    """
    è®°å½•æ¨¡å‹ä¿¡æ¯åˆ°SwanLab
    
    Args:
        run: SwanLab runå¯¹è±¡
        model: æ¨¡å‹å¯¹è±¡
        processor: å¤„ç†å™¨å¯¹è±¡
    """
    if run is None:
        return
    
    try:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        model_info = {
            "model/total_parameters": total_params,
            "model/trainable_parameters": trainable_params,
            "model/vocab_size": processor.tokenizer.vocab_size,
        }
        
        if hasattr(model.config, 'num_token_id'):
            model_info["model/num_token_id"] = model.config.num_token_id
        if hasattr(model.config, 'numeric_embedding_dim'):
            model_info["model/numeric_embedding_dim"] = model.config.numeric_embedding_dim
        
        run.log(model_info)
        
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯å·²è®°å½•åˆ°SwanLab:")
        print(f"   æ€»å‚æ•°é‡: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹ä¿¡æ¯è®°å½•å¤±è´¥: {e}")


def create_model_and_processor(
    model_path: str,
    numeric_config: Dict = None
) -> tuple:
    """
    åˆ›å»ºæ¨¡å‹å’Œå¤„ç†å™¨
    """
    default_config = {
        'numeric_embedding_dim': 512,
        'numeric_token': '<num>',
        'numeric_loss_weight': 1.0
    }
    
    if numeric_config:
        default_config.update(numeric_config)
    
    try:
        config = NumericQwen2_5_VLConfig.from_pretrained(model_path)
    except:
        from transformers import Qwen2_5_VLConfig as BaseConfig
        base_config = BaseConfig.from_pretrained(model_path)
        config = NumericQwen2_5_VLConfig(**base_config.to_dict())
    
    for key, value in default_config.items():
        setattr(config, key, value)
    
    try:
        processor = NumericQwen2_5_VLProcessor.from_pretrained(model_path)
    except:
        from transformers import Qwen2_5_VLProcessor as BaseProcessor
        base_processor = BaseProcessor.from_pretrained(model_path)
        processor = NumericQwen2_5_VLProcessor(
            image_processor=base_processor.image_processor,
            tokenizer=base_processor.tokenizer
        )
    
    model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
        model_path,
        config=config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    numeric_token = default_config['numeric_token']
    numeric_pad_token = '<num_pad>'
    
    vocab = processor.tokenizer.get_vocab()
    tokens_to_add = []
    
    if numeric_token not in vocab:
        tokens_to_add.append(numeric_token)
    if numeric_pad_token not in vocab:
        tokens_to_add.append(numeric_pad_token)
    
    if tokens_to_add:
        processor.tokenizer.add_special_tokens({
            'additional_special_tokens': tokens_to_add
        })
        model.resize_token_embeddings(len(processor.tokenizer))
        config.vocab_size = len(processor.tokenizer)
        config.num_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_token)
        config.num_pad_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_pad_token)
        model.config.vocab_size = len(processor.tokenizer)
        model.config.num_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_token)
        model.config.num_pad_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_pad_token)
    else:
        config.num_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_token)
        config.num_pad_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_pad_token)
        model.config.num_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_token)
        model.config.num_pad_token_id = processor.tokenizer.convert_tokens_to_ids(numeric_pad_token)
    
    return model, processor


# åœ¨ training_config.py ä¸­æ·»åŠ è¿™äº›ä¿®æ”¹

def get_training_config(
    output_dir: str = "./numeric_qwen2_5_vl_output",
    model_path: str = "Qwen/Qwen2.5-VL-3B-Instruct",
    data_path: str = "./data/numeric_training_data.json",
    val_data_path: str = None,
    test_data_path: str = None,
    image_folder: str = "./data/images",
    enable_swanlab: bool = True,
    swanlab_project: str = "qsinghua",
    swanlab_experiment: str = None,
    **kwargs
) -> NumericTrainingArguments:
    """
    è·å–è®­ç»ƒé…ç½®ï¼Œæ·»åŠ é˜²NaNè®¾ç½®
    """
    
    exp_name = kwargs.pop('exp_name', 'exp1')
    
    conflicting_params = [
        'use_lora', 'lora_rank', 'lora_alpha', 'lora_dropout',
        'num_train_epochs', 'per_device_train_batch_size', 'per_device_eval_batch_size',
        'gradient_accumulation_steps', 'eval_strategy', 'save_strategy', 'save_steps',
        'save_total_limit', 'learning_rate', 'weight_decay', 'warmup_ratio',
        'lr_scheduler_type', 'bf16', 'tf32', 'dataloader_num_workers',
        'gradient_checkpointing', 'logging_steps', 'report_to', 'remove_unused_columns'
    ]
    
    filtered_kwargs = {k: v for k, v in kwargs.items() if k not in conflicting_params}
    
    report_to_value = "swanlab" if enable_swanlab else "none"
    eval_strategy = "steps" if val_data_path else "no"
    
    config = NumericTrainingArguments(
        output_dir=output_dir,
        model_name_or_path=model_path,
        data_path=data_path,
        val_data_path=val_data_path,
        test_data_path=test_data_path,
        image_folder=image_folder,
        enable_swanlab=enable_swanlab,
        swanlab_project=swanlab_project,
        swanlab_experiment=swanlab_experiment,
        num_train_epochs=3,
        per_device_train_batch_size=1,  # å‡å°batch sizeé˜²æ­¢å†…å­˜æº¢å‡º
        per_device_eval_batch_size=1,   # å‡å°eval batch size
        gradient_accumulation_steps=16,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ¥è¡¥å¿å°batch
        eval_strategy=eval_strategy,
        eval_steps=100,  # å¢åŠ è¯„ä¼°é—´éš”
        save_strategy="steps",
        save_steps=100,  # å¢åŠ ä¿å­˜é—´éš”
        save_total_limit=3,
        metric_for_best_model="eval_loss" if val_data_path else None,
        greater_is_better=False,
        load_best_model_at_end=True if val_data_path else False,
        learning_rate=5e-6,  # é™ä½å­¦ä¹ ç‡é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        vision_lr=1e-6,      # é™ä½è§†è§‰ç¼–ç å™¨å­¦ä¹ ç‡
        numeric_lr=5e-5,     # é™ä½æ•°å€¼å±‚å­¦ä¹ ç‡
        weight_decay=0.01,
        warmup_ratio=0.1,    # å¢åŠ warmupæ¯”ä¾‹
        lr_scheduler_type="cosine",
        numeric_loss_weight=0.1,  # é™ä½æ•°å€¼æŸå¤±æƒé‡
        bf16=False,          # æš‚æ—¶ç¦ç”¨bf16ï¼Œä½¿ç”¨fp32é˜²æ­¢ç²¾åº¦é—®é¢˜
        tf32=True,
        dataloader_num_workers=0,  # å‡å°‘workeræ•°é‡é¿å…å¹¶å‘é—®é¢˜
        gradient_checkpointing=True,
        logging_steps=10,
        report_to=report_to_value,
        run_name=f"numeric_qwen2_5_vl_{exp_name}",
        remove_unused_columns=False,
        max_grad_norm=1.0,   # æ·»åŠ æ¢¯åº¦è£å‰ª
        **filtered_kwargs
    )
    
    return config


# ä¿®æ”¹ _compute_mixed_loss æ–¹æ³•ï¼Œæ·»åŠ æ›´å¤šå®‰å…¨æ£€æŸ¥
def _compute_mixed_loss_safe(
    self,
    logits: torch.Tensor,
    predicted_floats: torch.Tensor,
    labels: torch.LongTensor,
    input_ids: torch.LongTensor,
    numeric_values: Optional[List[List[float]]] = None,
    numeric_positions: Optional[List[List[int]]] = None
) -> torch.Tensor:
    """
    è®¡ç®—æ··åˆæŸå¤±ï¼šäº¤å‰ç†µæŸå¤± + æ•°å€¼å›å½’æŸå¤±ï¼Œæ·»åŠ å®‰å…¨æ£€æŸ¥
    """
    import math
    
    batch_size, seq_len = input_ids.shape
    
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«NaN
    if torch.isnan(logits).any():
        print("ERROR: logitsåŒ…å«NaNï¼Œæ›¿æ¢ä¸º0")
        logits = torch.nan_to_num(logits, nan=0.0, posinf=1e6, neginf=-1e6)
    
    if torch.isnan(predicted_floats).any():
        print("ERROR: predicted_floatsåŒ…å«NaNï¼Œæ›¿æ¢ä¸º0") 
        predicted_floats = torch.nan_to_num(predicted_floats, nan=0.0, posinf=1e6, neginf=-1e6)
    
    shift_logits = logits[..., :-1, :].contiguous()
    shift_predicted_floats = predicted_floats[..., :-1].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    prev_input_ids = input_ids[..., :-1].contiguous()
    
    num_pad_token_id = getattr(self.config, 'num_pad_token_id', None) or getattr(self, 'num_pad_token_id', None)
    
    if num_pad_token_id is not None and num_pad_token_id >= 0:
        is_float_target_mask = (prev_input_ids == num_pad_token_id)
    else:
        is_float_target_mask = torch.zeros_like(prev_input_ids, dtype=torch.bool)
    
    # è®¡ç®—tokenæŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
    loss_fct_token = CrossEntropyLoss()
    token_labels = shift_labels.clone()
    token_labels[is_float_target_mask] = -100
    
    try:
        actual_vocab_size = shift_logits.size(-1)
        loss_token = loss_fct_token(
            shift_logits.view(-1, actual_vocab_size), token_labels.view(-1)
        )
        
        # æ£€æŸ¥tokenæŸå¤±æ˜¯å¦ä¸ºNaN
        if torch.isnan(loss_token):
            print("ERROR: tokenæŸå¤±ä¸ºNaNï¼Œè®¾ç½®ä¸º0")
            loss_token = torch.tensor(0.0, device=logits.device, requires_grad=True)
            
    except Exception as e:
        print(f"ERROR: è®¡ç®—tokenæŸå¤±å¤±è´¥: {e}")
        loss_token = torch.tensor(0.0, device=logits.device, requires_grad=True)
    
    # è®¡ç®—æ•°å€¼æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰
    loss_float = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
    
    if numeric_values is not None and numeric_positions is not None:
        try:
            loss_fct_float = MSELoss()
            float_labels = torch.zeros_like(shift_predicted_floats)
            
            valid_numeric_count = 0
            
            for i in range(batch_size):
                if i < len(numeric_values) and numeric_values[i]:
                    values = numeric_values[i]
                    positions = numeric_positions[i]
                    
                    if not isinstance(values, list) or not isinstance(positions, list):
                        continue
                        
                    if isinstance(values, torch.Tensor):
                        values = values.flatten().tolist()
                    
                    for j, (pos, val) in enumerate(zip(positions, values)):
                        if isinstance(pos, (list, tuple)):
                            pos = pos[0] if len(pos) > 0 else 0
                        
                        try:
                            target_pos = int(pos) - 1
                            val_scalar = float(val)
                            
                            # æ£€æŸ¥æ•°å€¼æœ‰æ•ˆæ€§
                            if math.isnan(val_scalar) or math.isinf(val_scalar):
                                print(f"WARNING: æ— æ•ˆæ•°å€¼æ ‡ç­¾ {val_scalar}")
                                val_scalar = 0.0
                            
                            # é™åˆ¶æ•°å€¼èŒƒå›´
                            val_scalar = max(-1e3, min(1e3, val_scalar))
                            
                            if 0 <= target_pos < float_labels.shape[1]:
                                float_labels[i, target_pos] = val_scalar
                                valid_numeric_count += 1
                                
                        except (ValueError, TypeError, IndexError) as e:
                            print(f"WARNING: å¤„ç†æ•°å€¼æ ‡ç­¾å¤±è´¥: pos={pos}, val={val}, error={e}")
                            continue
            
            if is_float_target_mask.any() and valid_numeric_count > 0:
                is_float_target_mask = is_float_target_mask.to(shift_predicted_floats.device)
                float_labels = float_labels.to(shift_predicted_floats.device)
                
                valid_float_preds = shift_predicted_floats[is_float_target_mask]
                valid_float_labels = float_labels[is_float_target_mask]
                
                if valid_float_preds.numel() > 0:
                    # è£å‰ªé¢„æµ‹å€¼é˜²æ­¢æº¢å‡º
                    valid_float_preds = torch.clamp(valid_float_preds, -1e3, 1e3)
                    
                    loss_float = loss_fct_float(valid_float_preds, valid_float_labels)
                    
                    # æ£€æŸ¥æ•°å€¼æŸå¤±æ˜¯å¦ä¸ºNaN
                    if torch.isnan(loss_float):
                        print("ERROR: æ•°å€¼æŸå¤±ä¸ºNaNï¼Œè®¾ç½®ä¸º0")
                        loss_float = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
                        
        except Exception as e:
            print(f"ERROR: è®¡ç®—æ•°å€¼æŸå¤±å¤±è´¥: {e}")
            loss_float = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
    
    # åˆå¹¶æŸå¤±
    try:
        target_device = loss_token.device
        if loss_float.device != target_device:
            loss_float = loss_float.to(target_device)
        
        # ç¡®ä¿æŸå¤±æƒé‡ä¸ä¼šå¯¼è‡´æº¢å‡º
        numeric_weight = min(max(self.numeric_loss_weight, 0.01), 10.0)
        total_loss = loss_token + numeric_weight * loss_float
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(total_loss):
            print("ERROR: æ€»æŸå¤±ä¸ºNaNï¼Œå›é€€åˆ°ä»…tokenæŸå¤±")
            total_loss = loss_token
            
    except Exception as e:
        print(f"ERROR: åˆå¹¶æŸå¤±å¤±è´¥: {e}")
        total_loss = loss_token
    
    # å®‰å…¨åœ°æ‰“å°æŸå¤±ä¿¡æ¯
    try:
        token_loss_val = loss_token.item() if not torch.isnan(loss_token) else 0.0
        float_loss_val = loss_float.item() if not torch.isnan(loss_float) else 0.0
        print(f"Token Loss: {token_loss_val:.4f}, Float Loss: {float_loss_val:.4f}")
    except:
        print("Token Loss: [error], Float Loss: [error]")
    
    return total_loss

def create_deepspeed_config(
    output_file: str = "ds_config_zero2.json",
    zero_stage: int = 2,
    gradient_accumulation_steps: int = 8
) -> str:
    """
    åˆ›å»ºDeepSpeedé…ç½®
    """
    config = {
        "bf16": {"enabled": True},
        "optimizer": {
            "type": "AdamW",
            "params": {"lr": "auto", "betas": "auto", "eps": "auto", "weight_decay": "auto"}
        },
        "scheduler": {
            "type": "WarmupLR",
            "params": {"warmup_min_lr": "auto", "warmup_max_lr": "auto", "warmup_num_steps": "auto"}
        },
        "zero_optimization": {
            "stage": zero_stage,
            "offload_optimizer": {"device": "cpu" if zero_stage >= 2 else "none"},
            "offload_param": {"device": "cpu" if zero_stage >= 3 else "none"},
            "overlap_comm": True,
            "contiguous_gradients": True,
            "sub_group_size": 1e9,
            "reduce_bucket_size": "auto"
        },
        "gradient_accumulation_steps": gradient_accumulation_steps,
        "gradient_clipping": 1.0,
        "steps_per_print": 10,
        "train_batch_size": "auto",
        "train_micro_batch_size_per_gpu": "auto",
        "wall_clock_breakdown": False
    }
    
    with open(output_file, 'w') as f:
        json.dump(config, f, indent=2)
    
    return output_file
