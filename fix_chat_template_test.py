#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¿®å¤chat templateå¹¶æµ‹è¯•å›¾åƒæ¨ç†
"""

import torch
from PIL import Image
import json

# æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹ç»„ä»¶
from numeric_qwen2_5_vl import (
    NumericQwen2_5_VLConfig,
    NumericQwen2_5_VLForConditionalGeneration, 
    NumericQwen2_5_VLProcessor
)

print("ğŸ¯ ä¿®å¤chat templateæµ‹è¯•")
print("=" * 60)

MODEL_PATH = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"

def load_and_fix_chat_template():
    """åŠ è½½å¤„ç†å™¨å¹¶ä¿®å¤chat template"""
    try:
        # åŠ è½½å¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(MODEL_PATH)
        
        # æ‰‹åŠ¨è¯»å–å¹¶è®¾ç½®chat template
        chat_template_path = f"{MODEL_PATH}/chat_template.json"
        with open(chat_template_path, 'r', encoding='utf-8') as f:
            chat_template_data = json.load(f)
        
        # è®¾ç½®chat templateåˆ°tokenizer
        processor.tokenizer.chat_template = chat_template_data["chat_template"]
        
        print("âœ… Chat templateä¿®å¤æˆåŠŸ")
        return processor
        
    except Exception as e:
        print(f"âŒ Chat templateä¿®å¤å¤±è´¥: {e}")
        return None

def test_fixed_image_inference():
    """æµ‹è¯•ä¿®å¤åçš„å›¾åƒæ¨ç†"""
    try:
        # åŠ è½½å¹¶ä¿®å¤å¤„ç†å™¨
        processor = load_and_fix_chat_template()
        if not processor:
            return False
        
        # åˆ›å»ºç®€å•æµ‹è¯•å›¾åƒ
        image = Image.new('RGB', (224, 224), color='red')
        from PIL import ImageDraw
        draw = ImageDraw.Draw(image)
        draw.rectangle([50, 50, 150, 150], fill='blue')
        
        print("âœ… æµ‹è¯•å›¾åƒåˆ›å»ºæˆåŠŸ")
        
        # æ„å»ºå¯¹è¯
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image,
                    },
                    {"type": "text", "text": "æè¿°è¿™ä¸ªå›¾åƒã€‚"}
                ],
            }
        ]
        
        # åº”ç”¨chat template
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        print("âœ… Chat templateåº”ç”¨æˆåŠŸ")
        print(f"Generated prompt: {text[:200]}...")
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[text],
            images=[image],
            padding=True,
            return_tensors="pt"
        )
        
        print("âœ… è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"Input IDs shape: {inputs.input_ids.shape}")
        print(f"Pixel values shape: {inputs.pixel_values.shape}")
        
        # åŠ è½½æ¨¡å‹
        print("ğŸš€ åŠ è½½æ¨¡å‹...")
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = inputs.to(model.device)
        
        # ç”Ÿæˆ
        print("ğŸ¯ å¼€å§‹ç”Ÿæˆ...")
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=64,
                do_sample=True,
                temperature=0.7,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        print("ğŸ‰ å›¾åƒæ¨ç†æˆåŠŸ!")
        print("=" * 50)
        print("æ¨¡å‹è¾“å‡º:")
        print(output_text[0])
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"âŒ å›¾åƒæ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_text_only():
    """æµ‹è¯•çº¯æ–‡æœ¬æ¨ç†"""
    try:
        processor = load_and_fix_chat_template()
        if not processor:
            return False
            
        messages = [
            {"role": "user", "content": "è®¡ç®— <num>15</num> + <num>25</num>"}
        ]
        
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        inputs = processor(text=[text], return_tensors="pt")
        
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        inputs = inputs.to(model.device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=32,
                do_sample=True,
                temperature=0.7
            )
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        print("ğŸ‰ æ–‡æœ¬æ¨ç†æˆåŠŸ!")
        print("=" * 50)
        print("æ•°å­¦è®¡ç®—ç»“æœ:")
        print(output_text[0])
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬æ¨ç†å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•...")
    
    # å…ˆæµ‹è¯•æ–‡æœ¬æ¨ç†
    print("\nğŸ“ æµ‹è¯•æ–‡æœ¬æ¨ç†...")
    text_success = test_text_only()
    
    # å†æµ‹è¯•å›¾åƒæ¨ç†
    print("\nğŸ–¼ï¸ æµ‹è¯•å›¾åƒæ¨ç†...")
    image_success = test_fixed_image_inference()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ æœ€ç»ˆç»“æœ:")
    print(f"âœ… æ–‡æœ¬æ¨ç†: {'æˆåŠŸ' if text_success else 'å¤±è´¥'}")
    print(f"âœ… å›¾åƒæ¨ç†: {'æˆåŠŸ' if image_success else 'å¤±è´¥'}")
    
    if text_success and image_success:
        print("ğŸ‰ å®Œç¾ï¼æ‚¨çš„checkpoint-4250æ¨¡å‹å®Œå…¨æ­£å¸¸å·¥ä½œï¼")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥è¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦è¯„ä¼°äº†ã€‚")
    elif text_success:
        print("âœ… æ–‡æœ¬åŠŸèƒ½æ­£å¸¸ï¼Œå›¾åƒåŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    else:
        print("âŒ éœ€è¦æ£€æŸ¥æ¨¡å‹é…ç½®å’Œæƒé‡ã€‚")
