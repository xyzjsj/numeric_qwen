#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ¨¡å‹åŠ è½½å’Œä½¿ç”¨ç¤ºä¾‹
ä¿®å¤äº†è®¾å¤‡ç§»åŠ¨é—®é¢˜
"""
import os
import torch

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

def load_model_simple():
    """ç®€å•å¯é çš„æ¨¡å‹åŠ è½½æ–¹å¼"""
    print("ğŸš€ åŠ è½½checkpoint-4250æ¨¡å‹...")
    
    # æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
    import sys
    sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')
    
    from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    # åŠ è½½æ¨¡å‹
    model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
        checkpoint_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½å¤„ç†å™¨
    processor = NumericQwen2_5_VLProcessor.from_pretrained(
        checkpoint_path,
        trust_remote_code=True
    )
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {type(model).__name__}")
    print(f"ğŸ“‹ æ¨¡å‹è®¾å¤‡: {model.device}")
    
    return model, processor

def test_simple_inference(model, processor):
    """ç®€å•çš„æ¨ç†æµ‹è¯•"""
    print("\nğŸ§ª æµ‹è¯•ç®€å•æ¨ç†...")
    
    # æµ‹è¯•æ–‡æœ¬
    test_prompt = "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€‚"
    
    # æ„å»ºè¾“å…¥
    prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{test_prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # å¤„ç†è¾“å…¥
    inputs = processor(
        text=[prompt],
        return_tensors="pt"
    )
    
    # å®‰å…¨çš„è®¾å¤‡ç§»åŠ¨
    device_inputs = {}
    for k, v in inputs.items():
        if hasattr(v, 'to') and hasattr(v, 'device'):
            device_inputs[k] = v.to(model.device)
        else:
            device_inputs[k] = v
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            **device_inputs,
            max_new_tokens=150,
            do_sample=True,
            temperature=0.7,
            pad_token_id=processor.tokenizer.eos_token_id
        )
    
    # è§£ç å›ç­”
    generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
    response = processor.decode(generated_ids, skip_special_tokens=True)
    
    print(f"ğŸ“ é—®é¢˜: {test_prompt}")
    print(f"ğŸ¯ å›ç­”: {response}")
    
    return response

def test_numeric_enhancement(model, processor):
    """æµ‹è¯•æ•°å€¼å¢å¼ºåŠŸèƒ½"""
    print("\nğŸ”¢ æµ‹è¯•æ•°å€¼å¢å¼ºåŠŸèƒ½...")
    
    # æµ‹è¯•æ•°å€¼å¢å¼º
    test_prompt = "å¤„ç†è¿™äº›æ•°å€¼ï¼š<num>3.14</num> å’Œ <num>-2.5</num>ï¼Œè®¡ç®—å®ƒä»¬çš„è½¨è¿¹ã€‚"
    
    # æ„å»ºè¾“å…¥
    prompt = f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n{test_prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # å¤„ç†è¾“å…¥
    inputs = processor(
        text=[prompt],
        return_tensors="pt"
    )
    
    # å®‰å…¨çš„è®¾å¤‡ç§»åŠ¨
    device_inputs = {}
    for k, v in inputs.items():
        if hasattr(v, 'to') and hasattr(v, 'device'):
            device_inputs[k] = v.to(model.device)
        else:
            device_inputs[k] = v
    
    # ç”Ÿæˆå›ç­”
    with torch.no_grad():
        outputs = model.generate(
            **device_inputs,
            max_new_tokens=100,
            do_sample=False,
            pad_token_id=processor.tokenizer.eos_token_id
        )
    
    # è§£ç å›ç­”
    generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
    response = processor.decode(generated_ids, skip_special_tokens=True)
    
    print(f"ğŸ“ æ•°å€¼é—®é¢˜: {test_prompt}")
    print(f"ğŸ¯ æ•°å€¼å›ç­”: {response}")
    
    return response

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ checkpoint-4250æ¨¡å‹ç®€å•åŠ è½½å’Œä½¿ç”¨")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    model, processor = load_model_simple()
    
    # æµ‹è¯•æ™®é€šæ¨ç†
    test_simple_inference(model, processor)
    
    # æµ‹è¯•æ•°å€¼å¢å¼º
    test_numeric_enhancement(model, processor)
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ ä½¿ç”¨æ–¹æ³•æ€»ç»“:")
    print("1. å¯¼å…¥æ¨¡å—:")
    print("   import sys")
    print("   sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')")
    print("   from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor")
    print()
    print("2. åŠ è½½æ¨¡å‹:")
    print("   model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(checkpoint_path, ...)")
    print("   processor = NumericQwen2_5_VLProcessor.from_pretrained(checkpoint_path, ...)")
    print()
    print("3. æ¨ç†:")
    print("   inputs = processor(text=[prompt], return_tensors='pt')")
    print("   outputs = model.generate(**inputs, max_new_tokens=100)")
    print("   response = processor.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)")

if __name__ == "__main__":
    main()
