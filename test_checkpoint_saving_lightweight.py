#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½»é‡çº§æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½
ä¸éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œä»…æµ‹è¯•ä¿å­˜æœºåˆ¶
"""

import os
import torch
import tempfile
from transformers import TrainingArguments
from swanlab_trainer import SwanLabNumericTrainer, create_swanlab_trainer
import swanlab

def test_checkpoint_saving_lightweight():
    """è½»é‡çº§æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½"""
    print("å¼€å§‹è½»é‡çº§æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½æµ‹è¯•...")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„è™šæ‹Ÿæ¨¡å‹å’Œå¤„ç†å™¨
    class MockModel:
        def __init__(self):
            self.config = {"model_type": "mock", "hidden_size": 512}
        
        def save_pretrained(self, path):
            import json
            os.makedirs(path, exist_ok=True)
            with open(os.path.join(path, "config.json"), "w") as f:
                json.dump(self.config, f)
            print(f"è™šæ‹Ÿæ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    class MockProcessor:
        def __init__(self):
            self.config = {
                "processor_class": "NumericQwen2_5_VLProcessor",
                "image_processor": {"class": "mock"},
                "tokenizer": {"class": "mock"},
                "numeric_tokens": ["<num>", "<num_pad>"]
            }
        
        def save_pretrained(self, path):
            import json
            os.makedirs(path, exist_ok=True)
            with open(os.path.join(path, "preprocessor_config.json"), "w") as f:
                json.dump(self.config, f)
            with open(os.path.join(path, "tokenizer_config.json"), "w") as f:
                json.dump({"tokenizer_class": "mock"}, f)
            with open(os.path.join(path, "special_tokens_map.json"), "w") as f:
                json.dump({"additional_special_tokens": ["<num>", "<num_pad>"]}, f)
            print(f"è™šæ‹Ÿå¤„ç†å™¨å·²ä¿å­˜åˆ°: {path}")
        
        def _process_text_with_numeric_tokens(self, text):
            # ç®€å•çš„æ¨¡æ‹Ÿå®ç°
            return (text.replace("<num>", "<num_pad>"), [8.5])
    
    model = MockModel()
    processor = MockProcessor()
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºä¿å­˜æ£€æŸ¥ç‚¹
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=temp_dir,
            save_steps=1,
            save_total_limit=1,
            logging_steps=1,
            max_steps=1,
            per_device_train_batch_size=1,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            report_to=None,
        )
        
        # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†
        class DummyDataset:
            def __len__(self):
                return 1
            
            def __getitem__(self, idx):
                return {
                    'input_ids': torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
                    'attention_mask': torch.tensor([1, 1, 1, 1, 1], dtype=torch.long),
                    'labels': torch.tensor([1, 2, 3, 4, 5], dtype=torch.long),
                    'numeric_values': torch.tensor([1.0], dtype=torch.float32),
                    'numeric_masks': torch.tensor([1], dtype=torch.bool),
                }
        
        train_dataset = DummyDataset()
        
        # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¸ä½¿ç”¨SwanLabï¼‰
        print("åˆ›å»ºè®­ç»ƒå™¨...")
        trainer = create_swanlab_trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            swanlab_run=None,
            processor=processor  # ä¼ é€’å¤„ç†å™¨
        )
        
        # éªŒè¯å¤„ç†å™¨æ˜¯å¦æ­£ç¡®é™„åŠ 
        if hasattr(trainer, 'processor'):
            print("âœ… å¤„ç†å™¨å·²æ­£ç¡®é™„åŠ åˆ°è®­ç»ƒå™¨")
        else:
            print("âŒ å¤„ç†å™¨æœªé™„åŠ åˆ°è®­ç»ƒå™¨")
            return False
        
        # æ‰‹åŠ¨è§¦å‘æ£€æŸ¥ç‚¹ä¿å­˜
        print("è§¦å‘æ£€æŸ¥ç‚¹ä¿å­˜...")
        
        # ç›´æ¥è°ƒç”¨trainerçš„ä¿å­˜æ–¹æ³•
        checkpoint_dir = os.path.join(temp_dir, "checkpoint-1")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        model.save_pretrained(checkpoint_dir)
        
        # ä¿å­˜å¤„ç†å™¨ï¼ˆè¿™æ˜¯æˆ‘ä»¬è¦æµ‹è¯•çš„å…³é”®éƒ¨åˆ†ï¼‰
        if hasattr(trainer, 'processor') and trainer.processor is not None:
            try:
                trainer.processor.save_pretrained(checkpoint_dir)
                print("âœ… å¤„ç†å™¨ä¿å­˜æˆåŠŸ")
            except Exception as e:
                print(f"âŒ å¤„ç†å™¨ä¿å­˜å¤±è´¥: {e}")
                return False
        else:
            print("âŒ è®­ç»ƒå™¨ä¸­æ²¡æœ‰å¤„ç†å™¨")
            return False
        
        print(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        expected_files = [
            "config.json",
            "preprocessor_config.json",  # å¤„ç†å™¨é…ç½®
            "special_tokens_map.json",   # ç‰¹æ®Šä»¤ç‰Œæ˜ å°„
            "tokenizer_config.json",     # åˆ†è¯å™¨é…ç½®
        ]
        
        print("æ£€æŸ¥æ–‡ä»¶:")
        all_files_exist = True
        for file_name in expected_files:
            file_path = os.path.join(checkpoint_dir, file_name)
            if os.path.exists(file_path):
                print(f"  âœ… {file_name}")
            else:
                print(f"  âŒ {file_name} (ç¼ºå¤±)")
                all_files_exist = False
        
        # åˆ—å‡ºå®é™…å­˜åœ¨çš„æ–‡ä»¶
        actual_files = os.listdir(checkpoint_dir)
        print(f"\nå®é™…æ–‡ä»¶åˆ—è¡¨: {actual_files}")
        
        # æ£€æŸ¥å¤„ç†å™¨é…ç½®å†…å®¹
        preprocessor_config_path = os.path.join(checkpoint_dir, "preprocessor_config.json")
        if os.path.exists(preprocessor_config_path):
            import json
            with open(preprocessor_config_path, 'r') as f:
                config = json.load(f)
            print(f"âœ… å¤„ç†å™¨é…ç½®å·²ä¿å­˜ï¼ŒåŒ…å« {len(config)} ä¸ªé…ç½®é¡¹")
            print(f"   é…ç½®å†…å®¹: {config}")
        else:
            print("âŒ å¤„ç†å™¨é…ç½®æ–‡ä»¶ç¼ºå¤±")
        
        # éªŒè¯å¤„ç†å™¨åŠŸèƒ½ï¼ˆä½¿ç”¨åŸå§‹å¤„ç†å™¨ï¼‰
        print("\næµ‹è¯•å¤„ç†å™¨åŠŸèƒ½...")
        try:
            test_text = "è¿™æ˜¯ä¸€ä¸ªæ•°å­— <num>8.5</num> çš„æµ‹è¯•"
            result = processor._process_text_with_numeric_tokens(test_text)
            print(f"âœ… å¤„ç†å™¨åŠŸèƒ½æ­£å¸¸: {result}")
        except Exception as e:
            print(f"âŒ å¤„ç†å™¨åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        return all_files_exist

if __name__ == "__main__":
    success = test_checkpoint_saving_lightweight()
    if success:
        print("\nğŸ‰ è½»é‡çº§æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
    else:
        print("\nâŒ è½»é‡çº§æ£€æŸ¥ç‚¹ä¿å­˜åŠŸèƒ½æµ‹è¯•å¤±è´¥!")
