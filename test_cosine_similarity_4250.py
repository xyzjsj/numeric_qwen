#!/usr/bin/env python3
"""
Numeric Qwen2.5-VL (checkpoint-4250) ä½™å¼¦ç›¸ä¼¼åº¦ç®€æ˜“è¯„ä¼°è„šæœ¬
æ ¸å¿ƒç‰¹æ€§:
 1. ä½¿ç”¨è‡ªå®šä¹‰ NumericQwen2_5_VL æ¨¡å‹ä¸å¤„ç†å™¨
 2. ä¸å·²éªŒè¯å¯è¡Œçš„å›¾åƒæ¨ç†æ–¹å¼ä¿æŒä¸€è‡´ (å• <|image_pad|> å ä½ç¬¦, ç”±å¤„ç†å™¨æ³¨å…¥çœŸå®ç‰¹å¾)
 3. ç”Ÿæˆé˜¶æ®µå…³é—­é‡‡æ ·ï¼Œä¿è¯ç¨³å®šè¾“å‡º
 4. æä¾›ä¸¤ç§æ–‡æœ¬ç›¸ä¼¼åº¦: TF-IDF & (å¯é€‰) æ¨¡å‹éšè—å‘é‡å¹³å‡æ± åŒ–
 5. å¤±è´¥æ ·æœ¬è·³è¿‡ï¼Œä¸ä¸­æ–­æ•´ä½“è¯„ä¼°
"""

import os
import json
import torch
import numpy as np
from PIL import Image
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from numeric_qwen2_5_vl import (
    NumericQwen2_5_VLForConditionalGeneration,
    NumericQwen2_5_VLProcessor
)
import warnings
warnings.filterwarnings("ignore")

# ---------------- é…ç½® ---------------- #
CHECKPOINT_PATH = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
TEST_DATA_PATH = "/data1/wangzhiye/LLaMA-Factory/data/5vqa_data_extracted_test_converted.json"  # æ•°æ®é¡¹å« messages / images
NUM_SAMPLES = 15200          # 0 æˆ–è´Ÿæ•°è¡¨ç¤ºéå†å…¨éƒ¨é—®ç­”å¯¹
MAX_IMAGES_PER_Q = 6      # æ¯ä¸ªé—®é¢˜æœ€å¤šä½¿ç”¨çš„å›¾åƒæ•°é‡ï¼ˆåœºæ™¯æœ‰ 6 è§†è§’ï¼‰
USE_MULTI_IMAGE_PLACEHOLDERS = True  # True: ä¸ºæ¯å¼ å›¾åƒæ’å…¥ä¸€ä¸ª <|image_pad|> å ä½ç¬¦
USE_SINGLE_PLACEHOLDER_FOR_ALL = True  # True: æ— è®ºå¤šå°‘å›¾åƒåªæ”¾ 1 ç»„ <|vision_start|><|image_pad|><|vision_end|>
FORCE_MANUAL_TEMPLATE = True  # å¼ºåˆ¶ä½¿ç”¨æ‰‹åŠ¨ promptï¼Œä¸å°è¯• apply_chat_template
USE_EMBEDDING_SIM = False  # å¦‚éœ€å¼€å¯åµŒå…¥ç›¸ä¼¼åº¦æ”¹ä¸º Trueï¼ˆè¾ƒæ…¢ï¼‰
MAX_NEW_TOKENS = 128
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
OUTPUT_JSON_PATH = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/cosine_eval_results_num.json"  # ç»“æœä¿å­˜è·¯å¾„
SAVE_EVERY = 50  # æ¯å¤„ç†å¤šå°‘æ¡å¢é‡å†™ç›˜ä¸€æ¬¡ï¼ˆ0 è¡¨ç¤ºä»…æœ€åå†™ï¼‰


def load_model(checkpoint_path: str):
    print(f"ğŸ”„ åŠ è½½è‡ªå®šä¹‰æ•°å€¼å¢å¼ºæ¨¡å‹: {checkpoint_path}")
    dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16
    model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
        checkpoint_path,
        torch_dtype=dtype,
        device_map="auto",
        trust_remote_code=True
    )
    processor = NumericQwen2_5_VLProcessor.from_pretrained(checkpoint_path, trust_remote_code=True)
    print("âœ… æ¨¡å‹ä¸å¤„ç†å™¨åŠ è½½å®Œæˆ")
    return model, processor


def load_data(path: str):
    print(f"ğŸ“ è¯»å–æ•°æ®: {path}")
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… æ€»æ ·æœ¬æ•°: {len(data)}")
    return data


def is_trajectory_question(q: str) -> bool:
    q_lower = q.lower()
    keys = ["trajectory", "planning", "plan", "[pt"]
    return any(k in q_lower for k in keys)


def build_manual_prompt(question: str, image_count: int) -> str:
    """æ„é€ æ‰‹åŠ¨ prompt, å¯å«å¤šå›¾åƒå ä½ç¬¦ã€‚è½¨è¿¹ç±»é—®é¢˜æ‰åŠ  few-shotã€‚"""
    if image_count < 1:
        image_count = 1
    if USE_SINGLE_PLACEHOLDER_FOR_ALL:
        img_segment = "<|vision_start|><|image_pad|><|vision_end|>"
    else:
        if USE_MULTI_IMAGE_PLACEHOLDERS:
            img_segment = ''.join(["<|vision_start|><|image_pad|><|vision_end|>" for _ in range(image_count)])
        else:
            img_segment = "<|vision_start|><|image_pad|><|vision_end|>"
    # few-shotï¼šä»…åœ¨è½¨è¿¹é—®é¢˜æ—¶åŠ å…¥ï¼Œé¿å…æ™®é€šé—®ç­”è¢«æ¨¡æ¿å½±å“
    few_shot = ""
    if is_trajectory_question(question):
        few_shot = (
            "<|im_start|>user\nProvide the planning trajectory only.\n<|im_end|>\n"
            # æä¾›ä¸€ä¸ªæ›´å®Œæ•´çš„ç¤ºä¾‹ï¼ˆ6 ä¸ªç‚¹ï¼‰ï¼Œå¼•å¯¼æ¨¡å‹è¾“å‡ºå®Œæ•´è½¨è¿¹é•¿åº¦
            "<|im_start|>assistant\n[PT, (+4.10, +0.05), (+8.25, +0.37), (+12.40, +0.92), (+16.55, +1.65), (+20.70, +2.50), (+24.85, +3.40)]<|im_end|>\n"
        )
    system_inst = (
        "You are an autonomous driving perception and planning assistant. "
        "Given multi-view images, answer ONLY the user's request. "
        "If the user asks for a planning trajectory, output strictly a bracketed list in the form [PT, (+x1, +y1), (+x2, +y2), ...] with no extra words. "
        "Coordinates must keep the sign (use + or -) and two decimals when possible."
    )
    return (
        f"<|im_start|>system\n{system_inst}<|im_end|>\n" + few_shot +
        f"<|im_start|>user\n{img_segment}{question}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )


def generate_answer(model, processor, image_paths, question: str) -> str:
    """æ”¯æŒå¤šå›¾åƒï¼›ä¼˜å…ˆä½¿ç”¨ chat_templateï¼›å¤±è´¥åˆ™å›é€€æ‰‹åŠ¨å•å›¾åƒæ¨¡æ¿ã€‚"""
    valid_images = []
    for p in image_paths[:MAX_IMAGES_PER_Q]:
        if not p or not os.path.exists(p):
            continue
        try:
            valid_images.append(Image.open(p).convert('RGB'))
        except Exception:
            continue
    if not valid_images:
        print("âš ï¸ æ— å¯ç”¨å›¾åƒï¼Œè·³è¿‡")
        return ""

    # ç»Ÿä¸€ä½¿ç”¨æ‰‹åŠ¨ promptï¼›ä¹‹å‰æŠ¥é”™æ˜¯å› ä¸ºè‡ªå®šä¹‰å¤„ç†å™¨æ²¡æœ‰ä¿å­˜ chat templateã€‚
    prompt = build_manual_prompt(question, len(valid_images))
    inputs = processor(text=[prompt], images=valid_images, padding=True, return_tensors="pt")

    inputs = inputs.to(DEVICE)
    # è°ƒè¯•è§†è§‰è¾“å…¥
    if 'pixel_values' in inputs:
        pv = inputs.pixel_values
        print(f"[è°ƒè¯•] pixel_values.shape: {tuple(pv.shape)} (æ‰¹, å›¾åƒ, é€šé“, é«˜, å®½)" if pv.dim()==5 else f"[è°ƒè¯•] pixel_values.shape: {tuple(pv.shape)}")
    if 'image_grid_thw' in inputs:
        print(f"[è°ƒè¯•] image_grid_thw: {inputs.image_grid_thw}")
    image_token_id = getattr(model.config, 'image_token_id', None)
    if image_token_id is not None:
        cnt = (inputs.input_ids == image_token_id).sum().item()
        per_img = cnt / max(1, len(valid_images))
        placeholder_mode = 'single_placeholder_all' if USE_SINGLE_PLACEHOLDER_FOR_ALL else (
            'multi_placeholders' if USE_MULTI_IMAGE_PLACEHOLDERS else 'single_per_call'
        )
        print(f"[è°ƒè¯•] image_token æ€»æ•°: {cnt} | å›¾åƒæ•°: {len(valid_images)} | æ¯å›¾ token: {per_img:.1f} | æ¨¡å¼: {placeholder_mode}")
        if cnt == 0:
            print("â— æœªæ£€æµ‹åˆ°ä»»ä½•è§†è§‰ tokenï¼Œç»“æœä¸ä¼šåˆ©ç”¨å›¾åƒ")

    with torch.no_grad():
        out_ids = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            temperature=0.0,
            pad_token_id=processor.tokenizer.eos_token_id
        )
    new_tokens = [o[len(i):] for i, o in zip(inputs.input_ids, out_ids)]
    return processor.batch_decode(new_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0].strip()


def cosine_tfidf(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    try:
        vec = TfidfVectorizer(lowercase=True)
        m = vec.fit_transform([a, b])
        return cosine_similarity(m[0:1], m[1:2])[0][0]
    except Exception:
        return 0.0


def cosine_embedding(a: str, b: str, model, processor) -> float:
    if not a or not b:
        return 0.0
    try:
        inp1 = processor(text=[a], return_tensors="pt", padding=True, truncation=True, max_length=512).to(DEVICE)
        inp2 = processor(text=[b], return_tensors="pt", padding=True, truncation=True, max_length=512).to(DEVICE)
        with torch.no_grad():
            o1 = model(**inp1, output_hidden_states=True)
            o2 = model(**inp2, output_hidden_states=True)
        h1 = o1.hidden_states[-1]
        h2 = o2.hidden_states[-1]
        # å– attention_mask èšåˆï¼ˆé¿å… pad å½±å“ï¼‰
        def mean_pool(h, mask):
            if mask is None:
                return h.mean(dim=1)
            mask = mask.unsqueeze(-1).type_as(h)
            return (h * mask).sum(dim=1) / mask.sum(dim=1).clamp_min(1e-6)
        e1 = mean_pool(h1, inp1.get('attention_mask'))
        e2 = mean_pool(h2, inp2.get('attention_mask'))
        e1 = torch.nn.functional.normalize(e1, dim=-1)
        e2 = torch.nn.functional.normalize(e2, dim=-1)
        return float((e1 @ e2.T).item())
    except Exception:
        return 0.0


def extract_qa_pairs_from_sample(sample: dict):
    """ä»å•ä¸ªæ ·æœ¬ (messages + images) æå–å¤šè½® (question, answer) å¯¹ã€‚
    question å»æ‰ <image> æ ‡è®°ï¼›answer ä¸ºç´§éšå…¶åçš„ assistant å†…å®¹ã€‚
    è¿”å› list[(images_list, question, answer)]
    """
    pairs = []
    messages = sample.get('messages', [])
    imgs = sample.get('images', [])
    for i in range(len(messages) - 1):
        m = messages[i]
        n = messages[i+1]
        if m.get('role') == 'user' and n.get('role') == 'assistant':
            raw_q = m.get('content', '')
            # è®¡æ•° <image> ä»¥å†³å®šä½¿ç”¨å¤šå°‘å›¾åƒ
            img_count = raw_q.count('<image>')
            question = raw_q.replace('<image>', '').strip()
            if not question:
                continue
            answer = n.get('content', '').strip()
            if not answer:
                continue
            use_paths = imgs[:img_count] if img_count > 0 else imgs[:1]
            if not use_paths:
                continue
            pairs.append((use_paths, question, answer))
    return pairs


import re
traj_re = re.compile(r"\[\s*PT\s*,(.*?)\]", re.IGNORECASE)
point_re = re.compile(r"\(\s*([+-]?\d+(?:\.\d+)?)\s*,\s*([+-]?\d+(?:\.\d+)?)\s*\)")

# -------- æ•°å€¼åŒ…è£…æ¸…æ´—å·¥å…· -------- #
# åŒ¹é… <num><+4.79> æˆ– <num><-12.3> æˆ– <num><4>
NUM_VALUE_WRAP = re.compile(r"<num>\s*<\s*([+-]?\d+(?:\.\d+)?)\s*>", re.IGNORECASE)
NUM_TOKEN = re.compile(r"<num>", re.IGNORECASE)
NUM_PAD_TOKEN = re.compile(r"<num_pad>", re.IGNORECASE)

def normalize_numeric_wrappers(text: str) -> str:
    """ä»…ç”¨äº L2 è§£æé˜¶æ®µ: å°† <num><+4.79> å±•å¼€ä¸º +4.79ã€‚
    ä¿ç•™å…¶å®ƒ <num> / <num_pad> æ ‡è®°åŸæ ·ï¼ˆä¸åœ¨æ­¤å¤„åˆ é™¤ï¼‰ï¼Œä»¥å…å½±å“åŸå§‹å¯è§†åŒ–/å­˜å‚¨ã€‚
    æ³¨æ„: åªåœ¨ parse_trajectory å†…éƒ¨è°ƒç”¨ï¼Œä¸æ”¹å˜æ¨¡å‹è¾“å…¥ä¸æœ€ç»ˆå±•ç¤ºæ–‡æœ¬ã€‚
    """
    if not text:
        return text
    return NUM_VALUE_WRAP.sub(r"\1", text)

def parse_trajectory(text: str):
    """è§£æ [PT, (x,y), ...] è¿”å› list[(x,y)] (float)ã€‚è‡ªåŠ¨æ¸…æ´— <num> åŒ…è£…ã€‚å¤±è´¥è¿”å›ç©ºåˆ—è¡¨ã€‚"""
    if not text:
        return []
    cleaned = normalize_numeric_wrappers(text)
    m = traj_re.search(cleaned)
    if not m:
        return []
    inner = m.group(1)
    pts = []
    for xm, ym in point_re.findall(inner):
        try:
            pts.append((float(xm), float(ym)))
        except ValueError:
            continue
    return pts

def trajectory_metrics(pred_pts, ref_pts):
    if not pred_pts or not ref_pts:
        return {"parsed": bool(pred_pts), "point_count_pred": len(pred_pts), "point_count_ref": len(ref_pts), "count_diff": len(pred_pts)-len(ref_pts), "avg_l2": None}
    import math
    n = min(len(pred_pts), len(ref_pts))
    l2s = [math.sqrt((pred_pts[i][0]-ref_pts[i][0])**2 + (pred_pts[i][1]-ref_pts[i][1])**2) for i in range(n)]
    return {"parsed": True, "point_count_pred": len(pred_pts), "point_count_ref": len(ref_pts), "count_diff": len(pred_pts)-len(ref_pts), "avg_l2": sum(l2s)/n if n>0 else None}


def main():
    print("ğŸš€ å¼€å§‹ Numeric Qwen2.5-VL ä½™å¼¦ç›¸ä¼¼åº¦è¯„ä¼° (messages æ ¼å¼)")
    print("=" * 60)
    model, processor = load_model(CHECKPOINT_PATH)
    data_items = load_data(TEST_DATA_PATH)  # å…¨é‡è½½å…¥

    # èšåˆé—®ç­”å¯¹
    qa_pairs = []
    for sample in data_items:
        qa_pairs.extend(extract_qa_pairs_from_sample(sample))
        if NUM_SAMPLES and NUM_SAMPLES > 0 and len(qa_pairs) >= NUM_SAMPLES:
            break
    if NUM_SAMPLES and NUM_SAMPLES > 0:
        qa_pairs = qa_pairs[:NUM_SAMPLES]
    print(f"âœ… æ”¶é›†åˆ° {len(qa_pairs)} ä¸ªé—®ç­”å¯¹ç”¨äºè¯„ä¼° (NUM_SAMPLES={NUM_SAMPLES if NUM_SAMPLES else 'ALL'})")
    if not qa_pairs:
        print("âš ï¸ æ²¡æœ‰é—®ç­”å¯¹ï¼Œé€€å‡º")
        return

    tfidf_scores = []
    traj_avg_l2_list = []
    traj_parse_success = 0
    embed_scores = []
    results = []  # ä¿å­˜æ¯ä¸ªæ ·æœ¬ç»“æœ
    for idx, (img_paths, q, ref) in enumerate(qa_pairs, 1):
        print(f"\n--- æ ·æœ¬ {idx}/{len(qa_pairs)} ---")
        print(f"ğŸ–¼ï¸ ä½¿ç”¨å›¾åƒæ•°: {len(img_paths)}  (æ˜¾ç¤ºé¦–å›¾è·¯å¾„: {img_paths[0]})")
        is_traj = is_trajectory_question(q)
        if is_traj:
            print(f"â“ é—®é¢˜: {q}")
            print(f"ğŸ“š å‚è€ƒ: {ref}")
        else:
            print(f"â“ é—®é¢˜: {q[:100]}{'...' if len(q)>100 else ''}")
            print(f"ğŸ“š å‚è€ƒ: {ref[:100]}{'...' if len(ref)>100 else ''}")
        ans = generate_answer(model, processor, img_paths, q)
        if not ans:
            print("âš ï¸ æ— ç”Ÿæˆç­”æ¡ˆï¼Œè·³è¿‡")
            continue
        if is_traj:
            print(f"ğŸ¤– ç­”æ¡ˆ: {ans}")
        else:
            print(f"ğŸ¤– ç­”æ¡ˆ: {ans[:100]}{'...' if len(ans)>100 else ''}")
        s_t = cosine_tfidf(ans, ref)
        tfidf_scores.append(s_t)
        print(f"ğŸ“ˆ TF-IDF ç›¸ä¼¼åº¦: {s_t:.4f}")
        sample_record = {
            "index": idx,
            "image_paths": img_paths,
            "question": q,
            "reference": ref,
            "answer": ans,
            "tfidf": s_t,
        }
        # è½¨è¿¹ä¸“å±è¯„ä¼°
        if is_traj:
            pred_pts = parse_trajectory(ans)
            ref_pts = parse_trajectory(ref)
            tm = trajectory_metrics(pred_pts, ref_pts)
            if tm["parsed"] and tm["avg_l2"] is not None:
                traj_avg_l2_list.append(tm["avg_l2"])
            if tm["parsed"]:
                traj_parse_success += 1
            print(f"ğŸ›£ï¸ TrajEval parsed={tm['parsed']} pred_pts={tm['point_count_pred']} ref_pts={tm['point_count_ref']} diff={tm['count_diff']} avg_l2={tm['avg_l2']}")
            # è°ƒè¯•æ¸…æ´—ç»“æœï¼ˆåªæ˜¾ç¤ºå‰ 120 å­—ï¼‰
            print(f"[TrajDebug] clean_ref: {normalize_numeric_wrappers(ref)[:120]}")
            print(f"[TrajDebug] clean_ans: {normalize_numeric_wrappers(ans)[:120]}")
            sample_record.update({
                "trajectory": {
                    "parsed": tm['parsed'],
                    "pred_point_count": tm['point_count_pred'],
                    "ref_point_count": tm['point_count_ref'],
                    "count_diff": tm['count_diff'],
                    "avg_l2": tm['avg_l2']
                }
            })
        if USE_EMBEDDING_SIM:
            s_e = cosine_embedding(ans, ref, model, processor)
            embed_scores.append(s_e)
            print(f"ğŸ§  åµŒå…¥ç›¸ä¼¼åº¦: {s_e:.4f}")
            sample_record["embedding_sim"] = s_e
        results.append(sample_record)

        # å¢é‡ä¿å­˜
        if SAVE_EVERY and SAVE_EVERY > 0 and idx % SAVE_EVERY == 0:
            try:
                with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
                    json.dump({
                        "checkpoint": CHECKPOINT_PATH,
                        "total_processed": idx,
                        "num_samples_limit": NUM_SAMPLES,
                        "results": results
                    }, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ å·²ä¸´æ—¶ä¿å­˜ {idx} æ¡åˆ° {OUTPUT_JSON_PATH}")
            except Exception as e:
                print(f"âš ï¸ ä¸´æ—¶ä¿å­˜å¤±è´¥: {e}")

    print("\n" + "=" * 60)
    print("ğŸ“Š æ±‡æ€»ç»“æœ")
    if tfidf_scores:
        arr = np.array(tfidf_scores)
        print(f"TF-IDF å¹³å‡: {arr.mean():.4f} | ä¸­ä½: {np.median(arr):.4f} | æœ€å°: {arr.min():.4f} | æœ€å¤§: {arr.max():.4f} | æ ·æœ¬: {len(arr)}")
    else:
        print("æ—  TF-IDF ç»“æœ")
    if traj_avg_l2_list:
        arr_l2 = np.array(traj_avg_l2_list)
        print(f"ğŸ›£ï¸ è½¨è¿¹å¹³å‡ L2: {arr_l2.mean():.4f} | ä¸­ä½: {np.median(arr_l2):.4f} | æœ€å°: {arr_l2.min():.4f} | æœ€å¤§: {arr_l2.max():.4f} | è§£ææˆåŠŸ {traj_parse_success} / è½¨è¿¹é—®ç­”")
    if USE_EMBEDDING_SIM:
        if embed_scores:
            arr = np.array(embed_scores)
            print(f"åµŒå…¥ å¹³å‡: {arr.mean():.4f} | ä¸­ä½: {np.median(arr):.4f} | æœ€å°: {arr.min():.4f} | æœ€å¤§: {arr.max():.4f} | æ ·æœ¬: {len(arr)}")
        else:
            print("æ—  åµŒå…¥ ç»“æœ")
    print("\nâœ… è¯„ä¼°å®Œæˆ")

    # æœ€ç»ˆä¿å­˜
    try:
        with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
            json.dump({
                "checkpoint": CHECKPOINT_PATH,
                "total_processed": len(results),
                "num_samples_limit": NUM_SAMPLES,
                "tfidf_summary": {
                    "mean": float(np.mean(tfidf_scores)) if tfidf_scores else None,
                    "median": float(np.median(tfidf_scores)) if tfidf_scores else None,
                    "min": float(np.min(tfidf_scores)) if tfidf_scores else None,
                    "max": float(np.max(tfidf_scores)) if tfidf_scores else None,
                },
                "trajectory_summary": {
                    "parsed_success": traj_parse_success,
                    "avg_l2_mean": float(np.mean(traj_avg_l2_list)) if traj_avg_l2_list else None,
                    "avg_l2_median": float(np.median(traj_avg_l2_list)) if traj_avg_l2_list else None,
                },
                "embedding_summary": (
                    {
                        "mean": float(np.mean(embed_scores)),
                        "median": float(np.median(embed_scores)),
                        "min": float(np.min(embed_scores)),
                        "max": float(np.max(embed_scores)),
                    } if embed_scores else None
                ),
                "results": results
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜å®Œæ•´ç»“æœåˆ°: {OUTPUT_JSON_PATH}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


if __name__ == "__main__":
    main()

def load_test_data(data_path, num_samples=10):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"ğŸ“ åŠ è½½æµ‹è¯•æ•°æ®: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å–å‰num_samplesä¸ªæ ·æœ¬
    test_samples = data[:num_samples]
    print(f"âœ… åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    return test_samples

def generate_response(model, processor, image_path, question):
    """ç”Ÿæˆæ¨¡å‹å›ç­”"""
    try:
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": question}
                ]
            }
        ]
        
        # å¤„ç†è¾“å…¥
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = processor.process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = inputs.to("cuda")
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=False,
                temperature=0.0
            )
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        return output_text[0].strip()
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
        return ""

def calculate_text_cosine_similarity(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if not text1 or not text2:
        return 0.0
    
    # ä½¿ç”¨TF-IDFå‘é‡åŒ–
    vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')
    
    try:
        # è®¡ç®—TF-IDFçŸ©é˜µ
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        return similarity
        
    except Exception as e:
        print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
        return 0.0

def calculate_embedding_similarity(text1, text2, model, processor):
    """ä½¿ç”¨æ¨¡å‹åµŒå…¥è®¡ç®—ç›¸ä¼¼åº¦"""
    try:
        # ç®€å•çš„æ–‡æœ¬åµŒå…¥è®¡ç®—ï¼ˆä½¿ç”¨æ¨¡å‹çš„éšè—çŠ¶æ€ï¼‰
        inputs1 = processor(text=[text1], return_tensors="pt", padding=True, truncation=True, max_length=512)
        inputs2 = processor(text=[text2], return_tensors="pt", padding=True, truncation=True, max_length=512)
        
        inputs1 = {k: v.to("cuda") for k, v in inputs1.items()}
        inputs2 = {k: v.to("cuda") for k, v in inputs2.items()}
        
        with torch.no_grad():
            outputs1 = model(**inputs1, output_hidden_states=True)
            outputs2 = model(**inputs2, output_hidden_states=True)
            
            # ä½¿ç”¨æœ€åä¸€å±‚çš„å¹³å‡æ± åŒ–ä½œä¸ºåµŒå…¥
            embed1 = outputs1.hidden_states[-1].mean(dim=1).cpu().numpy()
            embed2 = outputs2.hidden_states[-1].mean(dim=1).cpu().numpy()
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity(embed1, embed2)[0][0]
            
        return similarity
        
    except Exception as e:
        print(f"âŒ åµŒå…¥ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
        return 0.0

    # (æ—§æ®‹ç•™ä»£ç å·²æ¸…ç†)
