#!/usr/bin/env python3
"""
åŸºäºGitCodeæ–‡ç« å’ŒDeepWikiå»ºè®®çš„å®Œæ•´Qwen2.5-VLå›¾åƒTokenä¿®å¤è§£å†³æ–¹æ¡ˆ
å‚è€ƒé“¾æ¥:
- https://blog.gitcode.com/d41d68b8e2ccdd03c0c59a4ca19a517b.html
- https://blog.gitcode.com/ca29fd2798662a2888c8ed01fd2a4207.html
"""
import os
import torch
from PIL import Image
import json

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def fix_chat_template(checkpoint_path):
    """
    ä¿®å¤chat templateé…ç½®
    åŸºäºGitCodeæ–‡ç« çš„è§£å†³æ–¹æ¡ˆ
    """
    tokenizer_config_path = os.path.join(checkpoint_path, "tokenizer_config.json")
    
    print(f"ğŸ”§ æ£€æŸ¥å’Œä¿®å¤chat template: {tokenizer_config_path}")
    
    try:
        # è¯»å–ç°æœ‰é…ç½®
        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰chat_template
        if 'chat_template' not in config or not config['chat_template']:
            print("   âš ï¸ ç¼ºå°‘chat_templateï¼Œæ­£åœ¨æ·»åŠ ...")
            
            # æ·»åŠ æ ‡å‡†çš„Qwen2.5-VL chat template
            config['chat_template'] = "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\n'}}{% endif %}"
            
            # å¤‡ä»½åŸæ–‡ä»¶
            import shutil
            shutil.copy2(tokenizer_config_path, f"{tokenizer_config_path}.backup")
            
            # å†™å…¥ä¿®å¤åçš„é…ç½®
            with open(tokenizer_config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print("   âœ… chat_templateå·²æ·»åŠ å¹¶ä¿å­˜")
        else:
            print("   âœ… chat_templateå·²å­˜åœ¨")
            
    except Exception as e:
        print(f"   âŒ ä¿®å¤chat_templateå¤±è´¥: {e}")

def load_checkpoint_model_with_fixes(checkpoint_path):
    """åŠ è½½æ¨¡å‹å¹¶åº”ç”¨æ‰€æœ‰ä¿®å¤"""
    print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹å¹¶åº”ç”¨ä¿®å¤: {checkpoint_path}")
    
    # é¦–å…ˆä¿®å¤chat template
    fix_chat_template(checkpoint_path)
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å¤„ç†å™¨
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        print("âœ… æ•°å€¼å¢å¼ºæ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def create_optimal_image_with_exact_tokens(target_tokens=64):
    """
    åˆ›å»ºç²¾ç¡®Tokenæ•°é‡çš„å›¾åƒ
    æ ¹æ®å…¬å¼: Tokenæ•°é‡ = å›¾åƒåƒç´ æ•° / (28 * 28)
    """
    target_pixels = target_tokens * 28 * 28
    
    # è®¡ç®—æ¥è¿‘æ­£æ–¹å½¢çš„å°ºå¯¸
    import math
    side_length = int(math.sqrt(target_pixels))
    
    # å¾®è°ƒåˆ°æœ€æ¥è¿‘ç›®æ ‡åƒç´ æ•°
    width = side_length
    height = target_pixels // width
    
    actual_pixels = width * height
    actual_tokens = actual_pixels // (28 * 28)
    
    print(f"ğŸ¨ åˆ›å»ºç²¾ç¡®Tokenå›¾åƒ:")
    print(f"   ç›®æ ‡Tokenæ•°: {target_tokens}")
    print(f"   å›¾åƒå°ºå¯¸: {width}x{height}")
    print(f"   å®é™…åƒç´ æ•°: {actual_pixels}")
    print(f"   å®é™…Tokenæ•°: {actual_tokens}")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    image = Image.new('RGB', (width, height), color='lightblue')
    
    # æ·»åŠ è§†è§‰ç‰¹å¾
    import numpy as np
    img_array = np.array(image)
    
    # æ·»åŠ å½©è‰²æ¡çº¹
    for i in range(0, height, 20):
        img_array[i:i+10, :] = [255, 200, 100]  # æ©™è‰²
    
    for j in range(0, width, 30):
        img_array[:, j:j+15] = [100, 255, 100]  # ç»¿è‰²
    
    # æ·»åŠ å¯¹è§’çº¿
    for k in range(min(width, height)):
        if k < height and k < width:
            img_array[k, k] = [255, 0, 0]  # çº¢è‰²å¯¹è§’çº¿
    
    return Image.fromarray(img_array)

def test_with_proper_chat_template(model, processor, image, question):
    """
    ä½¿ç”¨æ­£ç¡®çš„chat templateè¿›è¡ŒVQAæµ‹è¯•
    """
    try:
        print(f"ğŸ¤” é—®é¢˜: {question}")
        
        # æ–¹æ³•1: ä½¿ç”¨apply_chat_templateï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            messages = [
                {
                    "role": "user", 
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": question}
                    ]
                }
            ]
            
            # å°è¯•ä½¿ç”¨chat template
            text = processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            print("âœ… ä½¿ç”¨apply_chat_templateæˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ apply_chat_templateå¤±è´¥: {e}")
            print("ğŸ”„ ä½¿ç”¨æ‰‹åŠ¨æ„å»ºçš„æ ¼å¼...")
            
            # æ–¹æ³•2: æ‰‹åŠ¨æ„å»ºæ­£ç¡®æ ¼å¼
            text = f"<|im_start|>user\n<|image|>{question}<|im_end|>\n<|im_start|>assistant\n"
        
        print(f"ğŸ“ æœ€ç»ˆprompté•¿åº¦: {len(text)}")
        
        # å¤„ç†è¾“å…¥
        image_inputs = processor(
            text=[text],
            images=[image],
            return_tensors="pt"
        )
        
        print("ğŸ“¦ è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š input_ids shape: {image_inputs.input_ids.shape}")
        
        if hasattr(image_inputs, 'pixel_values') and image_inputs.pixel_values is not None:
            print(f"ğŸ–¼ï¸ pixel_values shape: {image_inputs.pixel_values.shape}")
        
        # æ£€æŸ¥Tokenæ•°é‡
        decoded_text = processor.tokenizer.decode(image_inputs.input_ids[0], skip_special_tokens=False)
        image_token_count = decoded_text.count('<|image_pad|>')
        print(f"ğŸ”¢ å®é™…å›¾åƒTokenæ•°é‡: {image_token_count}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device_inputs = {}
        for k, v in image_inputs.items():
            if hasattr(v, 'to'):
                device_inputs[k] = v.to(model.device)
            else:
                device_inputs[k] = v
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = model.generate(
                **device_inputs,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_p=0.8,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ¯ å›ç­”: {response}")
        return response
        
    except Exception as e:
        print(f"âŒ VQAæ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def comprehensive_test_suite():
    """ç»¼åˆæµ‹è¯•å¥—ä»¶"""
    print("ğŸš€ å¼€å§‹ç»¼åˆå›¾åƒTokenä¿®å¤æµ‹è¯•")
    print("=" * 70)
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    try:
        # åŠ è½½æ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰ä¿®å¤ï¼‰
        model, processor = load_checkpoint_model_with_fixes(checkpoint_path)
        
        # æµ‹è¯•ä¸åŒTokenæ•°é‡çš„å›¾åƒ
        test_configs = [
            {"tokens": 16, "question": "è¿™å¼ å›¾ç‰‡çš„ä¸»è¦é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ"},
            {"tokens": 64, "question": "æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å›¾æ¡ˆå’Œç‰¹å¾ã€‚"},
            {"tokens": 144, "question": "è¿™å¼ å›¾ç‰‡ç»™ä½ ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ"}
        ]
        
        for config in test_configs:
            print(f"\nğŸ“‹ æµ‹è¯•é…ç½®: {config['tokens']} tokens")
            print("-" * 50)
            
            # åˆ›å»ºç²¾ç¡®Tokenæ•°é‡çš„å›¾åƒ
            image = create_optimal_image_with_exact_tokens(config['tokens'])
            
            # è¿›è¡ŒVQAæµ‹è¯•
            result = test_with_proper_chat_template(
                model, processor, image, config['question']
            )
            
            if result:
                print("âœ… æµ‹è¯•æˆåŠŸ")
            else:
                print("âŒ æµ‹è¯•å¤±è´¥")
        
        print("\nğŸ‰ ç»¼åˆæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç»¼åˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    comprehensive_test_suite()
