#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨åŸç”ŸQwen2.5-VLå¤„ç†å™¨æµ‹è¯•å›¾ç‰‡æ¨ç†ï¼ˆåº”ç”¨GitHub issueä¿®å¤ï¼‰
"""

import torch
from PIL import Image
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
import json
import os

# æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹
from numeric_qwen2_5_vl import (
    NumericQwen2_5_VLConfig,
    NumericQwen2_5_VLForConditionalGeneration
)

print("ğŸ¯ ä½¿ç”¨åŸç”Ÿå¤„ç†å™¨æµ‹è¯•å›¾ç‰‡æ¨ç†ï¼ˆåº”ç”¨GitHubä¿®å¤ï¼‰")
print("=" * 60)

MODEL_PATH = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"

def test_with_native_processor_fixed():
    """ä½¿ç”¨åŸç”Ÿå¤„ç†å™¨æµ‹è¯•ï¼ˆåº”ç”¨ä¿®å¤ï¼‰"""
    try:
        print("ğŸ”§ åŠ è½½åŸç”Ÿå¤„ç†å™¨...")
        
        # ä½¿ç”¨åŸç”ŸQwen2.5-VLå¤„ç†å™¨
        processor = AutoProcessor.from_pretrained(MODEL_PATH)
        
        # æ‰‹åŠ¨è®¾ç½®num_additional_image_tokensï¼ˆæ ¹æ®GitHub issueä¿®å¤ï¼‰
        processor.num_additional_image_tokens = 1
        print(f"âœ… è®¾ç½® num_additional_image_tokens: {processor.num_additional_image_tokens}")
        
        print("âœ… åŸç”Ÿå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        image = Image.new('RGB', (336, 336), color='blue')  # ä½¿ç”¨336x336ç¬¦åˆæ¨¡å‹é…ç½®
        from PIL import ImageDraw
        draw = ImageDraw.Draw(image)
        draw.rectangle([50, 50, 150, 150], fill='red')
        draw.ellipse([200, 200, 280, 280], fill='yellow')
        print("âœ… æµ‹è¯•å›¾åƒåˆ›å»ºæˆåŠŸ")
        
        # æ„å»ºç®€å•çš„å¯¹è¯
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image,
                    },
                    {"type": "text", "text": "è¿™ä¸ªå›¾åƒé‡Œæœ‰ä»€ä¹ˆï¼Ÿ"}
                ],
            }
        ]
        
        # åº”ç”¨chat template
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        print("âœ… Chat templateåº”ç”¨æˆåŠŸ")
        print(f"Generated text preview: {text[:200]}...")
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[text],
            images=[image],
            padding=True,
            return_tensors="pt"
        )
        print("âœ… è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"Input shape: {inputs.input_ids.shape}")
        print(f"Image shape: {inputs.pixel_values.shape}")
        
        # åŠ è½½è‡ªå®šä¹‰æ¨¡å‹
        print("ğŸš€ åŠ è½½è‡ªå®šä¹‰æ¨¡å‹...")
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="cuda"
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = inputs.to(model.device)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("ğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            outputs = model(**inputs)
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
        
        # ç”Ÿæˆ
        print("ğŸ¯ å¼€å§‹ç”Ÿæˆ...")
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=64,
                do_sample=True,
                temperature=0.7
            )
        
        # è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        print("ğŸ‰ å›¾åƒæ¨ç†æˆåŠŸ!")
        print("=" * 50)
        print("æ¨¡å‹è¾“å‡º:")
        print(output_text[0])
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def manual_prompt_test():
    """æ‰‹åŠ¨æ„å»ºpromptæµ‹è¯•"""
    try:
        print("\nğŸ”§ æ‰‹åŠ¨æ„å»ºpromptæµ‹è¯•...")
        
        # ç›´æ¥åŠ è½½æ¨¡å‹å’Œtokenizer
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="cuda"
        )
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH)
        processor.num_additional_image_tokens = 1
        
        # åˆ›å»ºå›¾åƒ
        image = Image.new('RGB', (336, 336), color='green')
        
        # æ‰‹åŠ¨æ„å»ºåŒ…å«å›¾åƒtokensçš„prompt
        prompt = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>æè¿°è¿™ä¸ªå›¾åƒã€‚<|im_end|>\n<|im_start|>assistant\n"
        
        print(f"ä½¿ç”¨æ‰‹åŠ¨prompt: {prompt}")
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[prompt],
            images=[image],
            return_tensors="pt"
        )
        
        inputs = inputs.to(model.device)
        
        print(f"Input IDs shape: {inputs.input_ids.shape}")
        print(f"Pixel values shape: {inputs.pixel_values.shape}")
        
        # å°è¯•ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=32,
                do_sample=True,
                temperature=0.7
            )
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        print("ğŸ‰ æ‰‹åŠ¨promptæµ‹è¯•æˆåŠŸ!")
        print("=" * 50)
        print("ç”Ÿæˆç»“æœ:")
        print(output_text[0])
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰‹åŠ¨promptæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•...")
    
    # æµ‹è¯•åŸç”Ÿå¤„ç†å™¨
    success1 = test_with_native_processor_fixed()
    
    # æµ‹è¯•æ‰‹åŠ¨prompt
    success2 = manual_prompt_test()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ æœ€ç»ˆç»“æœ:")
    print(f"âœ… åŸç”Ÿå¤„ç†å™¨æµ‹è¯•: {'æˆåŠŸ' if success1 else 'å¤±è´¥'}")
    print(f"âœ… æ‰‹åŠ¨promptæµ‹è¯•: {'æˆåŠŸ' if success2 else 'å¤±è´¥'}")
    
    if success1 or success2:
        print("ğŸ‰ è‡³å°‘ä¸€ç§æ–¹æ³•æˆåŠŸï¼å›¾åƒæ¨ç†åŠŸèƒ½æ­£å¸¸ï¼")
    else:
        print("âš ï¸ éƒ½å¤±è´¥äº†ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
