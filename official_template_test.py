#!/usr/bin/env python3
"""
åŸºäºå®˜æ–¹Qwen2.5-VL Chat Templateæ–‡æ¡£çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
å‚è€ƒæ–‡æ¡£: Qwen2.5-VL Chat Template Format
"""
import os
import torch
from PIL import Image
import json

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def fix_chat_template_official(checkpoint_path):
    """
    ä¿®å¤chat templateé…ç½® - ä½¿ç”¨å®˜æ–¹æ–‡æ¡£ä¸­çš„å®Œæ•´æ¨¡æ¿
    """
    tokenizer_config_path = os.path.join(checkpoint_path, "tokenizer_config.json")
    chat_template_path = os.path.join(checkpoint_path, "chat_template.json")
    
    print(f"ğŸ”§ ä¿®å¤å®˜æ–¹chat templateé…ç½®...")
    
    # å®˜æ–¹å®Œæ•´çš„Chat Template
    official_chat_template = "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
    
    try:
        # 1. æ›´æ–°tokenizer_config.json
        with open(tokenizer_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        config['chat_template'] = official_chat_template
        
        # å¤‡ä»½å¹¶ä¿å­˜
        import shutil
        shutil.copy2(tokenizer_config_path, f"{tokenizer_config_path}.backup")
        
        with open(tokenizer_config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print("   âœ… tokenizer_config.json å·²æ›´æ–°")
        
        # 2. åˆ›å»ºç‹¬ç«‹çš„chat_template.jsonæ–‡ä»¶
        chat_template_content = {
            "chat_template": official_chat_template
        }
        
        with open(chat_template_path, 'w', encoding='utf-8') as f:
            json.dump(chat_template_content, f, indent=2, ensure_ascii=False)
        
        print("   âœ… chat_template.json å·²åˆ›å»º")
        
    except Exception as e:
        print(f"   âŒ ä¿®å¤å¤±è´¥: {e}")

def load_model_with_official_template(checkpoint_path):
    """åŠ è½½æ¨¡å‹å¹¶åº”ç”¨å®˜æ–¹æ¨¡æ¿ä¿®å¤"""
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹å¹¶åº”ç”¨å®˜æ–¹Chat Templateä¿®å¤...")
    
    # ä¿®å¤chat template
    fix_chat_template_official(checkpoint_path)
    
    try:
        # åŠ è½½æ¨¡å‹
        model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
            checkpoint_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½å¤„ç†å™¨ - é‡æ–°ä»ä¿®å¤åçš„é…ç½®åŠ è½½
        processor = NumericQwen2_5_VLProcessor.from_pretrained(
            checkpoint_path,
            trust_remote_code=True
        )
        
        # æ‰‹åŠ¨è®¾ç½®chat templateï¼ˆå¦‚æœå¤„ç†å™¨ä»ç„¶æ²¡æœ‰åŠ è½½åˆ°ï¼‰
        if not hasattr(processor, 'chat_template') or not processor.chat_template:
            print("ğŸ”„ æ‰‹åŠ¨è®¾ç½®chat template...")
            
            # è¯»å–chat template
            chat_template_path = os.path.join(checkpoint_path, "chat_template.json")
            with open(chat_template_path, 'r', encoding='utf-8') as f:
                template_data = json.load(f)
            
            # è®¾ç½®åˆ°å¤„ç†å™¨
            processor.chat_template = template_data['chat_template']
            print("âœ… æ‰‹åŠ¨è®¾ç½®chat templateæˆåŠŸ")
        
        print("âœ… æ¨¡å‹å’Œå¤„ç†å™¨åŠ è½½æˆåŠŸ")
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def create_test_image(size=(224, 224)):
    """åˆ›å»ºæµ‹è¯•å›¾åƒ"""
    print(f"ğŸ¨ åˆ›å»ºæµ‹è¯•å›¾åƒ: {size}")
    
    image = Image.new('RGB', size, color='lightblue')
    
    # æ·»åŠ è§†è§‰ç‰¹å¾
    import numpy as np
    img_array = np.array(image)
    
    # æ·»åŠ çº¢è‰²å¯¹è§’çº¿
    for i in range(min(size)):
        if i < img_array.shape[0] and i < img_array.shape[1]:
            img_array[i, i] = [255, 0, 0]
    
    # æ·»åŠ ç»¿è‰²æ¡çº¹
    for i in range(0, size[1], 20):
        if i < img_array.shape[1]:
            img_array[:, i:i+5] = [0, 255, 0]
    
    return Image.fromarray(img_array)

def test_official_chat_template(model, processor, image, question):
    """ä½¿ç”¨å®˜æ–¹Chat Templateæ ¼å¼è¿›è¡Œæµ‹è¯•"""
    try:
        print(f"ğŸ¤” é—®é¢˜: {question}")
        
        # æ„å»ºå®˜æ–¹æ ¼å¼çš„æ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": question}
                ]
            }
        ]
        
        print("ğŸ“ æ„å»ºå®˜æ–¹æ ¼å¼æ¶ˆæ¯æˆåŠŸ")
        
        # ä½¿ç”¨apply_chat_template
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True,
            add_vision_id=False  # ä¸æ·»åŠ å›¾ç‰‡ç¼–å·
        )
        
        print(f"âœ… å®˜æ–¹Chat Templateåº”ç”¨æˆåŠŸ")
        print(f"ğŸ“„ ç”Ÿæˆçš„æ¨¡æ¿é•¿åº¦: {len(text)}")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ¨¡æ¿ç‰‡æ®µ
        print(f"ğŸ“‹ æ¨¡æ¿é¢„è§ˆ: {text[:200]}...")
        
        # å¤„ç†è¾“å…¥
        image_inputs = processor(
            text=[text],
            images=[image],
            return_tensors="pt",
            padding=True
        )
        
        print("ğŸ“¦ è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š input_ids shape: {image_inputs.input_ids.shape}")
        
        if hasattr(image_inputs, 'pixel_values') and image_inputs.pixel_values is not None:
            print(f"ğŸ–¼ï¸ pixel_values shape: {image_inputs.pixel_values.shape}")
        
        # æ£€æŸ¥vision tokens
        decoded_text = processor.tokenizer.decode(image_inputs.input_ids[0], skip_special_tokens=False)
        vision_start_count = decoded_text.count('<|vision_start|>')
        vision_end_count = decoded_text.count('<|vision_end|>')
        image_pad_count = decoded_text.count('<|image_pad|>')
        
        print(f"ğŸ” Vision Tokenç»Ÿè®¡:")
        print(f"   <|vision_start|>: {vision_start_count}")
        print(f"   <|vision_end|>: {vision_end_count}")
        print(f"   <|image_pad|>: {image_pad_count}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device_inputs = {}
        for k, v in image_inputs.items():
            if hasattr(v, 'to'):
                device_inputs[k] = v.to(model.device)
            else:
                device_inputs[k] = v
        
        # ç”Ÿæˆå›ç­”
        print("ğŸš€ å¼€å§‹ç”Ÿæˆå›ç­”...")
        with torch.no_grad():
            outputs = model.generate(
                **device_inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.8,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ¯ æ¨¡å‹å›ç­”: {response}")
        return response
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®˜æ–¹Chat Templateæ ¼å¼æµ‹è¯•")
    print("=" * 70)
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    try:
        # åŠ è½½æ¨¡å‹ï¼ˆåº”ç”¨å®˜æ–¹æ¨¡æ¿ä¿®å¤ï¼‰
        model, processor = load_model_with_official_template(checkpoint_path)
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = create_test_image((224, 224))
        
        # æµ‹è¯•é—®é¢˜åˆ—è¡¨
        test_questions = [
            "è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆé¢œè‰²ï¼Ÿ",
            "æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡çš„ç‰¹å¾ã€‚",
            "ä½ çœ‹åˆ°äº†ä»€ä¹ˆå›¾æ¡ˆï¼Ÿ"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\nğŸ“‹ æµ‹è¯• {i}/{len(test_questions)}")
            print("-" * 50)
            
            result = test_official_chat_template(model, processor, test_image, question)
            
            if result:
                print("âœ… æµ‹è¯•æˆåŠŸï¼")
            else:
                print("âŒ æµ‹è¯•å¤±è´¥")
        
        print("\nğŸ‰ å®˜æ–¹Chat Templateæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¸»æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
