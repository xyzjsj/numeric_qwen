#!/usr/bin/env python3
"""
é›†æˆSwanLabçš„æ•°å€¼å¢å¼ºè®­ç»ƒå™¨

ä¸“é—¨ä¸ºæ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹è®¾è®¡çš„SwanLabé›†æˆè®­ç»ƒå™¨
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch
import numpy as np
from typing import Dict, List, Optional, Any
from transformers import Trainer
import swanlab
import time
from training_config import log_to_swanlab


class SwanLabNumericTrainer(Trainer):
    """
    é›†æˆSwanLabçš„æ•°å€¼å¢å¼ºè®­ç»ƒå™¨
    """
    
    def __init__(self, swanlab_run=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.swanlab_run = swanlab_run
        self.start_time = time.time()
        self.step_start_time = time.time()
        
    def log(self, logs: Dict[str, float], start_time=None) -> None:
        """
        é‡å†™logæ–¹æ³•ï¼ŒåŒæ—¶è®°å½•åˆ°SwanLab
        å…¼å®¹Transformersä¼ é€’çš„start_timeå‚æ•°
        """
        try:
            # è°ƒç”¨çˆ¶ç±»çš„logæ–¹æ³•ï¼Œä¼ é€’æ‰€æœ‰å‚æ•°
            if start_time is not None:
                super().log(logs, start_time)
            else:
                super().log(logs)
            
            if self.swanlab_run is not None and logs:
                # æ·»åŠ æ—¶é—´ä¿¡æ¯
                current_time = time.time()
                logs_with_time = logs.copy()
                logs_with_time.update({
                    "time/elapsed_time": current_time - self.start_time,
                    "time/step_time": current_time - self.step_start_time,
                })
                
                # è®°å½•åˆ°SwanLab
                log_to_swanlab(
                    self.swanlab_run, 
                    logs_with_time, 
                    step=self.state.global_step
                )
                
                self.step_start_time = current_time
        except Exception as e:
            print(f"âš ï¸  æ—¥å¿—è®°å½•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        è®¡ç®—æŸå¤±ï¼ŒåŒ…æ‹¬æ•°å€¼æŸå¤±
        """
        try:
            # è·å–æ¨¡å‹è¾“å‡ºï¼Œç¡®ä¿è¿”å›å­—å…¸æ ¼å¼
            outputs = model(**inputs, return_dict=True)
            
            # è·å–æŸå¤±
            if hasattr(outputs, 'loss') and outputs.loss is not None:
                loss = outputs.loss
            elif isinstance(outputs, dict) and 'loss' in outputs:
                loss = outputs['loss']
            else:
                # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„lossï¼Œéœ€è¦æ‰‹åŠ¨è®¡ç®—
                if 'labels' in inputs:
                    logits = outputs.logits if hasattr(outputs, 'logits') else outputs['logits']
                    labels = inputs['labels']
                    
                    # è®¡ç®—äº¤å‰ç†µæŸå¤±
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    
                    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
                    loss = loss_fct(
                        shift_logits.view(-1, shift_logits.size(-1)),
                        shift_labels.view(-1)
                    )
                else:
                    raise ValueError("æ— æ³•è®¡ç®—æŸå¤±ï¼šæ¨¡å‹è¾“å‡ºä¸­æ²¡æœ‰lossï¼Œè¾“å…¥ä¸­ä¹Ÿæ²¡æœ‰labels")
            
            # è®°å½•æŸå¤±ç»„ä»¶åˆ°SwanLab
            if self.swanlab_run is not None:
                loss_components = {
                    "loss/total_loss": loss.item(),
                }
                
                # å¦‚æœæœ‰æ•°å€¼æŸå¤±ç»„ä»¶ï¼Œä¹Ÿè®°å½•
                if hasattr(outputs, 'numeric_loss') and outputs.numeric_loss is not None:
                    loss_components["loss/numeric_loss"] = outputs.numeric_loss.item()
                if hasattr(outputs, 'language_loss') and outputs.language_loss is not None:
                    loss_components["loss/language_loss"] = outputs.language_loss.item()
                
                log_to_swanlab(
                    self.swanlab_run,
                    loss_components,
                    step=self.state.global_step
                )
            
            # ç¡®ä¿æ­£ç¡®å¤„ç†è¿”å›å€¼
            if return_outputs:
                return loss, outputs
            else:
                return loss
            
        except Exception as e:
            print(f"è®¡ç®—æŸå¤±æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›ä¸€ä¸ªé»˜è®¤æŸå¤±
            dummy_loss = torch.tensor(0.0, requires_grad=True)
            if hasattr(model, 'device'):
                dummy_loss = dummy_loss.to(model.device)
            elif hasattr(model, 'parameters'):
                dummy_loss = dummy_loss.to(next(model.parameters()).device)
            
            if return_outputs:
                return dummy_loss, None
            else:
                return dummy_loss
    
    def _save_checkpoint(self, model, trial, metrics=None):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹æ—¶è®°å½•åˆ°SwanLabï¼Œå¹¶ä¿å­˜å¤„ç†å™¨
        å‚è€ƒ HuggingFace æœ€ä½³å®è·µï¼Œç¡®ä¿ä¿å­˜æ‰€æœ‰å¿…è¦ç»„ä»¶
        """
        import json
        import os
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•ä¿å­˜æ¨¡å‹æƒé‡
        checkpoint_path = super()._save_checkpoint(model, trial)
        
        if checkpoint_path:
            print(f"æ­£åœ¨ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹åˆ°: {checkpoint_path}")
            
            # 1. ä¿å­˜å¤„ç†å™¨ï¼ˆåŒ…å«tokenizerå’Œimage_processorï¼‰
            if hasattr(self, 'processor') and self.processor is not None:
                try:
                    print("ä¿å­˜å¤„ç†å™¨...")
                    self.processor.save_pretrained(checkpoint_path)
                    print("âœ… å¤„ç†å™¨ä¿å­˜æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†å™¨ä¿å­˜å¤±è´¥: {e}")
            
            # 2. ç¡®ä¿ preprocessor_config.json å­˜åœ¨
            preprocessor_config_path = os.path.join(checkpoint_path, "preprocessor_config.json")
            if not os.path.exists(preprocessor_config_path):
                print("åˆ›å»º preprocessor_config.json...")
                preprocessor_config = {
                    "processor_class": "NumericQwen2_5_VLProcessor",
                    "auto_map": {
                        "AutoProcessor": "numeric_qwen2_5_vl.NumericQwen2_5_VLProcessor"
                    },
                    "image_processor": {
                        "do_convert_rgb": True,
                        "do_normalize": True,
                        "do_rescale": True,
                        "do_resize": True,
                        "image_mean": [0.48145466, 0.4578275, 0.40821073],
                        "image_std": [0.26862954, 0.26130258, 0.27577711],
                        "resample": 3,
                        "size": {"shortest_edge": 336}
                    },
                    "image_processor_type": "Qwen2VLImageProcessor",  # æ·»åŠ è¿™ä¸ªå…³é”®å­—æ®µ
                    "tokenizer": {
                        "padding_side": "left",
                        "truncation_side": "left",
                        "model_max_length": 32768,
                        "tokenizer_class": "Qwen2Tokenizer"
                    },
                    "num_token_id": 151665,
                    "num_pad_token_id": 151666,
                    "numeric_tokens": ["<num>", "<num_pad>"]
                }
                
                with open(preprocessor_config_path, 'w', encoding='utf-8') as f:
                    json.dump(preprocessor_config, f, indent=2, ensure_ascii=False)
                print("âœ… preprocessor_config.json åˆ›å»ºæˆåŠŸ")
            else:
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¹¶æ›´æ–°å¿…è¦å­—æ®µ
                print("æ›´æ–° preprocessor_config.json...")
                with open(preprocessor_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                # ç¡®ä¿åŒ…å«æ•°å€¼tokené…ç½®å’Œimage_processor_type
                config.update({
                    "processor_class": "NumericQwen2_5_VLProcessor",
                    "auto_map": {
                        "AutoProcessor": "numeric_qwen2_5_vl.NumericQwen2_5_VLProcessor"
                    },
                    "image_processor_type": "Qwen2VLImageProcessor",  # ç¡®ä¿æœ‰è¿™ä¸ªå­—æ®µ
                    "num_token_id": 151665,
                    "num_pad_token_id": 151666,
                    "numeric_tokens": ["<num>", "<num_pad>"]
                })
                
                with open(preprocessor_config_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, indent=2, ensure_ascii=False)
                print("âœ… preprocessor_config.json æ›´æ–°æˆåŠŸ")
            
            # 3. ä¿å­˜ç”Ÿæˆé…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(model, 'generation_config') and model.generation_config is not None:
                try:
                    model.generation_config.save_pretrained(checkpoint_path)
                    print("âœ… generation_config ä¿å­˜æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ generation_config ä¿å­˜å¤±è´¥: {e}")
            
            # 4. éªŒè¯ä¿å­˜çš„æ–‡ä»¶
            saved_files = os.listdir(checkpoint_path)
            print(f"æ£€æŸ¥ç‚¹ä¿å­˜çš„æ–‡ä»¶: {saved_files}")
            
            # æ£€æŸ¥å…³é”®æ–‡ä»¶
            required_files = [
                "config.json",
                "added_tokens.json", 
                "special_tokens_map.json",
                "tokenizer_config.json",
                "preprocessor_config.json"
            ]
            
            missing_files = []
            for file_name in required_files:
                if file_name not in saved_files:
                    missing_files.append(file_name)
            
            if missing_files:
                print(f"âš ï¸ ç¼ºå¤±æ–‡ä»¶: {missing_files}")
            else:
                print("âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å·²ä¿å­˜")
        
        # è®°å½•æ£€æŸ¥ç‚¹ä¿¡æ¯åˆ°SwanLab
        if self.swanlab_run is not None and checkpoint_path:
            checkpoint_info = {
                "checkpoint/path": str(checkpoint_path),
                "checkpoint/step": self.state.global_step,
                "checkpoint/epoch": self.state.epoch,
            }
            
            if metrics:
                for key, value in metrics.items():
                    checkpoint_info[f"checkpoint/{key}"] = value
            
            log_to_swanlab(
                self.swanlab_run,
                checkpoint_info,
                step=self.state.global_step
            )
        
        return checkpoint_path
    
    def train(self, resume_from_checkpoint: Optional[str] = None, trial=None, ignore_keys_for_eval=None, **kwargs):
        """
        é‡å†™è®­ç»ƒæ–¹æ³•ï¼Œæ·»åŠ SwanLabé›†æˆ
        """
        # è®°å½•è®­ç»ƒå¼€å§‹
        if self.swanlab_run is not None:
            log_to_swanlab(
                self.swanlab_run,
                {
                    "training/status": 1.0,  # 1.0è¡¨ç¤ºå¼€å§‹
                    "training/resume_from_checkpoint": 1.0 if resume_from_checkpoint is not None else 0.0,
                },
                step=0
            )
        
        try:
            # è°ƒç”¨çˆ¶ç±»çš„è®­ç»ƒæ–¹æ³•
            result = super().train(
                resume_from_checkpoint=resume_from_checkpoint,
                trial=trial,
                ignore_keys_for_eval=ignore_keys_for_eval,
                **kwargs
            )
            
            # è®°å½•è®­ç»ƒå®Œæˆ
            if self.swanlab_run is not None:
                final_metrics = {
                    "training/status": 2.0,  # 2.0è¡¨ç¤ºå®Œæˆ
                    "training/total_time": time.time() - self.start_time,
                    "training/total_steps": self.state.global_step,
                    "training/total_epochs": self.state.epoch,
                }
                
                # æ·»åŠ æœ€ç»ˆçš„è®­ç»ƒæŒ‡æ ‡
                if hasattr(result, 'training_loss'):
                    final_metrics["training/final_loss"] = result.training_loss
                
                log_to_swanlab(
                    self.swanlab_run,
                    final_metrics,
                    step=self.state.global_step
                )
            
            return result
            
        except Exception as e:
            # è®°å½•è®­ç»ƒå¤±è´¥
            if self.swanlab_run is not None:
                log_to_swanlab(
                    self.swanlab_run,
                    {
                        "training/status": -1.0,  # -1.0è¡¨ç¤ºå¤±è´¥
                        "training/error_occurred": 1.0,
                        "training/total_time": time.time() - self.start_time,
                    },
                    step=self.state.global_step
                )
            raise
    
    def evaluation_loop(self, dataloader, description: str, prediction_loss_only=None, ignore_keys=None, metric_key_prefix="eval"):
        """
        é‡å†™è¯„ä¼°å¾ªç¯ï¼Œè®°å½•è¯„ä¼°æŒ‡æ ‡åˆ°SwanLab
        """
        if self.swanlab_run is not None:
            log_to_swanlab(
                self.swanlab_run,
                {f"{metric_key_prefix}/status": 1.0},  # 1.0è¡¨ç¤ºå¼€å§‹è¯„ä¼°
                step=self.state.global_step
            )
        
        try:
            # è°ƒç”¨çˆ¶ç±»çš„è¯„ä¼°æ–¹æ³•
            result = super().evaluation_loop(
                dataloader=dataloader,
                description=description,
                prediction_loss_only=prediction_loss_only,
                ignore_keys=ignore_keys,
                metric_key_prefix=metric_key_prefix
            )
            
            # è®°å½•è¯„ä¼°ç»“æœåˆ°SwanLab
            if self.swanlab_run is not None and result.metrics:
                eval_metrics = {}
                for key, value in result.metrics.items():
                    if isinstance(value, (int, float, np.number)):
                        eval_metrics[key] = float(value)
                
                log_to_swanlab(
                    self.swanlab_run,
                    eval_metrics,
                    step=self.state.global_step
                )
            
            return result
            
        except Exception as e:
            if self.swanlab_run is not None:
                log_to_swanlab(
                    self.swanlab_run,
                    {f"{metric_key_prefix}/status": -1.0, f"{metric_key_prefix}/error_occurred": 1.0},
                    step=self.state.global_step
                )
            raise


def create_swanlab_trainer(
    model,
    args,
    train_dataset=None,
    eval_dataset=None,
    tokenizer=None,
    data_collator=None,
    compute_metrics=None,
    swanlab_run=None,
    processor=None,  # æ·»åŠ å¤„ç†å™¨å‚æ•°
    **kwargs
):
    """
    åˆ›å»ºé›†æˆSwanLabçš„è®­ç»ƒå™¨
    
    Args:
        model: è¦è®­ç»ƒçš„æ¨¡å‹
        args: è®­ç»ƒå‚æ•°
        train_dataset: è®­ç»ƒæ•°æ®é›†
        eval_dataset: è¯„ä¼°æ•°æ®é›†
        tokenizer: åˆ†è¯å™¨
        data_collator: æ•°æ®æ•´ç†å™¨
        compute_metrics: æŒ‡æ ‡è®¡ç®—å‡½æ•°
        swanlab_run: SwanLabè¿è¡Œå¯¹è±¡
        processor: å¤„ç†å™¨ï¼ˆç”¨äºä¿å­˜ï¼‰
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        SwanLabNumericTrainer: é›†æˆSwanLabçš„è®­ç»ƒå™¨
    """
    
    trainer = SwanLabNumericTrainer(
        swanlab_run=swanlab_run,
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        **kwargs
    )
    
    # å°†å¤„ç†å™¨é™„åŠ åˆ°è®­ç»ƒå™¨ä¸Šï¼Œä»¥ä¾¿åœ¨ä¿å­˜æ—¶ä½¿ç”¨
    if processor is not None:
        trainer.processor = processor
    
    return trainer


def log_sample_data_to_swanlab(swanlab_run, dataset, processor, num_samples=5):
    """
    è®°å½•æ•°æ®æ ·æœ¬åˆ°SwanLab
    
    Args:
        swanlab_run: SwanLabè¿è¡Œå¯¹è±¡
        dataset: æ•°æ®é›†
        processor: å¤„ç†å™¨
        num_samples: è¦è®°å½•çš„æ ·æœ¬æ•°é‡
    """
    if swanlab_run is None or len(dataset) == 0:
        return
    
    try:
        sample_data = []
        for i in range(min(num_samples, len(dataset))):
            item = dataset[i]
            
            # å‡†å¤‡æ ·æœ¬ä¿¡æ¯
            sample_info = {
                "sample_id": i,
                "input_length": len(item.get('input_ids', [])),
                "has_image": 'pixel_values' in item,
                "has_numeric": len(item.get('numeric_values', [])) > 0,
            }
            
            if item.get('numeric_values'):
                if item['numeric_values']:
                    first_group = item['numeric_values'][0]
                    if not isinstance(first_group, (list, tuple)):
                        first_group = [first_group]
                    sample_info["numeric_count"] = len(first_group)
                    sample_info["numeric_values"] = str(first_group[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°å€¼
                else:
                    sample_info["numeric_count"] = 0
            
            # è§£ç æ–‡æœ¬æ ·æœ¬
            if 'input_ids' in item and processor:
                try:
                    text_sample = processor.tokenizer.decode(
                        item['input_ids'][:100],  # åªæ˜¾ç¤ºå‰100ä¸ªtoken
                        skip_special_tokens=False
                    )
                    sample_info["text_preview"] = text_sample[:200] + "..." if len(text_sample) > 200 else text_sample
                except:
                    sample_info["text_preview"] = "æ— æ³•è§£ç "
            
            sample_data.append(sample_info)
        
        # è®°å½•åˆ°SwanLab - åªè®°å½•æ•°å€¼ç»Ÿè®¡ä¿¡æ¯
        sample_stats = {
            "data/total_samples": len(sample_data),
            "data/avg_input_length": sum(item["input_length"] for item in sample_data) / len(sample_data),
            "data/samples_with_image": sum(1 for item in sample_data if item["has_image"]),
            "data/samples_with_numeric": sum(1 for item in sample_data if item["has_numeric"]),
        }
        
        log_to_swanlab(
            swanlab_run,
            sample_stats
        )
        
        print(f"ğŸ“Š å·²è®°å½• {len(sample_data)} ä¸ªæ•°æ®æ ·æœ¬åˆ°SwanLab")
        
    except Exception as e:
        print(f"âš ï¸  è®°å½•æ•°æ®æ ·æœ¬å¤±è´¥: {e}")


def log_training_progress_to_swanlab(swanlab_run, progress_info: Dict[str, Any]):
    """
    è®°å½•è®­ç»ƒè¿›åº¦ä¿¡æ¯åˆ°SwanLab
    
    Args:
        swanlab_run: SwanLabè¿è¡Œå¯¹è±¡
        progress_info: è¿›åº¦ä¿¡æ¯å­—å…¸
    """
    if swanlab_run is None:
        return
    
    try:
        log_to_swanlab(swanlab_run, progress_info)
    except Exception as e:
        print(f"âš ï¸  è®°å½•è®­ç»ƒè¿›åº¦å¤±è´¥: {e}")
