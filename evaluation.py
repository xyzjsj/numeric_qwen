#!/usr/bin/env python3
"""
æ•°å€¼å¢å¼ºQwen2.5-VLæ¨¡å‹è¯„ä¼°æ¨¡å—

æä¾›å®Œæ•´çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œå¯¹æ¯”åŠŸèƒ½
"""
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import json
import torch
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import time
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader
import swanlab

# BLEUå’ŒROUGEè¯„ä¼°
try:
    from nltk.translate.bleu_score import sentence_bleu
    from rouge import Rouge
    EVAL_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…nltkæˆ–rougeï¼Œæ–‡æœ¬è¯„ä¼°åŠŸèƒ½å°†è¢«ç¦ç”¨")
    EVAL_AVAILABLE = False


class NumericModelEvaluator:
    """æ•°å€¼å¢å¼ºæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model, processor, device='cuda'):
        self.model = model
        self.processor = processor
        self.device = device
        self.model.eval()
        
    def evaluate_numeric_performance(self, dataset, batch_size=4) -> Dict[str, float]:
        """è¯„ä¼°æ•°å€¼é¢„æµ‹æ€§èƒ½"""
        predictions = []
        targets = []
        
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        
        with torch.no_grad():
            for batch in dataloader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**batch)
                
                # æå–æ•°å€¼é¢„æµ‹å’ŒçœŸå®å€¼
                if hasattr(outputs, 'predicted_floats'):
                    pred_floats = outputs.predicted_floats
                    
                    # æå–çœŸå®æ•°å€¼
                    if 'numeric_values' in batch and batch['numeric_values']:
                        for i, sample_values in enumerate(batch['numeric_values']):
                            if sample_values:  # å¦‚æœæœ‰æ•°å€¼æ ‡ç­¾
                                # è·å–<num>tokenä½ç½®
                                num_token_id = getattr(self.model.config, 'num_token_id', None)
                                if num_token_id is not None:
                                    input_ids = batch['input_ids'][i]
                                    mask = (input_ids == num_token_id)
                                    
                                    if mask.any():
                                        pred_values = pred_floats[i][mask].cpu().numpy()
                                        true_values = np.array(sample_values[0])  # å±•å¹³åµŒå¥—åˆ—è¡¨
                                        
                                        # ç¡®ä¿é•¿åº¦åŒ¹é…
                                        min_len = min(len(pred_values), len(true_values))
                                        predictions.extend(pred_values[:min_len])
                                        targets.extend(true_values[:min_len])
        
        if len(predictions) == 0 or len(targets) == 0:
            return {
                'MSE': float('inf'),
                'RMSE': float('inf'),
                'MAE': float('inf'),
                'MAPE': float('inf'),
                'RÂ²': 0.0,
                'sample_count': 0
            }
        
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(targets, predictions)
        
        # è®¡ç®—MAPE (é¿å…é™¤é›¶)
        mask = targets != 0
        if mask.any():
            mape = np.mean(np.abs((targets[mask] - predictions[mask]) / targets[mask])) * 100
        else:
            mape = float('inf')
        
        r2 = r2_score(targets, predictions)
        
        return {
            'MSE': float(mse),
            'RMSE': float(rmse),
            'MAE': float(mae),
            'MAPE': float(mape),
            'RÂ²': float(r2),
            'sample_count': len(predictions)
        }
    
    def evaluate_text_generation(self, dataset, batch_size=2, max_new_tokens=128) -> Dict[str, float]:
        """è¯„ä¼°æ–‡æœ¬ç”Ÿæˆè´¨é‡"""
        if not EVAL_AVAILABLE:
            return {'BLEU-4': 0.0, 'ROUGE-L': 0.0, 'ROUGE-1': 0.0, 'ROUGE-2': 0.0}
        
        generated_texts = []
        reference_texts = []
        
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        rouge = Rouge()
        
        with torch.no_grad():
            for batch in dataloader:
                # ç”Ÿæˆæ–‡æœ¬
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                # å¦‚æœæœ‰å›¾åƒï¼Œä¹Ÿä¼ é€’å›¾åƒ
                generation_kwargs = {
                    'input_ids': input_ids,
                    'attention_mask': attention_mask,
                    'max_new_tokens': max_new_tokens,
                    'do_sample': False,
                    'temperature': 1.0,
                    'pad_token_id': self.processor.tokenizer.eos_token_id
                }
                
                if 'pixel_values' in batch and batch['pixel_values'] is not None:
                    generation_kwargs['pixel_values'] = batch['pixel_values'].to(self.device)
                if 'image_grid_thw' in batch and batch['image_grid_thw'] is not None:
                    generation_kwargs['image_grid_thw'] = batch['image_grid_thw'].to(self.device)
                
                generated_ids = self.model.generate(**generation_kwargs)
                
                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                for i, gen_ids in enumerate(generated_ids):
                    # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„æ–°token
                    new_tokens = gen_ids[len(input_ids[i]):]
                    generated_text = self.processor.tokenizer.decode(new_tokens, skip_special_tokens=True)
                    
                    # è·å–å‚è€ƒæ–‡æœ¬
                    if 'labels' in batch:
                        ref_ids = batch['labels'][i]
                        # ç§»é™¤-100çš„padding
                        ref_ids = ref_ids[ref_ids != -100]
                        reference_text = self.processor.tokenizer.decode(ref_ids, skip_special_tokens=True)
                    else:
                        reference_text = "å‚è€ƒæ–‡æœ¬"  # é»˜è®¤å€¼
                    
                    generated_texts.append(generated_text.strip())
                    reference_texts.append(reference_text.strip())
        
        # è®¡ç®—BLEUåˆ†æ•°
        bleu_scores = []
        for gen, ref in zip(generated_texts, reference_texts):
            if gen and ref:
                # ç®€å•çš„BLEUè®¡ç®—
                gen_tokens = gen.split()
                ref_tokens = [ref.split()]  # BLEUéœ€è¦å‚è€ƒæ–‡æœ¬åˆ—è¡¨
                if len(gen_tokens) > 0 and len(ref_tokens[0]) > 0:
                    bleu = sentence_bleu(ref_tokens, gen_tokens)
                    bleu_scores.append(bleu)
        
        # è®¡ç®—ROUGEåˆ†æ•°
        rouge_scores = {'rouge-1': [], 'rouge-2': [], 'rouge-l': []}
        for gen, ref in zip(generated_texts, reference_texts):
            if gen and ref:
                try:
                    scores = rouge.get_scores(gen, ref)[0]
                    rouge_scores['rouge-1'].append(scores['rouge-1']['f'])
                    rouge_scores['rouge-2'].append(scores['rouge-2']['f'])
                    rouge_scores['rouge-l'].append(scores['rouge-l']['f'])
                except:
                    continue
        
        return {
            'BLEU-4': np.mean(bleu_scores) if bleu_scores else 0.0,
            'ROUGE-L': np.mean(rouge_scores['rouge-l']) if rouge_scores['rouge-l'] else 0.0,
            'ROUGE-1': np.mean(rouge_scores['rouge-1']) if rouge_scores['rouge-1'] else 0.0,
            'ROUGE-2': np.mean(rouge_scores['rouge-2']) if rouge_scores['rouge-2'] else 0.0,
            'generation_count': len(generated_texts)
        }
    
    def evaluate_inference_speed(self, dataset, batch_size=4, num_samples=50) -> Dict[str, float]:
        """è¯„ä¼°æ¨ç†é€Ÿåº¦"""
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        
        times = []
        
        with torch.no_grad():
            sample_count = 0
            for batch in dataloader:
                if sample_count >= num_samples:
                    break
                
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # é¢„çƒ­
                if sample_count == 0:
                    _ = self.model(**batch)
                
                # è®¡æ—¶
                torch.cuda.synchronize() if self.device == 'cuda' else None
                start_time = time.time()
                
                outputs = self.model(**batch)
                
                torch.cuda.synchronize() if self.device == 'cuda' else None
                end_time = time.time()
                
                times.append(end_time - start_time)
                sample_count += batch_size
        
        return {
            'avg_inference_time': np.mean(times),
            'std_inference_time': np.std(times),
            'samples_per_second': batch_size / np.mean(times) if times else 0.0
        }
    
    def comprehensive_evaluation(self, dataset, batch_size=4) -> Dict[str, Any]:
        """ç»¼åˆè¯„ä¼°"""
        print("ğŸ” å¼€å§‹ç»¼åˆè¯„ä¼°...")
        
        # æ•°å€¼æ€§èƒ½è¯„ä¼°
        print("  ğŸ“Š è¯„ä¼°æ•°å€¼é¢„æµ‹æ€§èƒ½...")
        numeric_metrics = self.evaluate_numeric_performance(dataset, batch_size)
        
        # æ–‡æœ¬ç”Ÿæˆè¯„ä¼°
        print("  ğŸ“ è¯„ä¼°æ–‡æœ¬ç”Ÿæˆè´¨é‡...")
        text_metrics = self.evaluate_text_generation(dataset, batch_size=2)
        
        # æ¨ç†é€Ÿåº¦è¯„ä¼°
        print("  âš¡ è¯„ä¼°æ¨ç†é€Ÿåº¦...")
        speed_metrics = self.evaluate_inference_speed(dataset, batch_size)
        
        return {
            'numeric': numeric_metrics,
            'text': text_metrics,
            'speed': speed_metrics,
            'evaluation_time': datetime.now().isoformat()
        }


def compare_models(model_results: Dict[str, Dict], save_path: str = None) -> Dict[str, str]:
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    
    comparison = {
        'winner_by_metric': {},
        'summary': {}
    }
    
    # æ•°å€¼é¢„æµ‹å¯¹æ¯” (è¶Šå°è¶Šå¥½çš„æŒ‡æ ‡)
    for metric in ['MSE', 'RMSE', 'MAE', 'MAPE']:
        best_model = min(model_results.keys(), 
                        key=lambda m: model_results[m]['numeric'].get(metric, float('inf')))
        comparison['winner_by_metric'][metric] = best_model
    
    # RÂ²å¯¹æ¯” (è¶Šå¤§è¶Šå¥½)
    if any('RÂ²' in model_results[m]['numeric'] for m in model_results):
        best_model = max(model_results.keys(),
                        key=lambda m: model_results[m]['numeric'].get('RÂ²', -float('inf')))
        comparison['winner_by_metric']['RÂ²'] = best_model
    
    # æ–‡æœ¬ç”Ÿæˆå¯¹æ¯” (è¶Šå¤§è¶Šå¥½)
    for metric in ['BLEU-4', 'ROUGE-L']:
        if any(metric in model_results[m]['text'] for m in model_results):
            best_model = max(model_results.keys(),
                            key=lambda m: model_results[m]['text'].get(metric, 0.0))
            comparison['winner_by_metric'][metric] = best_model
    
    # é€Ÿåº¦å¯¹æ¯” (samples_per_secondè¶Šå¤§è¶Šå¥½)
    best_model = max(model_results.keys(),
                    key=lambda m: model_results[m]['speed'].get('samples_per_second', 0.0))
    comparison['winner_by_metric']['speed'] = best_model
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report = generate_comparison_report(model_results, comparison)
    
    if save_path:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ“„ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
    
    return comparison


def generate_comparison_report(model_results: Dict, comparison: Dict) -> str:
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    
    report = []
    report.append("="*80)
    report.append("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
    report.append("="*80)
    report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # æ¨¡å‹åˆ—è¡¨
    models = list(model_results.keys())
    report.append(f"ğŸ“‹ å¯¹æ¯”æ¨¡å‹: {', '.join(models)}")
    report.append("")
    
    # æ•°å€¼é¢„æµ‹æ€§èƒ½å¯¹æ¯”
    report.append("ğŸ”¢ æ•°å€¼é¢„æµ‹æ€§èƒ½å¯¹æ¯”:")
    report.append("-" * 60)
    report.append(f"{'æŒ‡æ ‡':<15} " + " ".join(f"{model:<15}" for model in models) + f" {'æœ€ä¼˜æ¨¡å‹':<15}")
    report.append("-" * 60)
    
    for metric in ['MSE', 'RMSE', 'MAE', 'MAPE', 'RÂ²']:
        line = f"{metric:<15} "
        for model in models:
            value = model_results[model]['numeric'].get(metric, 0.0)
            if metric == 'MAPE' and value == float('inf'):
                line += f"{'âˆ':<15} "
            else:
                line += f"{value:<15.4f} "
        
        winner = comparison['winner_by_metric'].get(metric, 'N/A')
        line += f"{winner:<15}"
        report.append(line)
    
    report.append("")
    
    # æ–‡æœ¬ç”Ÿæˆè´¨é‡å¯¹æ¯”
    report.append("ğŸ“ æ–‡æœ¬ç”Ÿæˆè´¨é‡å¯¹æ¯”:")
    report.append("-" * 60)
    for metric in ['BLEU-4', 'ROUGE-L', 'ROUGE-1', 'ROUGE-2']:
        line = f"{metric:<15} "
        for model in models:
            value = model_results[model]['text'].get(metric, 0.0)
            line += f"{value:<15.4f} "
        
        winner = comparison['winner_by_metric'].get(metric, 'N/A')
        line += f"{winner:<15}"
        report.append(line)
    
    report.append("")
    
    # æ¨ç†é€Ÿåº¦å¯¹æ¯”
    report.append("âš¡ æ¨ç†é€Ÿåº¦å¯¹æ¯”:")
    report.append("-" * 60)
    for metric in ['avg_inference_time', 'samples_per_second']:
        line = f"{metric:<15} "
        for model in models:
            value = model_results[model]['speed'].get(metric, 0.0)
            line += f"{value:<15.4f} "
        report.append(line)
    
    report.append("")
    
    # ç»¼åˆè¯„åˆ†
    report.append("ğŸ† ç»¼åˆè¡¨ç°:")
    report.append("-" * 60)
    scores = calculate_overall_scores(model_results)
    for model, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):
        report.append(f"{model}: {score:.2f}åˆ†")
    
    best_overall = max(scores.items(), key=lambda x: x[1])
    report.append(f"\nğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_overall[0]} (ç»¼åˆå¾—åˆ†: {best_overall[1]:.2f})")
    
    return "\n".join(report)


def calculate_overall_scores(model_results: Dict) -> Dict[str, float]:
    """è®¡ç®—ç»¼åˆå¾—åˆ†"""
    scores = {}
    
    for model in model_results.keys():
        score = 0.0
        
        # æ•°å€¼é¢„æµ‹å¾—åˆ† (40%)
        numeric_score = 0.0
        rmse = model_results[model]['numeric'].get('RMSE', float('inf'))
        if rmse != float('inf') and rmse > 0:
            numeric_score = 100 / (1 + rmse)  # RMSEè¶Šå°å¾—åˆ†è¶Šé«˜
        
        r2 = model_results[model]['numeric'].get('RÂ²', 0.0)
        if r2 > 0:
            numeric_score += r2 * 100  # RÂ²æœ¬èº«å°±æ˜¯0-1çš„åˆ†æ•°
        
        score += numeric_score * 0.4
        
        # æ–‡æœ¬ç”Ÿæˆå¾—åˆ† (30%)
        bleu = model_results[model]['text'].get('BLEU-4', 0.0)
        rouge = model_results[model]['text'].get('ROUGE-L', 0.0)
        text_score = (bleu + rouge) * 50  # å½’ä¸€åŒ–åˆ°100åˆ†
        score += text_score * 0.3
        
        # é€Ÿåº¦å¾—åˆ† (30%)
        sps = model_results[model]['speed'].get('samples_per_second', 0.0)
        speed_score = min(sps * 10, 100)  # é€Ÿåº¦å¾—åˆ†ï¼Œæœ€é«˜100åˆ†
        score += speed_score * 0.3
        
        scores[model] = score
    
    return scores


def log_evaluation_to_swanlab(swanlab_run, evaluation_results: Dict, prefix: str = "eval"):
    """å°†è¯„ä¼°ç»“æœè®°å½•åˆ°SwanLab"""
    if swanlab_run is None:
        return
    
    try:
        # å±•å¹³ç»“æœç”¨äºè®°å½•
        flat_results = {}
        
        # æ•°å€¼æŒ‡æ ‡
        for metric, value in evaluation_results['numeric'].items():
            if isinstance(value, (int, float)) and not np.isinf(value):
                flat_results[f"{prefix}/numeric_{metric.lower()}"] = value
        
        # æ–‡æœ¬æŒ‡æ ‡
        for metric, value in evaluation_results['text'].items():
            if isinstance(value, (int, float)):
                flat_results[f"{prefix}/text_{metric.lower().replace('-', '_')}"] = value
        
        # é€Ÿåº¦æŒ‡æ ‡
        for metric, value in evaluation_results['speed'].items():
            if isinstance(value, (int, float)):
                flat_results[f"{prefix}/speed_{metric}"] = value
        
        swanlab_run.log(flat_results)
        print(f"âœ… è¯„ä¼°ç»“æœå·²è®°å½•åˆ°SwanLab ({prefix})")
        
    except Exception as e:
        print(f"âš ï¸  SwanLabè®°å½•å¤±è´¥: {e}")


def save_evaluation_results(results: Dict, filepath: str):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
    import json
    import os
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_numpy_types(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        else:
            return obj
    
    # è½¬æ¢ç»“æœ
    converted_results = convert_numpy_types(results)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(converted_results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    return filepath
