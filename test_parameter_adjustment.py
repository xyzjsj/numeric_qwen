#!/usr/bin/env python3
"""
åŸºäºGitCodeåšå®¢æ–‡ç« çš„å‚æ•°è°ƒæ•´è§£å†³æ–¹æ¡ˆ
è°ƒæ•´max_pixelså’Œmax_prompt_lengthå‚æ•°æ¥è§£å†³Tokenä¸åŒ¹é…é—®é¢˜
"""
import os
import torch
from PIL import Image

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def test_parameter_adjustment_solution():
    """æµ‹è¯•å‚æ•°è°ƒæ•´è§£å†³æ–¹æ¡ˆ"""
    print("ğŸš€ GitCodeåšå®¢å‚æ•°è°ƒæ•´è§£å†³æ–¹æ¡ˆæµ‹è¯•")
    print("=" * 60)
    
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“‚ åŠ è½½æ¨¡å‹...")
    model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
        checkpoint_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½å¤„ç†å™¨å¹¶è°ƒæ•´å‚æ•°
    print("ğŸ“‚ åŠ è½½å¤„ç†å™¨å¹¶è°ƒæ•´å‚æ•°...")
    processor = NumericQwen2_5_VLProcessor.from_pretrained(
        checkpoint_path,
        trust_remote_code=True
    )
    
    # æ ¹æ®GitCodeæ–‡ç« å»ºè®®è°ƒæ•´å‚æ•°
    print("ğŸ”§ è°ƒæ•´å›¾åƒå¤„ç†å™¨å‚æ•°...")
    if hasattr(processor, 'image_processor'):
        # æ–¹æ¡ˆ1: é™ä½max_pixelsï¼Œç¡®ä¿Tokenæ•°é‡ä¸è¶…è¿‡é™åˆ¶
        original_max_pixels = processor.image_processor.max_pixels
        original_min_pixels = processor.image_processor.min_pixels
        
        print(f"   ğŸ“Š åŸå§‹max_pixels: {original_max_pixels}")
        print(f"   ğŸ“Š åŸå§‹min_pixels: {original_min_pixels}")
        
        # è®¾ç½®æ›´å°çš„max_pixelsä»¥å‡å°‘Tokenæ•°é‡
        processor.image_processor.max_pixels = 3136  # 56*56 = 3136
        processor.image_processor.min_pixels = 784   # 28*28 = 784
        
        print(f"   ğŸ“Š è°ƒæ•´åmax_pixels: {processor.image_processor.max_pixels}")
        print(f"   ğŸ“Š è°ƒæ•´åmin_pixels: {processor.image_processor.min_pixels}")
    
    # æµ‹è¯•ä¸åŒå°ºå¯¸çš„å›¾åƒ
    test_sizes = [
        (28, 28),   # 1ä¸ªToken
        (56, 56),   # 4ä¸ªToken
        (84, 84),   # 9ä¸ªToken
    ]
    
    for size in test_sizes:
        print(f"\nğŸ“‹ æµ‹è¯•å›¾åƒå°ºå¯¸: {size}")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        image = Image.new('RGB', size, color='red')
        pixels = size[0] * size[1]
        expected_tokens = pixels // (28 * 28)
        
        print(f"   ğŸ“Š åƒç´ æ•°é‡: {pixels}")
        print(f"   ğŸ“Š é¢„æœŸTokenæ•°é‡: {expected_tokens}")
        
        # æµ‹è¯•å¤„ç†
        try:
            # ä½¿ç”¨ç®€å•çš„prompt
            prompt = f"<|vision_start|><|image_pad|><|vision_end|>What color is this {size[0]}x{size[1]} image?"
            
            inputs = processor(
                text=[prompt],
                images=[image],
                return_tensors="pt",
                padding=True
            )
            
            print(f"   âœ… è¾“å…¥å¤„ç†æˆåŠŸ")
            print(f"   ğŸ“Š input_ids shape: {inputs.input_ids.shape}")
            print(f"   ğŸ–¼ï¸ pixel_values shape: {inputs.pixel_values.shape}")
            
            # æ£€æŸ¥TokenåŒ¹é…
            decoded = processor.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=False)
            image_pad_count = decoded.count('<|image_pad|>')
            features_count = inputs.pixel_values.shape[0]
            
            print(f"   ğŸ” å›¾åƒTokenæ•°é‡: {image_pad_count}")
            print(f"   ğŸ” å›¾åƒç‰¹å¾æ•°é‡: {features_count}")
            
            if image_pad_count == features_count:
                print(f"   âœ… TokenåŒ¹é…æˆåŠŸï¼è¿›è¡Œæ¨ç†æµ‹è¯•...")
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                device_inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
                
                # ç”Ÿæˆå›ç­”
                with torch.no_grad():
                    outputs = model.generate(
                        **device_inputs,
                        max_new_tokens=50,
                        do_sample=True,
                        temperature=0.7,
                        pad_token_id=processor.tokenizer.eos_token_id
                    )
                
                # è§£ç å›ç­”
                generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
                response = processor.decode(generated_ids, skip_special_tokens=True)
                
                print(f"   ğŸ¯ å›ç­”: {response.strip()}")
                
                if response.strip():
                    print(f"   âœ… å°ºå¯¸ {size} æµ‹è¯•å®Œå…¨æˆåŠŸï¼")
                else:
                    print(f"   âš ï¸ å°ºå¯¸ {size} æŠ€æœ¯æˆåŠŸï¼Œä½†æ— æ–‡æœ¬å›ç­”")
            else:
                print(f"   âŒ Tokenä¸åŒ¹é…: {image_pad_count} vs {features_count}")
                
        except Exception as e:
            print(f"   âŒ å°ºå¯¸ {size} æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•æ•°å€¼å¢å¼ºåŠŸèƒ½
    print("\nğŸ”¢ æµ‹è¯•æ•°å€¼å¢å¼ºåŠŸèƒ½...")
    test_image = Image.new('RGB', (56, 56), color='blue')
    
    numeric_questions = [
        "This image has area <num>3136</num> pixels. What color is it?",
        "The RGB value of this image is approximately <num>0.0</num>, <num>0.0</num>, <num>1.0</num>. Describe it.",
    ]
    
    for question in numeric_questions:
        print(f"\n   ğŸ¤” æ•°å€¼é—®é¢˜: {question[:50]}...")
        
        try:
            prompt = f"<|vision_start|><|image_pad|><|vision_end|>{question}"
            
            inputs = processor(
                text=[prompt],
                images=[test_image],
                return_tensors="pt",
                padding=True
            )
            
            # æ£€æŸ¥TokenåŒ¹é…
            decoded = processor.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=False)
            image_pad_count = decoded.count('<|image_pad|>')
            features_count = inputs.pixel_values.shape[0]
            
            if image_pad_count == features_count:
                print(f"   âœ… æ•°å€¼å¢å¼ºTokenåŒ¹é…æˆåŠŸ")
                
                # æ¨ç†
                device_inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model.generate(
                        **device_inputs,
                        max_new_tokens=50,
                        do_sample=True,
                        temperature=0.7,
                        pad_token_id=processor.tokenizer.eos_token_id
                    )
                
                generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
                response = processor.decode(generated_ids, skip_special_tokens=True)
                
                print(f"   ğŸ¯ æ•°å€¼å›ç­”: {response.strip()}")
            else:
                print(f"   âŒ æ•°å€¼å¢å¼ºTokenä¸åŒ¹é…: {image_pad_count} vs {features_count}")
                
        except Exception as e:
            print(f"   âŒ æ•°å€¼å¢å¼ºæµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… GitCodeåšå®¢å‚æ•°è°ƒæ•´è§£å†³æ–¹æ¡ˆæµ‹è¯•å®Œæˆ")
    print("\nğŸ“Š å…³é”®å‘ç°:")
    print("   ğŸ¯ è°ƒæ•´max_pixelså¯ä»¥æ§åˆ¶å›¾åƒå¤„ç†çš„Tokenæ•°é‡")
    print("   ğŸ¯ ç¡®ä¿image_pad_count == features_countæ˜¯æˆåŠŸçš„å…³é”®")
    print("   ğŸ¯ æ•°å€¼å¢å¼ºåŠŸèƒ½ä¸å›¾åƒå¤„ç†å¯ä»¥å…¼å®¹")

if __name__ == "__main__":
    test_parameter_adjustment_solution()
