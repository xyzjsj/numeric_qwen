#!/usr/bin/env python3
"""
å®Œæ•´çš„å›¾åƒTokenåŒ¹é…ä¿®å¤ç‰ˆæœ¬
åŸºäºæˆåŠŸçš„è§£å†³æ–¹æ¡ˆè¿›è¡Œå®Œæ•´æµ‹è¯•
"""
import os
import torch
from PIL import Image
import numpy as np

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# æ·»åŠ æ•°å€¼å¢å¼ºæ¨¡å—è·¯å¾„
import sys
sys.path.append('/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250')

from numeric_qwen2_5_vl import NumericQwen2_5_VLForConditionalGeneration, NumericQwen2_5_VLProcessor

def load_model():
    """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
    checkpoint_path = "/data1/wangzhiye/1a1a11/custom_qwen_checkpoint_4250/output/checkpoint-4250"
    
    print("ğŸ“‚ åŠ è½½checkpoint-4250æ¨¡å‹...")
    model = NumericQwen2_5_VLForConditionalGeneration.from_pretrained(
        checkpoint_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    processor = NumericQwen2_5_VLProcessor.from_pretrained(
        checkpoint_path,
        trust_remote_code=True
    )
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    return model, processor

def create_test_image(size=(56, 56), image_type="colorful"):
    """åˆ›å»ºä¸åŒç±»å‹çš„æµ‹è¯•å›¾åƒ"""
    if image_type == "colorful":
        # åˆ›å»ºå½©è‰²å›¾åƒ
        image = Image.new('RGB', size, color='blue')
        img_array = np.array(image)
        
        # æ·»åŠ çº¢è‰²çŸ©å½¢
        h, w = img_array.shape[:2]
        img_array[h//4:3*h//4, w//4:3*w//4] = [255, 0, 0]  # çº¢è‰²
        
        # æ·»åŠ ç»¿è‰²åœ†å½¢åŒºåŸŸï¼ˆè¿‘ä¼¼ï¼‰
        center_y, center_x = h//2, w//2
        for y in range(h):
            for x in range(w):
                if (x - center_x)**2 + (y - center_y)**2 < (min(h, w)//6)**2:
                    img_array[y, x] = [0, 255, 0]  # ç»¿è‰²
        
        return Image.fromarray(img_array)
    
    elif image_type == "simple":
        # ç®€å•çº¢è‰²å›¾åƒ
        return Image.new('RGB', size, color='red')
    
    elif image_type == "pattern":
        # æ¡çº¹å›¾æ¡ˆ
        image = Image.new('RGB', size, color='white')
        img_array = np.array(image)
        
        # æ·»åŠ æ¡çº¹
        for i in range(0, size[1], 8):
            img_array[:, i:i+4] = [255, 100, 50]  # æ©™è‰²æ¡çº¹
            
        return Image.fromarray(img_array)

def test_vqa_with_correct_tokens(model, processor, image, question):
    """ä½¿ç”¨æ­£ç¡®Tokenæ ¼å¼è¿›è¡ŒVQAæµ‹è¯•"""
    print(f"\nğŸ¤” é—®é¢˜: {question}")
    
    # ä½¿ç”¨æˆåŠŸçš„Tokenæ ¼å¼
    prompt = f"<|vision_start|><|image_pad|><|vision_end|>{question}"
    
    try:
        # å¤„ç†è¾“å…¥
        inputs = processor(
            text=[prompt],
            images=[image],
            return_tensors="pt",
            padding=True
        )
        
        print(f"ğŸ“¦ è¾“å…¥å¤„ç†æˆåŠŸ")
        print(f"ğŸ“Š input_ids shape: {inputs.input_ids.shape}")
        print(f"ğŸ–¼ï¸ pixel_values shape: {inputs.pixel_values.shape}")
        
        # éªŒè¯TokenåŒ¹é…
        decoded = processor.tokenizer.decode(inputs.input_ids[0], skip_special_tokens=False)
        image_pad_count = decoded.count('<|image_pad|>')
        expected_features = inputs.pixel_values.shape[0]
        
        print(f"ğŸ” å›¾åƒTokenæ•°é‡: {image_pad_count}")
        print(f"ğŸ” å›¾åƒç‰¹å¾æ•°é‡: {expected_features}")
        print(f"âœ… TokenåŒ¹é…çŠ¶æ€: {'åŒ¹é…' if image_pad_count == expected_features else 'ä¸åŒ¹é…'}")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        device_inputs = {k: v.to(model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = model.generate(
                **device_inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        generated_ids = outputs[0][device_inputs['input_ids'].shape[1]:]
        response = processor.decode(generated_ids, skip_special_tokens=True)
        
        print(f"ğŸ¯ å›ç­”: {response.strip()}")
        return response.strip()
        
    except Exception as e:
        print(f"âŒ VQAå¤±è´¥: {e}")
        return None

def test_numeric_enhancement_with_images(model, processor):
    """æµ‹è¯•å¸¦å›¾åƒçš„æ•°å€¼å¢å¼ºåŠŸèƒ½"""
    print("\nğŸ”¢ æµ‹è¯•å›¾åƒ+æ•°å€¼å¢å¼ºåŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    image = create_test_image(size=(56, 56), image_type="simple")
    
    # åŒ…å«æ•°å€¼çš„é—®é¢˜
    questions_with_numbers = [
        "This image shows a shape. If its width is <num>5.5</num> units and height is <num>3.2</num> units, what is the area?",
        "The color intensity in this image is <num>0.8</num>. Describe what you see.",
        "If this image represents <num>2.5</num> objects, describe them."
    ]
    
    for question in questions_with_numbers:
        response = test_vqa_with_correct_tokens(model, processor, image, question)
        if response:
            print(f"âœ… æ•°å€¼å¢å¼ºæµ‹è¯•æˆåŠŸ")
        else:
            print(f"âŒ æ•°å€¼å¢å¼ºæµ‹è¯•å¤±è´¥")

def comprehensive_image_test():
    """ç»¼åˆå›¾åƒæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ç»¼åˆå›¾åƒTokenåŒ¹é…æµ‹è¯•")
    print("=" * 70)
    
    # åŠ è½½æ¨¡å‹
    model, processor = load_model()
    
    # æµ‹è¯•1: åŸºç¡€é¢œè‰²è¯†åˆ«
    print("\nğŸ“‹ æµ‹è¯•1: åŸºç¡€é¢œè‰²è¯†åˆ«")
    simple_image = create_test_image(size=(56, 56), image_type="simple")
    test_vqa_with_correct_tokens(model, processor, simple_image, "What color is this image?")
    
    # æµ‹è¯•2: å¤æ‚å›¾åƒæè¿°
    print("\nğŸ“‹ æµ‹è¯•2: å¤æ‚å›¾åƒæè¿°")
    colorful_image = create_test_image(size=(56, 56), image_type="colorful")
    test_vqa_with_correct_tokens(model, processor, colorful_image, "Describe the shapes and colors in this image.")
    
    # æµ‹è¯•3: å›¾æ¡ˆè¯†åˆ«
    print("\nğŸ“‹ æµ‹è¯•3: å›¾æ¡ˆè¯†åˆ«")
    pattern_image = create_test_image(size=(56, 56), image_type="pattern")
    test_vqa_with_correct_tokens(model, processor, pattern_image, "What pattern do you see in this image?")
    
    # æµ‹è¯•4: æ•°å€¼å¢å¼ºåŠŸèƒ½
    test_numeric_enhancement_with_images(model, processor)
    
    # æµ‹è¯•5: ä¸åŒå°ºå¯¸çš„å›¾åƒ
    print("\nğŸ“‹ æµ‹è¯•5: ä¸åŒå°ºå¯¸å›¾åƒ")
    sizes_to_test = [(28, 28), (56, 56), (84, 84)]
    
    for size in sizes_to_test:
        print(f"\n   ğŸ” æµ‹è¯•å°ºå¯¸: {size}")
        test_image = create_test_image(size=size, image_type="simple")
        pixels = size[0] * size[1]
        expected_tokens = pixels // (28 * 28)
        print(f"   ğŸ“Š é¢„æœŸTokenæ•°é‡: {expected_tokens}")
        
        response = test_vqa_with_correct_tokens(model, processor, test_image, f"What do you see in this {size[0]}x{size[1]} image?")
        if response:
            print(f"   âœ… å°ºå¯¸ {size} æµ‹è¯•æˆåŠŸ")
        else:
            print(f"   âš ï¸ å°ºå¯¸ {size} æµ‹è¯•æ— å“åº”")
    
    print("\n" + "=" * 70)
    print("âœ… ç»¼åˆå›¾åƒTokenåŒ¹é…æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š æ€»ç»“:")
    print("   ğŸ¯ æˆåŠŸè§£å†³äº†'Image features and image tokens do not match'é”™è¯¯")
    print("   ğŸ¯ ä½¿ç”¨æ­£ç¡®çš„Tokenæ ¼å¼: <|vision_start|><|image_pad|><|vision_end|>")
    print("   ğŸ¯ é€šè¿‡æ§åˆ¶å›¾åƒå°ºå¯¸ç¡®ä¿Tokenæ•°é‡åŒ¹é…")
    print("   ğŸ¯ éªŒè¯äº†æ•°å€¼å¢å¼ºåŠŸèƒ½ä¸å›¾åƒå¤„ç†çš„å…¼å®¹æ€§")

if __name__ == "__main__":
    comprehensive_image_test()
